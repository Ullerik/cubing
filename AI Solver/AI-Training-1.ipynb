{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA:\n",
    "We have table lookup for scrambles up to k=6\n",
    "\n",
    "First make a network, might be pretrained\n",
    "\n",
    "Then make a solver:\n",
    "For a scramble of move n, apply n-k AI moves, and run table lookup on each state we reach (including before first AI move)\n",
    "If a solution is found, return the scramble, else return False\n",
    "\n",
    "Next, put them together.\n",
    "We start by making an empty scramble list which we will use for training.\n",
    "For a fixed n, first generate a scramble of that length.\n",
    "Next, check if this is solvable using the solver.\n",
    "If not, we add this scramble into the scramble list.\n",
    "Repeat until we obtain N (=10 000) scrambles?\n",
    "Then train on all these, and all subscrambles (i.e. all scrambles from length k to n).\n",
    "This should ensure/make it so we don't lose the training on the smaller scramble sets\n",
    "We repeat all this until we obtain a reasonable rate r (=80%?) for finding solutions for that n.\n",
    "\n",
    "Finally, train by gradually increasing n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support algs imported!\n",
      "Cube initiated\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "%run ../cube/Cube.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "def cube_to_tensor(cube, long_repr = True):\n",
    "    '''\n",
    "    removes center stickers, top corners\n",
    "    (should I remove 1 whole corner and 3 edge stickers as well?)\n",
    "    '''\n",
    "    state = cube.state.tolist()\n",
    "    short_state = state[0:4]\n",
    "    short_state += state[5:13]\n",
    "    short_state += state[14:22]\n",
    "    short_state += state[23:31]\n",
    "    short_state += state[32:36]\n",
    "    short_state += [state[37],state[39],state[41],state[43],state[46],state[48],state[50],state[52]]\n",
    "    if long_repr:\n",
    "        long_state = np.zeros(len(short_state)*6)\n",
    "        for i in range(len(short_state)):\n",
    "            long_state[6*i:6*i+6] = np.array([0]*(short_state[i]-1) + [1] + [0]*(6-short_state[i]))\n",
    "        tensor = torch.from_numpy(np.array(long_state, dtype=np.float32))\n",
    "        return tensor.to(device)\n",
    "    tensor = torch.from_numpy(np.array(long_state, dtype=np.float32))\n",
    "    return tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(240, 512)\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.fc4 = nn.Linear(128, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 2*in_features)\n",
    "        self.fc2 = nn.Linear(2*in_features, in_features)\n",
    "        self.fc3 = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out += x\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size=240, output_size=18, hidden_sizes=[512, 256, 128], num_blocks=4):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.fc_input = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc_output = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            self.residual_blocks.append(ResidualBlock(hidden_sizes[i],hidden_sizes[i+1]))\n",
    "\n",
    "        self.fc_last = nn.Linear(hidden_sizes[-1], hidden_sizes[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_input(x)\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        for residual_block in self.residual_blocks:\n",
    "            out = residual_block(out)\n",
    "\n",
    "        out = self.fc_last(out)\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        out = self.fc_output(out)\n",
    "        return out\n",
    "\n",
    "# Create an instance of ResNet with input size 240, output size 18, and layer sizes [512, 256, 128]\n",
    "network = ResNet(input_size=240, output_size=18, hidden_sizes=[512, 256, 128])\n",
    "\n",
    "network = network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def algs_to_dataset(algs, min_alg_length):\n",
    "    # total number of data points: nr_of_algs*(max_alg_length - min_alg_length + 1)\n",
    "    dataset = []\n",
    "    for alg in algs:\n",
    "        cube = Cube()\n",
    "        cube.apply_moves(alg)\n",
    "        alg_moves = alg.split(\" \")\n",
    "        for j in range(len(alg_moves)-1,min_alg_length-2,-1):\n",
    "            inv = inverse_alg(alg_moves[j])\n",
    "            target = torch.tensor([moves.index(inv)])\n",
    "            target = target.to(device)\n",
    "            dataset.append((cube_to_tensor(cube),target))\n",
    "            cube.apply_moves(inv)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smart_dataset(nr_of_algs, data_exp, min_alg_length, max_alg_length):\n",
    "#     # total number of data points: nr_of_algs*(max_alg_length - min_alg_length + 1)\n",
    "#     dataset = []\n",
    "#     for i in range(nr_of_algs):\n",
    "#         alg = gen_random_alg(max_alg_length)\n",
    "#         sub_algs = []\n",
    "#         cube = Cube()\n",
    "#         cube.apply_moves(alg)\n",
    "#         for j in range(max_alg_length-1,min_alg_length-2,-1):\n",
    "#             inv = inverse_alg(alg.split(\" \")[j])\n",
    "#             target = torch.tensor([moves.index(inv)])\n",
    "#             target = target.to(device)\n",
    "#             dataset.append((cube_to_tensor(cube),target))\n",
    "#             cube.apply_moves(inv)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table for table lookup\n",
    "import pickle\n",
    "with open('dict6.pkl', 'rb') as f:\n",
    "    table = pickle.load(f)\n",
    "    \n",
    "def lookup_solver(cube):\n",
    "    '''\n",
    "    assumes there is some \"table\" to use\n",
    "    '''\n",
    "    ID = id_from_state(cube.state)\n",
    "    if ID in table.keys():\n",
    "        alg = table[ID]\n",
    "        return alg\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_cube(cube):\n",
    "    state_tensor = cube_to_tensor(cube)\n",
    "    output = network(state_tensor)\n",
    "#     print(output)\n",
    "    _, predicted_class = torch.max(output, dim=0)\n",
    "    return predicted_class.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AI_linear_solver(cube, iterations = 10):\n",
    "    solution = \"\"\n",
    "    last_move = \"NO\"\n",
    "    for i in range(iterations):\n",
    "        alg = lookup_solver(cube)\n",
    "        if alg:\n",
    "            cube.apply_moves(alg)\n",
    "            return solution + alg\n",
    "#         rot = reduce_symmetries(cube) # for the actual solver, we need to rotate back afterwards as well\n",
    "        ev = evalute_cube(cube)\n",
    "        move = moves[ev]\n",
    "        solution += move+\" \"\n",
    "        cube.apply_moves(move)\n",
    "    alg = lookup_solver(cube)\n",
    "    if alg:\n",
    "        cube.apply_moves(alg)\n",
    "        return solution + alg\n",
    "    else:\n",
    "        cube.apply_moves(inverse_alg(solution.strip()))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.2639, Time Spent: 30s\n",
      "Next Learning Rate: 0.0009993\n",
      "2. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.2411, Time Spent: 59s\n",
      "Next Learning Rate: 0.00099860049\n",
      "3. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.2488, Time Spent: 89s\n",
      "Next Learning Rate: 0.000997901469657\n",
      "4. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2907, Time Spent: 118s\n",
      "Next Learning Rate: 0.00099720293862824\n",
      "5. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2706, Time Spent: 148s\n",
      "Next Learning Rate: 0.0009965048965712001\n",
      "6. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2732, Time Spent: 177s\n",
      "Next Learning Rate: 0.0009958073431436004\n",
      "7. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2713, Time Spent: 206s\n",
      "Next Learning Rate: 0.0009951102780033998\n",
      "8. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2811, Time Spent: 235s\n",
      "Next Learning Rate: 0.0009944137008087975\n",
      "9. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.2542, Time Spent: 264s\n",
      "Next Learning Rate: 0.0009937176112182313\n",
      "10. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2603, Time Spent: 296s\n",
      "Next Learning Rate: 0.0009930220088903785\n",
      "11. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2735, Time Spent: 326s\n",
      "Next Learning Rate: 0.0009923268934841553\n",
      "12. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.2429, Time Spent: 355s\n",
      "Next Learning Rate: 0.0009916322646587165\n",
      "13. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2573, Time Spent: 384s\n",
      "Next Learning Rate: 0.0009909381220734553\n",
      "14. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2431, Time Spent: 413s\n",
      "Next Learning Rate: 0.000990244465388004\n",
      "15. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.2550, Time Spent: 443s\n",
      "Next Learning Rate: 0.0009895512942622322\n",
      "16. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2387, Time Spent: 472s\n",
      "Next Learning Rate: 0.0009888586083562486\n",
      "17. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2604, Time Spent: 501s\n",
      "Next Learning Rate: 0.0009881664073303992\n",
      "18. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.2331, Time Spent: 530s\n",
      "Next Learning Rate: 0.0009874746908452678\n",
      "19. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2288, Time Spent: 560s\n",
      "Next Learning Rate: 0.000986783458561676\n",
      "20. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2322, Time Spent: 589s\n",
      "Next Learning Rate: 0.0009860927101406828\n",
      "21. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2481, Time Spent: 618s\n",
      "Next Learning Rate: 0.0009854024452435844\n",
      "22. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.2759, Time Spent: 647s\n",
      "Next Learning Rate: 0.0009847126635319139\n",
      "23. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2460, Time Spent: 676s\n",
      "Next Learning Rate: 0.0009840233646674415\n",
      "24. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.2351, Time Spent: 705s\n",
      "Next Learning Rate: 0.0009833345483121743\n",
      "25. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.2613, Time Spent: 734s\n",
      "Next Learning Rate: 0.0009826462141283557\n",
      "26. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.2523, Time Spent: 764s\n",
      "Next Learning Rate: 0.0009819583617784657\n",
      "27. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2441, Time Spent: 793s\n",
      "Next Learning Rate: 0.0009812709909252208\n",
      "28. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2576, Time Spent: 822s\n",
      "Next Learning Rate: 0.000980584101231573\n",
      "29. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.2361, Time Spent: 851s\n",
      "Next Learning Rate: 0.000979897692360711\n",
      "30. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2342, Time Spent: 880s\n",
      "Next Learning Rate: 0.0009792117639760584\n",
      "31. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.2389, Time Spent: 909s\n",
      "Next Learning Rate: 0.000978526315741275\n",
      "32. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2260, Time Spent: 938s\n",
      "Next Learning Rate: 0.0009778413473202562\n",
      "33. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2226, Time Spent: 967s\n",
      "Next Learning Rate: 0.000977156858377132\n",
      "34. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2275, Time Spent: 996s\n",
      "Next Learning Rate: 0.000976472848576268\n",
      "35. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.2029, Time Spent: 1025s\n",
      "Next Learning Rate: 0.0009757893175822645\n",
      "36. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2243, Time Spent: 1055s\n",
      "Next Learning Rate: 0.0009751062650599569\n",
      "37. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.2150, Time Spent: 1084s\n",
      "Next Learning Rate: 0.000974423690674415\n",
      "38. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.2212, Time Spent: 1113s\n",
      "Next Learning Rate: 0.0009737415940909429\n",
      "39. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2351, Time Spent: 1142s\n",
      "Next Learning Rate: 0.0009730599749750792\n",
      "40. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2434, Time Spent: 1171s\n",
      "Next Learning Rate: 0.0009723788329925965\n",
      "41. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2495, Time Spent: 1200s\n",
      "Next Learning Rate: 0.0009716981678095017\n",
      "42. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.1922, Time Spent: 1229s\n",
      "Next Learning Rate: 0.000971017979092035\n",
      "43. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.2228, Time Spent: 1258s\n",
      "Next Learning Rate: 0.0009703382665066705\n",
      "44. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2205, Time Spent: 1288s\n",
      "Next Learning Rate: 0.0009696590297201158\n",
      "45. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.2294, Time Spent: 1317s\n",
      "Next Learning Rate: 0.0009689802683993117\n",
      "46. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.2330, Time Spent: 1346s\n",
      "Next Learning Rate: 0.0009683019822114322\n",
      "47. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 2.2325, Time Spent: 1375s\n",
      "Next Learning Rate: 0.0009676241708238842\n",
      "48. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1968, Time Spent: 1405s\n",
      "Next Learning Rate: 0.0009669468339043074\n",
      "49. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1977, Time Spent: 1434s\n",
      "Next Learning Rate: 0.0009662699711205743\n",
      "50. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1946, Time Spent: 1463s\n",
      "Next Learning Rate: 0.0009655935821407899\n",
      "51. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1934, Time Spent: 1492s\n",
      "Next Learning Rate: 0.0009649176666332914\n",
      "52. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.2055, Time Spent: 1522s\n",
      "Next Learning Rate: 0.000964242224266648\n",
      "53. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.2041, Time Spent: 1551s\n",
      "Next Learning Rate: 0.0009635672547096614\n",
      "54. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.2200, Time Spent: 1580s\n",
      "Next Learning Rate: 0.0009628927576313646\n",
      "55. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2072, Time Spent: 1609s\n",
      "Next Learning Rate: 0.0009622187327010226\n",
      "56. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2261, Time Spent: 1638s\n",
      "Next Learning Rate: 0.0009615451795881319\n",
      "57. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1980, Time Spent: 1668s\n",
      "Next Learning Rate: 0.0009608720979624202\n",
      "58. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.2013, Time Spent: 1697s\n",
      "Next Learning Rate: 0.0009601994874938464\n",
      "59. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1826, Time Spent: 1726s\n",
      "Next Learning Rate: 0.0009595273478526007\n",
      "60. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2124, Time Spent: 1755s\n",
      "Next Learning Rate: 0.0009588556787091039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1916, Time Spent: 1784s\n",
      "Next Learning Rate: 0.0009581844797340075\n",
      "62. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1971, Time Spent: 1813s\n",
      "Next Learning Rate: 0.0009575137505981936\n",
      "63. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.2066, Time Spent: 1843s\n",
      "Next Learning Rate: 0.0009568434909727749\n",
      "64. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.1900, Time Spent: 1872s\n",
      "Next Learning Rate: 0.0009561737005290939\n",
      "65. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1854, Time Spent: 1900s\n",
      "Next Learning Rate: 0.0009555043789387235\n",
      "66. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.1960, Time Spent: 1929s\n",
      "Next Learning Rate: 0.0009548355258734664\n",
      "67. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.2200, Time Spent: 1958s\n",
      "Next Learning Rate: 0.000954167141005355\n",
      "68. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1950, Time Spent: 1987s\n",
      "Next Learning Rate: 0.0009534992240066512\n",
      "69. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1607, Time Spent: 2015s\n",
      "Next Learning Rate: 0.0009528317745498466\n",
      "70. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1980, Time Spent: 2044s\n",
      "Next Learning Rate: 0.0009521647923076617\n",
      "71. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1821, Time Spent: 2073s\n",
      "Next Learning Rate: 0.0009514982769530463\n",
      "72. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1968, Time Spent: 2101s\n",
      "Next Learning Rate: 0.0009508322281591791\n",
      "73. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1922, Time Spent: 2130s\n",
      "Next Learning Rate: 0.0009501666455994677\n",
      "74. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1673, Time Spent: 2159s\n",
      "Next Learning Rate: 0.000949501528947548\n",
      "75. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1706, Time Spent: 2188s\n",
      "Next Learning Rate: 0.0009488368778772847\n",
      "76. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1850, Time Spent: 2217s\n",
      "Next Learning Rate: 0.0009481726920627706\n",
      "77. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1984, Time Spent: 2246s\n",
      "Next Learning Rate: 0.0009475089711783267\n",
      "78. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.2076, Time Spent: 2275s\n",
      "Next Learning Rate: 0.0009468457148985018\n",
      "79. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1568, Time Spent: 2303s\n",
      "Next Learning Rate: 0.0009461829228980728\n",
      "80. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1717, Time Spent: 2332s\n",
      "Next Learning Rate: 0.0009455205948520441\n",
      "81. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1630, Time Spent: 2361s\n",
      "Next Learning Rate: 0.0009448587304356476\n",
      "82. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1829, Time Spent: 2390s\n",
      "Next Learning Rate: 0.0009441973293243427\n",
      "83. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1860, Time Spent: 2419s\n",
      "Next Learning Rate: 0.0009435363911938156\n",
      "84. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1853, Time Spent: 2448s\n",
      "Next Learning Rate: 0.0009428759157199799\n",
      "85. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1578, Time Spent: 2476s\n",
      "Next Learning Rate: 0.0009422159025789759\n",
      "86. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.1922, Time Spent: 2505s\n",
      "Next Learning Rate: 0.0009415563514471706\n",
      "87. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1830, Time Spent: 2534s\n",
      "Next Learning Rate: 0.0009408972620011575\n",
      "88. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1737, Time Spent: 2563s\n",
      "Next Learning Rate: 0.0009402386339177566\n",
      "89. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.2023, Time Spent: 2592s\n",
      "Next Learning Rate: 0.0009395804668740141\n",
      "90. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1349, Time Spent: 2620s\n",
      "Next Learning Rate: 0.0009389227605472023\n",
      "91. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1782, Time Spent: 2649s\n",
      "Next Learning Rate: 0.0009382655146148192\n",
      "92. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1538, Time Spent: 2678s\n",
      "Next Learning Rate: 0.0009376087287545887\n",
      "93. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1571, Time Spent: 2707s\n",
      "Next Learning Rate: 0.0009369524026444604\n",
      "94. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1759, Time Spent: 2736s\n",
      "Next Learning Rate: 0.0009362965359626092\n",
      "95. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 2.1700, Time Spent: 2764s\n",
      "Next Learning Rate: 0.0009356411283874354\n",
      "96. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1564, Time Spent: 2793s\n",
      "Next Learning Rate: 0.0009349861795975642\n",
      "97. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1792, Time Spent: 2822s\n",
      "Next Learning Rate: 0.0009343316892718459\n",
      "98. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1608, Time Spent: 2851s\n",
      "Next Learning Rate: 0.0009336776570893556\n",
      "99. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1863, Time Spent: 2879s\n",
      "Next Learning Rate: 0.0009330240827293931\n",
      "100. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1272, Time Spent: 2908s\n",
      "Next Learning Rate: 0.0009323709658714825\n",
      "101. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1659, Time Spent: 2937s\n",
      "Next Learning Rate: 0.0009317183061953724\n",
      "102. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1501, Time Spent: 2966s\n",
      "Next Learning Rate: 0.0009310661033810356\n",
      "103. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1635, Time Spent: 2995s\n",
      "Next Learning Rate: 0.0009304143571086689\n",
      "104. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1660, Time Spent: 3023s\n",
      "Next Learning Rate: 0.0009297630670586928\n",
      "105. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1773, Time Spent: 3052s\n",
      "Next Learning Rate: 0.0009291122329117517\n",
      "106. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1756, Time Spent: 3081s\n",
      "Next Learning Rate: 0.0009284618543487134\n",
      "107. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 2.1797, Time Spent: 3110s\n",
      "Next Learning Rate: 0.0009278119310506692\n",
      "108. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1493, Time Spent: 3139s\n",
      "Next Learning Rate: 0.0009271624626989338\n",
      "109. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1642, Time Spent: 3167s\n",
      "Next Learning Rate: 0.0009265134489750445\n",
      "110. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1558, Time Spent: 3196s\n",
      "Next Learning Rate: 0.000925864889560762\n",
      "111. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1402, Time Spent: 3225s\n",
      "Next Learning Rate: 0.0009252167841380693\n",
      "112. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.1601, Time Spent: 3254s\n",
      "Next Learning Rate: 0.0009245691323891726\n",
      "113. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1531, Time Spent: 3283s\n",
      "Next Learning Rate: 0.0009239219339965002\n",
      "114. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1346, Time Spent: 3311s\n",
      "Next Learning Rate: 0.0009232751886427027\n",
      "115. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1441, Time Spent: 3340s\n",
      "Next Learning Rate: 0.0009226288960106527\n",
      "116. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1331, Time Spent: 3368s\n",
      "Next Learning Rate: 0.0009219830557834453\n",
      "117. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1384, Time Spent: 3397s\n",
      "Next Learning Rate: 0.0009213376676443968\n",
      "118. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1576, Time Spent: 3426s\n",
      "Next Learning Rate: 0.0009206927312770457\n",
      "119. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1478, Time Spent: 3455s\n",
      "Next Learning Rate: 0.0009200482463651517\n",
      "120. 1000/1004 scrambles genned. Solvable algs genned: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 2.1362, Time Spent: 3484s\n",
      "Next Learning Rate: 0.0009194042125926961\n",
      "121. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.1568, Time Spent: 3512s\n",
      "Next Learning Rate: 0.0009187606296438812\n",
      "122. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1582, Time Spent: 3541s\n",
      "Next Learning Rate: 0.0009181174972031304\n",
      "123. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.1514, Time Spent: 3570s\n",
      "Next Learning Rate: 0.0009174748149550882\n",
      "124. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1268, Time Spent: 3598s\n",
      "Next Learning Rate: 0.0009168325825846196\n",
      "125. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1571, Time Spent: 3627s\n",
      "Next Learning Rate: 0.0009161907997768103\n",
      "126. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1691, Time Spent: 3656s\n",
      "Next Learning Rate: 0.0009155494662169665\n",
      "127. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1260, Time Spent: 3684s\n",
      "Next Learning Rate: 0.0009149085815906146\n",
      "128. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1311, Time Spent: 3713s\n",
      "Next Learning Rate: 0.0009142681455835012\n",
      "129. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1261, Time Spent: 3742s\n",
      "Next Learning Rate: 0.0009136281578815927\n",
      "130. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1544, Time Spent: 3771s\n",
      "Next Learning Rate: 0.0009129886181710755\n",
      "131. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1613, Time Spent: 3800s\n",
      "Next Learning Rate: 0.0009123495261383558\n",
      "132. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.1477, Time Spent: 3829s\n",
      "Next Learning Rate: 0.0009117108814700589\n",
      "133. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1496, Time Spent: 3857s\n",
      "Next Learning Rate: 0.0009110726838530298\n",
      "134. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.1311, Time Spent: 3886s\n",
      "Next Learning Rate: 0.0009104349329743327\n",
      "135. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1564, Time Spent: 3914s\n",
      "Next Learning Rate: 0.0009097976285212506\n",
      "136. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1322, Time Spent: 3944s\n",
      "Next Learning Rate: 0.0009091607701812857\n",
      "137. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1279, Time Spent: 3972s\n",
      "Next Learning Rate: 0.0009085243576421588\n",
      "138. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.1301, Time Spent: 4001s\n",
      "Next Learning Rate: 0.0009078883905918093\n",
      "139. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1365, Time Spent: 4030s\n",
      "Next Learning Rate: 0.000907252868718395\n",
      "140. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1360, Time Spent: 4059s\n",
      "Next Learning Rate: 0.000906617791710292\n",
      "141. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1062, Time Spent: 4088s\n",
      "Next Learning Rate: 0.0009059831592560948\n",
      "142. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1403, Time Spent: 4117s\n",
      "Next Learning Rate: 0.0009053489710446155\n",
      "143. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.1156, Time Spent: 4146s\n",
      "Next Learning Rate: 0.0009047152267648843\n",
      "144. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1128, Time Spent: 4174s\n",
      "Next Learning Rate: 0.0009040819261061488\n",
      "145. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1172, Time Spent: 4203s\n",
      "Next Learning Rate: 0.0009034490687578745\n",
      "146. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.1293, Time Spent: 4232s\n",
      "Next Learning Rate: 0.0009028166544097439\n",
      "147. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1386, Time Spent: 4261s\n",
      "Next Learning Rate: 0.0009021846827516571\n",
      "148. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1318, Time Spent: 4289s\n",
      "Next Learning Rate: 0.0009015531534737308\n",
      "149. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1128, Time Spent: 4319s\n",
      "Next Learning Rate: 0.0009009220662662992\n",
      "150. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 2.1389, Time Spent: 4347s\n",
      "Next Learning Rate: 0.0009002914208199128\n",
      "151. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1371, Time Spent: 4376s\n",
      "Next Learning Rate: 0.0008996612168253388\n",
      "152. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0979, Time Spent: 4405s\n",
      "Next Learning Rate: 0.000899031453973561\n",
      "153. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1196, Time Spent: 4434s\n",
      "Next Learning Rate: 0.0008984021319557795\n",
      "154. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1176, Time Spent: 4463s\n",
      "Next Learning Rate: 0.0008977732504634105\n",
      "155. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1334, Time Spent: 4491s\n",
      "Next Learning Rate: 0.000897144809188086\n",
      "156. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1266, Time Spent: 4520s\n",
      "Next Learning Rate: 0.0008965168078216543\n",
      "157. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1032, Time Spent: 4549s\n",
      "Next Learning Rate: 0.0008958892460561791\n",
      "158. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1342, Time Spent: 4578s\n",
      "Next Learning Rate: 0.0008952621235839397\n",
      "159. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1143, Time Spent: 4606s\n",
      "Next Learning Rate: 0.0008946354400974309\n",
      "160. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1091, Time Spent: 4635s\n",
      "Next Learning Rate: 0.0008940091952893627\n",
      "161. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1110, Time Spent: 4664s\n",
      "Next Learning Rate: 0.0008933833888526601\n",
      "162. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1050, Time Spent: 4693s\n",
      "Next Learning Rate: 0.0008927580204804633\n",
      "163. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1413, Time Spent: 4722s\n",
      "Next Learning Rate: 0.0008921330898661269\n",
      "164. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1207, Time Spent: 4751s\n",
      "Next Learning Rate: 0.0008915085967032206\n",
      "165. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0975, Time Spent: 4779s\n",
      "Next Learning Rate: 0.0008908845406855283\n",
      "166. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.1187, Time Spent: 4808s\n",
      "Next Learning Rate: 0.0008902609215070484\n",
      "167. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1124, Time Spent: 4837s\n",
      "Next Learning Rate: 0.0008896377388619935\n",
      "168. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1321, Time Spent: 4866s\n",
      "Next Learning Rate: 0.00088901499244479\n",
      "169. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1134, Time Spent: 4894s\n",
      "Next Learning Rate: 0.0008883926819500787\n",
      "170. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.1310, Time Spent: 4923s\n",
      "Next Learning Rate: 0.0008877708070727135\n",
      "171. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1045, Time Spent: 4952s\n",
      "Next Learning Rate: 0.0008871493675077626\n",
      "172. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1242, Time Spent: 4980s\n",
      "Next Learning Rate: 0.0008865283629505071\n",
      "173. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1429, Time Spent: 5009s\n",
      "Next Learning Rate: 0.0008859077930964417\n",
      "174. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1132, Time Spent: 5038s\n",
      "Next Learning Rate: 0.0008852876576412742\n",
      "175. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0990, Time Spent: 5066s\n",
      "Next Learning Rate: 0.0008846679562809253\n",
      "176. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0860, Time Spent: 5095s\n",
      "Next Learning Rate: 0.0008840486887115287\n",
      "177. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1169, Time Spent: 5124s\n",
      "Next Learning Rate: 0.0008834298546294306\n",
      "178. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 2.1098, Time Spent: 5153s\n",
      "Next Learning Rate: 0.0008828114537311899\n",
      "179. 1000/1005 scrambles genned. Solvable algs genned: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 2.0864, Time Spent: 5181s\n",
      "Next Learning Rate: 0.0008821934857135781\n",
      "180. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0754, Time Spent: 5210s\n",
      "Next Learning Rate: 0.0008815759502735785\n",
      "181. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1120, Time Spent: 5239s\n",
      "Next Learning Rate: 0.000880958847108387\n",
      "182. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0881, Time Spent: 5268s\n",
      "Next Learning Rate: 0.000880342175915411\n",
      "183. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0969, Time Spent: 5297s\n",
      "Next Learning Rate: 0.0008797259363922702\n",
      "184. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0949, Time Spent: 5326s\n",
      "Next Learning Rate: 0.0008791101282367955\n",
      "185. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1041, Time Spent: 5355s\n",
      "Next Learning Rate: 0.0008784947511470297\n",
      "186. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1004, Time Spent: 5383s\n",
      "Next Learning Rate: 0.0008778798048212267\n",
      "187. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0998, Time Spent: 5412s\n",
      "Next Learning Rate: 0.0008772652889578518\n",
      "188. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.1076, Time Spent: 5441s\n",
      "Next Learning Rate: 0.0008766512032555813\n",
      "189. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1271, Time Spent: 5470s\n",
      "Next Learning Rate: 0.0008760375474133024\n",
      "190. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0690, Time Spent: 5499s\n",
      "Next Learning Rate: 0.000875424321130113\n",
      "191. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.1154, Time Spent: 5527s\n",
      "Next Learning Rate: 0.0008748115241053219\n",
      "192. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1040, Time Spent: 5556s\n",
      "Next Learning Rate: 0.0008741991560384482\n",
      "193. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0698, Time Spent: 5585s\n",
      "Next Learning Rate: 0.0008735872166292213\n",
      "194. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0899, Time Spent: 5614s\n",
      "Next Learning Rate: 0.0008729757055775807\n",
      "195. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1105, Time Spent: 5643s\n",
      "Next Learning Rate: 0.0008723646225836764\n",
      "196. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0818, Time Spent: 5672s\n",
      "Next Learning Rate: 0.0008717539673478678\n",
      "197. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1121, Time Spent: 5700s\n",
      "Next Learning Rate: 0.0008711437395707242\n",
      "198. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1248, Time Spent: 5729s\n",
      "Next Learning Rate: 0.0008705339389530246\n",
      "199. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1054, Time Spent: 5758s\n",
      "Next Learning Rate: 0.0008699245651957575\n",
      "200. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0829, Time Spent: 5787s\n",
      "Next Learning Rate: 0.0008693156180001204\n",
      "201. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1163, Time Spent: 5815s\n",
      "Next Learning Rate: 0.0008687070970675203\n",
      "202. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0923, Time Spent: 5844s\n",
      "Next Learning Rate: 0.000868099002099573\n",
      "203. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.1031, Time Spent: 5873s\n",
      "Next Learning Rate: 0.0008674913327981033\n",
      "204. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0858, Time Spent: 5902s\n",
      "Next Learning Rate: 0.0008668840888651446\n",
      "205. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.1034, Time Spent: 5930s\n",
      "Next Learning Rate: 0.000866277270002939\n",
      "206. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0907, Time Spent: 5959s\n",
      "Next Learning Rate: 0.0008656708759139369\n",
      "207. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0709, Time Spent: 5988s\n",
      "Next Learning Rate: 0.0008650649063007971\n",
      "208. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0939, Time Spent: 6016s\n",
      "Next Learning Rate: 0.0008644593608663865\n",
      "209. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0918, Time Spent: 6045s\n",
      "Next Learning Rate: 0.00086385423931378\n",
      "210. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0868, Time Spent: 6074s\n",
      "Next Learning Rate: 0.0008632495413462603\n",
      "211. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0700, Time Spent: 6102s\n",
      "Next Learning Rate: 0.0008626452666673179\n",
      "212. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0813, Time Spent: 6131s\n",
      "Next Learning Rate: 0.0008620414149806508\n",
      "213. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.1062, Time Spent: 6160s\n",
      "Next Learning Rate: 0.0008614379859901642\n",
      "214. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0871, Time Spent: 6188s\n",
      "Next Learning Rate: 0.0008608349793999711\n",
      "215. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0915, Time Spent: 6217s\n",
      "Next Learning Rate: 0.0008602323949143911\n",
      "216. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0864, Time Spent: 6246s\n",
      "Next Learning Rate: 0.000859630232237951\n",
      "217. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0728, Time Spent: 6275s\n",
      "Next Learning Rate: 0.0008590284910753844\n",
      "218. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0902, Time Spent: 6303s\n",
      "Next Learning Rate: 0.0008584271711316316\n",
      "219. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.1019, Time Spent: 6332s\n",
      "Next Learning Rate: 0.0008578262721118394\n",
      "220. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0945, Time Spent: 6361s\n",
      "Next Learning Rate: 0.0008572257937213611\n",
      "221. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0916, Time Spent: 6389s\n",
      "Next Learning Rate: 0.0008566257356657561\n",
      "222. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.1026, Time Spent: 6418s\n",
      "Next Learning Rate: 0.0008560260976507901\n",
      "223. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0773, Time Spent: 6447s\n",
      "Next Learning Rate: 0.0008554268793824346\n",
      "224. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0978, Time Spent: 6475s\n",
      "Next Learning Rate: 0.0008548280805668668\n",
      "225. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0870, Time Spent: 6504s\n",
      "Next Learning Rate: 0.00085422970091047\n",
      "226. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0590, Time Spent: 6533s\n",
      "Next Learning Rate: 0.0008536317401198327\n",
      "227. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.0823, Time Spent: 6562s\n",
      "Next Learning Rate: 0.0008530341979017488\n",
      "228. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0943, Time Spent: 6591s\n",
      "Next Learning Rate: 0.0008524370739632175\n",
      "229. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0857, Time Spent: 6619s\n",
      "Next Learning Rate: 0.0008518403680114432\n",
      "230. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0796, Time Spent: 6648s\n",
      "Next Learning Rate: 0.0008512440797538352\n",
      "231. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0632, Time Spent: 6677s\n",
      "Next Learning Rate: 0.0008506482088980075\n",
      "232. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0600, Time Spent: 6706s\n",
      "Next Learning Rate: 0.0008500527551517788\n",
      "233. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.0724, Time Spent: 6734s\n",
      "Next Learning Rate: 0.0008494577182231725\n",
      "234. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0695, Time Spent: 6763s\n",
      "Next Learning Rate: 0.0008488630978204163\n",
      "235. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0876, Time Spent: 6792s\n",
      "Next Learning Rate: 0.000848268893651942\n",
      "236. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0829, Time Spent: 6821s\n",
      "Next Learning Rate: 0.0008476751054263856\n",
      "237. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0703, Time Spent: 6850s\n",
      "Next Learning Rate: 0.0008470817328525871\n",
      "238. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0733, Time Spent: 6878s\n",
      "Next Learning Rate: 0.0008464887756395903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0500, Time Spent: 6907s\n",
      "Next Learning Rate: 0.0008458962334966425\n",
      "240. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0703, Time Spent: 6936s\n",
      "Next Learning Rate: 0.0008453041061331949\n",
      "241. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0764, Time Spent: 6964s\n",
      "Next Learning Rate: 0.0008447123932589016\n",
      "242. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0893, Time Spent: 6993s\n",
      "Next Learning Rate: 0.0008441210945836204\n",
      "243. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0441, Time Spent: 7022s\n",
      "Next Learning Rate: 0.0008435302098174118\n",
      "244. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0695, Time Spent: 7051s\n",
      "Next Learning Rate: 0.0008429397386705396\n",
      "245. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0681, Time Spent: 7079s\n",
      "Next Learning Rate: 0.0008423496808534701\n",
      "246. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0493, Time Spent: 7108s\n",
      "Next Learning Rate: 0.0008417600360768727\n",
      "247. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0582, Time Spent: 7137s\n",
      "Next Learning Rate: 0.0008411708040516188\n",
      "248. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0734, Time Spent: 7166s\n",
      "Next Learning Rate: 0.0008405819844887827\n",
      "249. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0795, Time Spent: 7194s\n",
      "Next Learning Rate: 0.0008399935770996405\n",
      "250. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0388, Time Spent: 7223s\n",
      "Next Learning Rate: 0.0008394055815956707\n",
      "251. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0745, Time Spent: 7252s\n",
      "Next Learning Rate: 0.0008388179976885537\n",
      "252. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0886, Time Spent: 7280s\n",
      "Next Learning Rate: 0.0008382308250901717\n",
      "253. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0563, Time Spent: 7309s\n",
      "Next Learning Rate: 0.0008376440635126085\n",
      "254. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0663, Time Spent: 7338s\n",
      "Next Learning Rate: 0.0008370577126681496\n",
      "255. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0511, Time Spent: 7366s\n",
      "Next Learning Rate: 0.0008364717722692819\n",
      "256. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0553, Time Spent: 7395s\n",
      "Next Learning Rate: 0.0008358862420286934\n",
      "257. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0749, Time Spent: 7424s\n",
      "Next Learning Rate: 0.0008353011216592732\n",
      "258. 1000/1011 scrambles genned. Solvable algs genned: 11\n",
      "Average Loss: 2.0782, Time Spent: 7453s\n",
      "Next Learning Rate: 0.0008347164108741117\n",
      "259. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0613, Time Spent: 7481s\n",
      "Next Learning Rate: 0.0008341321093864999\n",
      "260. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0581, Time Spent: 7510s\n",
      "Next Learning Rate: 0.0008335482169099293\n",
      "261. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0583, Time Spent: 7539s\n",
      "Next Learning Rate: 0.0008329647331580923\n",
      "262. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0792, Time Spent: 7568s\n",
      "Next Learning Rate: 0.0008323816578448816\n",
      "263. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0390, Time Spent: 7596s\n",
      "Next Learning Rate: 0.0008317989906843902\n",
      "264. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0495, Time Spent: 7625s\n",
      "Next Learning Rate: 0.0008312167313909111\n",
      "265. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0548, Time Spent: 7654s\n",
      "Next Learning Rate: 0.0008306348796789375\n",
      "266. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0593, Time Spent: 7683s\n",
      "Next Learning Rate: 0.0008300534352631622\n",
      "267. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0529, Time Spent: 7711s\n",
      "Next Learning Rate: 0.000829472397858478\n",
      "268. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0563, Time Spent: 7740s\n",
      "Next Learning Rate: 0.000828891767179977\n",
      "269. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0486, Time Spent: 7769s\n",
      "Next Learning Rate: 0.000828311542942951\n",
      "270. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0434, Time Spent: 7797s\n",
      "Next Learning Rate: 0.0008277317248628909\n",
      "271. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0487, Time Spent: 7826s\n",
      "Next Learning Rate: 0.0008271523126554868\n",
      "272. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0482, Time Spent: 7855s\n",
      "Next Learning Rate: 0.0008265733060366279\n",
      "273. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0543, Time Spent: 7884s\n",
      "Next Learning Rate: 0.0008259947047224023\n",
      "274. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0701, Time Spent: 7913s\n",
      "Next Learning Rate: 0.0008254165084290966\n",
      "275. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0366, Time Spent: 7942s\n",
      "Next Learning Rate: 0.0008248387168731962\n",
      "276. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0697, Time Spent: 7971s\n",
      "Next Learning Rate: 0.0008242613297713849\n",
      "277. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0478, Time Spent: 8000s\n",
      "Next Learning Rate: 0.0008236843468405449\n",
      "278. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0519, Time Spent: 8028s\n",
      "Next Learning Rate: 0.0008231077677977565\n",
      "279. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 2.0700, Time Spent: 8057s\n",
      "Next Learning Rate: 0.000822531592360298\n",
      "280. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0316, Time Spent: 8086s\n",
      "Next Learning Rate: 0.0008219558202456458\n",
      "281. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0527, Time Spent: 8115s\n",
      "Next Learning Rate: 0.0008213804511714738\n",
      "282. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 2.0724, Time Spent: 8144s\n",
      "Next Learning Rate: 0.0008208054848556538\n",
      "283. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0393, Time Spent: 8173s\n",
      "Next Learning Rate: 0.0008202309210162547\n",
      "284. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0551, Time Spent: 8202s\n",
      "Next Learning Rate: 0.0008196567593715433\n",
      "285. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0611, Time Spent: 8230s\n",
      "Next Learning Rate: 0.0008190829996399832\n",
      "286. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0521, Time Spent: 8259s\n",
      "Next Learning Rate: 0.0008185096415402351\n",
      "287. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0491, Time Spent: 8288s\n",
      "Next Learning Rate: 0.000817936684791157\n",
      "288. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0403, Time Spent: 8317s\n",
      "Next Learning Rate: 0.0008173641291118031\n",
      "289. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0285, Time Spent: 8346s\n",
      "Next Learning Rate: 0.0008167919742214248\n",
      "290. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0619, Time Spent: 8374s\n",
      "Next Learning Rate: 0.0008162202198394698\n",
      "291. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0214, Time Spent: 8403s\n",
      "Next Learning Rate: 0.0008156488656855821\n",
      "292. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0324, Time Spent: 8432s\n",
      "Next Learning Rate: 0.0008150779114796022\n",
      "293. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0279, Time Spent: 8461s\n",
      "Next Learning Rate: 0.0008145073569415664\n",
      "294. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0449, Time Spent: 8489s\n",
      "Next Learning Rate: 0.0008139372017917073\n",
      "295. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 2.0503, Time Spent: 8518s\n",
      "Next Learning Rate: 0.0008133674457504531\n",
      "296. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0074, Time Spent: 8547s\n",
      "Next Learning Rate: 0.0008127980885384278\n",
      "297. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0540, Time Spent: 8576s\n",
      "Next Learning Rate: 0.0008122291298764509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0417, Time Spent: 8605s\n",
      "Next Learning Rate: 0.0008116605694855374\n",
      "299. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0655, Time Spent: 8633s\n",
      "Next Learning Rate: 0.0008110924070868974\n",
      "300. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0637, Time Spent: 8662s\n",
      "Next Learning Rate: 0.0008105246424019365\n",
      "301. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0316, Time Spent: 8691s\n",
      "Next Learning Rate: 0.0008099572751522552\n",
      "302. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0806, Time Spent: 8719s\n",
      "Next Learning Rate: 0.0008093903050596485\n",
      "303. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.0291, Time Spent: 8748s\n",
      "Next Learning Rate: 0.0008088237318461068\n",
      "304. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0376, Time Spent: 8777s\n",
      "Next Learning Rate: 0.0008082575552338145\n",
      "305. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0631, Time Spent: 8805s\n",
      "Next Learning Rate: 0.0008076917749451508\n",
      "306. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0465, Time Spent: 8834s\n",
      "Next Learning Rate: 0.0008071263907026892\n",
      "307. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0102, Time Spent: 8863s\n",
      "Next Learning Rate: 0.0008065614022291972\n",
      "308. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0406, Time Spent: 8892s\n",
      "Next Learning Rate: 0.0008059968092476368\n",
      "309. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0303, Time Spent: 8921s\n",
      "Next Learning Rate: 0.0008054326114811635\n",
      "310. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0282, Time Spent: 8949s\n",
      "Next Learning Rate: 0.0008048688086531266\n",
      "311. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0298, Time Spent: 8978s\n",
      "Next Learning Rate: 0.0008043054004870693\n",
      "312. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0419, Time Spent: 9007s\n",
      "Next Learning Rate: 0.0008037423867067284\n",
      "313. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0404, Time Spent: 9036s\n",
      "Next Learning Rate: 0.0008031797670360336\n",
      "314. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0489, Time Spent: 9064s\n",
      "Next Learning Rate: 0.0008026175411991084\n",
      "315. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0410, Time Spent: 9093s\n",
      "Next Learning Rate: 0.0008020557089202689\n",
      "316. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0502, Time Spent: 9122s\n",
      "Next Learning Rate: 0.0008014942699240247\n",
      "317. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0259, Time Spent: 9151s\n",
      "Next Learning Rate: 0.0008009332239350778\n",
      "318. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0154, Time Spent: 9179s\n",
      "Next Learning Rate: 0.0008003725706783232\n",
      "319. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0511, Time Spent: 9208s\n",
      "Next Learning Rate: 0.0007998123098788484\n",
      "320. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0072, Time Spent: 9237s\n",
      "Next Learning Rate: 0.0007992524412619332\n",
      "321. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0208, Time Spent: 9266s\n",
      "Next Learning Rate: 0.0007986929645530498\n",
      "322. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0293, Time Spent: 9295s\n",
      "Next Learning Rate: 0.0007981338794778626\n",
      "323. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0171, Time Spent: 9324s\n",
      "Next Learning Rate: 0.0007975751857622281\n",
      "324. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0290, Time Spent: 9353s\n",
      "Next Learning Rate: 0.0007970168831321945\n",
      "325. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0154, Time Spent: 9381s\n",
      "Next Learning Rate: 0.000796458971314002\n",
      "326. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0149, Time Spent: 9410s\n",
      "Next Learning Rate: 0.0007959014500340822\n",
      "327. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0600, Time Spent: 9439s\n",
      "Next Learning Rate: 0.0007953443190190583\n",
      "328. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0201, Time Spent: 9468s\n",
      "Next Learning Rate: 0.0007947875779957449\n",
      "329. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0561, Time Spent: 9497s\n",
      "Next Learning Rate: 0.0007942312266911479\n",
      "330. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 2.0354, Time Spent: 9526s\n",
      "Next Learning Rate: 0.0007936752648324641\n",
      "331. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0404, Time Spent: 9555s\n",
      "Next Learning Rate: 0.0007931196921470813\n",
      "332. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0313, Time Spent: 9583s\n",
      "Next Learning Rate: 0.0007925645083625783\n",
      "333. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0313, Time Spent: 9612s\n",
      "Next Learning Rate: 0.0007920097132067245\n",
      "334. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0167, Time Spent: 9641s\n",
      "Next Learning Rate: 0.0007914553064074797\n",
      "335. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0429, Time Spent: 9670s\n",
      "Next Learning Rate: 0.0007909012876929944\n",
      "336. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0286, Time Spent: 9698s\n",
      "Next Learning Rate: 0.0007903476567916093\n",
      "337. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0345, Time Spent: 9727s\n",
      "Next Learning Rate: 0.0007897944134318552\n",
      "338. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0726, Time Spent: 9756s\n",
      "Next Learning Rate: 0.0007892415573424529\n",
      "339. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0219, Time Spent: 9785s\n",
      "Next Learning Rate: 0.0007886890882523131\n",
      "340. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0041, Time Spent: 9814s\n",
      "Next Learning Rate: 0.0007881370058905365\n",
      "341. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0309, Time Spent: 9843s\n",
      "Next Learning Rate: 0.0007875853099864131\n",
      "342. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0295, Time Spent: 9871s\n",
      "Next Learning Rate: 0.0007870340002694225\n",
      "343. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0084, Time Spent: 9900s\n",
      "Next Learning Rate: 0.0007864830764692339\n",
      "344. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0253, Time Spent: 9929s\n",
      "Next Learning Rate: 0.0007859325383157054\n",
      "345. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0304, Time Spent: 9958s\n",
      "Next Learning Rate: 0.0007853823855388844\n",
      "346. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0223, Time Spent: 9986s\n",
      "Next Learning Rate: 0.0007848326178690071\n",
      "347. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0188, Time Spent: 10015s\n",
      "Next Learning Rate: 0.0007842832350364988\n",
      "348. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0181, Time Spent: 10044s\n",
      "Next Learning Rate: 0.0007837342367719732\n",
      "349. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0254, Time Spent: 10073s\n",
      "Next Learning Rate: 0.0007831856228062328\n",
      "350. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0277, Time Spent: 10102s\n",
      "Next Learning Rate: 0.0007826373928702684\n",
      "351. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0148, Time Spent: 10131s\n",
      "Next Learning Rate: 0.0007820895466952592\n",
      "352. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0254, Time Spent: 10159s\n",
      "Next Learning Rate: 0.0007815420840125725\n",
      "353. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0011, Time Spent: 10188s\n",
      "Next Learning Rate: 0.0007809950045537637\n",
      "354. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0159, Time Spent: 10217s\n",
      "Next Learning Rate: 0.000780448308050576\n",
      "355. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0115, Time Spent: 10246s\n",
      "Next Learning Rate: 0.0007799019942349406\n",
      "356. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0227, Time Spent: 10274s\n",
      "Next Learning Rate: 0.0007793560628389761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 2.0432, Time Spent: 10303s\n",
      "Next Learning Rate: 0.0007788105135949888\n",
      "358. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0154, Time Spent: 10332s\n",
      "Next Learning Rate: 0.0007782653462354722\n",
      "359. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9966, Time Spent: 10361s\n",
      "Next Learning Rate: 0.0007777205604931074\n",
      "360. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9984, Time Spent: 10390s\n",
      "Next Learning Rate: 0.0007771761561007622\n",
      "361. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.0217, Time Spent: 10418s\n",
      "Next Learning Rate: 0.0007766321327914916\n",
      "362. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0302, Time Spent: 10447s\n",
      "Next Learning Rate: 0.0007760884902985375\n",
      "363. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 2.0219, Time Spent: 10476s\n",
      "Next Learning Rate: 0.0007755452283553286\n",
      "364. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0273, Time Spent: 10505s\n",
      "Next Learning Rate: 0.0007750023466954798\n",
      "365. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0152, Time Spent: 10533s\n",
      "Next Learning Rate: 0.000774459845052793\n",
      "366. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9925, Time Spent: 10562s\n",
      "Next Learning Rate: 0.000773917723161256\n",
      "367. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0366, Time Spent: 10591s\n",
      "Next Learning Rate: 0.000773375980755043\n",
      "368. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0311, Time Spent: 10620s\n",
      "Next Learning Rate: 0.0007728346175685145\n",
      "369. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0413, Time Spent: 10648s\n",
      "Next Learning Rate: 0.0007722936333362164\n",
      "370. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0112, Time Spent: 10677s\n",
      "Next Learning Rate: 0.0007717530277928811\n",
      "371. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9994, Time Spent: 10706s\n",
      "Next Learning Rate: 0.0007712128006734261\n",
      "372. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0238, Time Spent: 10735s\n",
      "Next Learning Rate: 0.0007706729517129547\n",
      "373. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0175, Time Spent: 10764s\n",
      "Next Learning Rate: 0.0007701334806467556\n",
      "374. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0097, Time Spent: 10793s\n",
      "Next Learning Rate: 0.0007695943872103028\n",
      "375. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0042, Time Spent: 10821s\n",
      "Next Learning Rate: 0.0007690556711392556\n",
      "376. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9988, Time Spent: 10850s\n",
      "Next Learning Rate: 0.0007685173321694581\n",
      "377. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0409, Time Spent: 10879s\n",
      "Next Learning Rate: 0.0007679793700369395\n",
      "378. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0229, Time Spent: 10908s\n",
      "Next Learning Rate: 0.0007674417844779136\n",
      "379. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 2.0353, Time Spent: 10937s\n",
      "Next Learning Rate: 0.000766904575228779\n",
      "380. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0098, Time Spent: 10965s\n",
      "Next Learning Rate: 0.0007663677420261188\n",
      "381. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0000, Time Spent: 10994s\n",
      "Next Learning Rate: 0.0007658312846067005\n",
      "382. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0322, Time Spent: 11023s\n",
      "Next Learning Rate: 0.0007652952027074758\n",
      "383. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9973, Time Spent: 11052s\n",
      "Next Learning Rate: 0.0007647594960655806\n",
      "384. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0055, Time Spent: 11080s\n",
      "Next Learning Rate: 0.0007642241644183346\n",
      "385. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0223, Time Spent: 11109s\n",
      "Next Learning Rate: 0.0007636892075032417\n",
      "386. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0149, Time Spent: 11140s\n",
      "Next Learning Rate: 0.0007631546250579895\n",
      "387. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0024, Time Spent: 11169s\n",
      "Next Learning Rate: 0.0007626204168204488\n",
      "388. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0180, Time Spent: 11197s\n",
      "Next Learning Rate: 0.0007620865825286744\n",
      "389. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0194, Time Spent: 11226s\n",
      "Next Learning Rate: 0.0007615531219209044\n",
      "390. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0099, Time Spent: 11255s\n",
      "Next Learning Rate: 0.0007610200347355596\n",
      "391. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0123, Time Spent: 11284s\n",
      "Next Learning Rate: 0.0007604873207112447\n",
      "392. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0259, Time Spent: 11313s\n",
      "Next Learning Rate: 0.0007599549795867468\n",
      "393. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0344, Time Spent: 11341s\n",
      "Next Learning Rate: 0.0007594230111010361\n",
      "394. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0285, Time Spent: 11370s\n",
      "Next Learning Rate: 0.0007588914149932653\n",
      "395. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9940, Time Spent: 11399s\n",
      "Next Learning Rate: 0.0007583601910027699\n",
      "396. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9779, Time Spent: 11428s\n",
      "Next Learning Rate: 0.000757829338869068\n",
      "397. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0202, Time Spent: 11456s\n",
      "Next Learning Rate: 0.0007572988583318597\n",
      "398. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0108, Time Spent: 11485s\n",
      "Next Learning Rate: 0.0007567687491310274\n",
      "399. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0152, Time Spent: 11514s\n",
      "Next Learning Rate: 0.0007562390110066356\n",
      "400. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0303, Time Spent: 11545s\n",
      "Next Learning Rate: 0.000755709643698931\n",
      "401. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0100, Time Spent: 11574s\n",
      "Next Learning Rate: 0.0007551806469483417\n",
      "402. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0120, Time Spent: 11603s\n",
      "Next Learning Rate: 0.0007546520204954779\n",
      "403. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0253, Time Spent: 11632s\n",
      "Next Learning Rate: 0.000754123764081131\n",
      "404. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0006, Time Spent: 11661s\n",
      "Next Learning Rate: 0.0007535958774462742\n",
      "405. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0025, Time Spent: 11690s\n",
      "Next Learning Rate: 0.0007530683603320618\n",
      "406. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9954, Time Spent: 11718s\n",
      "Next Learning Rate: 0.0007525412124798293\n",
      "407. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0047, Time Spent: 11747s\n",
      "Next Learning Rate: 0.0007520144336310934\n",
      "408. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9872, Time Spent: 11776s\n",
      "Next Learning Rate: 0.0007514880235275517\n",
      "409. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9953, Time Spent: 11804s\n",
      "Next Learning Rate: 0.0007509619819110824\n",
      "410. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0010, Time Spent: 11833s\n",
      "Next Learning Rate: 0.0007504363085237446\n",
      "411. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9851, Time Spent: 11862s\n",
      "Next Learning Rate: 0.000749911003107778\n",
      "412. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9834, Time Spent: 11891s\n",
      "Next Learning Rate: 0.0007493860654056025\n",
      "413. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9990, Time Spent: 11919s\n",
      "Next Learning Rate: 0.0007488614951598185\n",
      "414. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0139, Time Spent: 11948s\n",
      "Next Learning Rate: 0.0007483372921132066\n",
      "415. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0291, Time Spent: 11977s\n",
      "Next Learning Rate: 0.0007478134560087274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9992, Time Spent: 12005s\n",
      "Next Learning Rate: 0.0007472899865895213\n",
      "417. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0126, Time Spent: 12034s\n",
      "Next Learning Rate: 0.0007467668835989086\n",
      "418. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 2.0169, Time Spent: 12063s\n",
      "Next Learning Rate: 0.0007462441467803893\n",
      "419. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0019, Time Spent: 12092s\n",
      "Next Learning Rate: 0.000745721775877643\n",
      "420. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9950, Time Spent: 12120s\n",
      "Next Learning Rate: 0.0007451997706345286\n",
      "421. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9856, Time Spent: 12149s\n",
      "Next Learning Rate: 0.0007446781307950844\n",
      "422. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0126, Time Spent: 12178s\n",
      "Next Learning Rate: 0.0007441568561035279\n",
      "423. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 2.0201, Time Spent: 12207s\n",
      "Next Learning Rate: 0.0007436359463042553\n",
      "424. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9798, Time Spent: 12235s\n",
      "Next Learning Rate: 0.0007431154011418423\n",
      "425. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9933, Time Spent: 12264s\n",
      "Next Learning Rate: 0.000742595220361043\n",
      "426. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0014, Time Spent: 12293s\n",
      "Next Learning Rate: 0.0007420754037067902\n",
      "427. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9995, Time Spent: 12322s\n",
      "Next Learning Rate: 0.0007415559509241955\n",
      "428. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9732, Time Spent: 12351s\n",
      "Next Learning Rate: 0.0007410368617585486\n",
      "429. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0162, Time Spent: 12379s\n",
      "Next Learning Rate: 0.0007405181359553175\n",
      "430. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9962, Time Spent: 12408s\n",
      "Next Learning Rate: 0.0007399997732601488\n",
      "431. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9913, Time Spent: 12437s\n",
      "Next Learning Rate: 0.0007394817734188667\n",
      "432. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0032, Time Spent: 12466s\n",
      "Next Learning Rate: 0.0007389641361774734\n",
      "433. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9961, Time Spent: 12494s\n",
      "Next Learning Rate: 0.0007384468612821491\n",
      "434. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9992, Time Spent: 12523s\n",
      "Next Learning Rate: 0.0007379299484792516\n",
      "435. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9948, Time Spent: 12552s\n",
      "Next Learning Rate: 0.0007374133975153162\n",
      "436. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9945, Time Spent: 12581s\n",
      "Next Learning Rate: 0.0007368972081370554\n",
      "437. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0139, Time Spent: 12609s\n",
      "Next Learning Rate: 0.0007363813800913594\n",
      "438. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0092, Time Spent: 12638s\n",
      "Next Learning Rate: 0.0007358659131252954\n",
      "439. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 2.0265, Time Spent: 12667s\n",
      "Next Learning Rate: 0.0007353508069861077\n",
      "440. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9914, Time Spent: 12696s\n",
      "Next Learning Rate: 0.0007348360614212174\n",
      "441. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 2.0056, Time Spent: 12725s\n",
      "Next Learning Rate: 0.0007343216761782225\n",
      "442. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9876, Time Spent: 12753s\n",
      "Next Learning Rate: 0.0007338076510048977\n",
      "443. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0163, Time Spent: 12782s\n",
      "Next Learning Rate: 0.0007332939856491943\n",
      "444. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0000, Time Spent: 12811s\n",
      "Next Learning Rate: 0.0007327806798592398\n",
      "445. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9838, Time Spent: 12839s\n",
      "Next Learning Rate: 0.0007322677333833383\n",
      "446. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9971, Time Spent: 12868s\n",
      "Next Learning Rate: 0.00073175514596997\n",
      "447. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9850, Time Spent: 12897s\n",
      "Next Learning Rate: 0.000731242917367791\n",
      "448. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9742, Time Spent: 12926s\n",
      "Next Learning Rate: 0.0007307310473256335\n",
      "449. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9843, Time Spent: 12955s\n",
      "Next Learning Rate: 0.0007302195355925055\n",
      "450. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0152, Time Spent: 12983s\n",
      "Next Learning Rate: 0.0007297083819175907\n",
      "451. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0095, Time Spent: 13012s\n",
      "Next Learning Rate: 0.0007291975860502484\n",
      "452. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9770, Time Spent: 13041s\n",
      "Next Learning Rate: 0.0007286871477400131\n",
      "453. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9657, Time Spent: 13070s\n",
      "Next Learning Rate: 0.0007281770667365951\n",
      "454. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9897, Time Spent: 13099s\n",
      "Next Learning Rate: 0.0007276673427898795\n",
      "455. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9830, Time Spent: 13128s\n",
      "Next Learning Rate: 0.0007271579756499265\n",
      "456. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9797, Time Spent: 13156s\n",
      "Next Learning Rate: 0.0007266489650669715\n",
      "457. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 2.0070, Time Spent: 13184s\n",
      "Next Learning Rate: 0.0007261403107914246\n",
      "458. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9809, Time Spent: 13213s\n",
      "Next Learning Rate: 0.0007256320125738706\n",
      "459. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0219, Time Spent: 13242s\n",
      "Next Learning Rate: 0.0007251240701650688\n",
      "460. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9916, Time Spent: 13271s\n",
      "Next Learning Rate: 0.0007246164833159532\n",
      "461. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9741, Time Spent: 13299s\n",
      "Next Learning Rate: 0.000724109251777632\n",
      "462. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9858, Time Spent: 13328s\n",
      "Next Learning Rate: 0.0007236023753013876\n",
      "463. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9745, Time Spent: 13357s\n",
      "Next Learning Rate: 0.0007230958536386766\n",
      "464. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9830, Time Spent: 13386s\n",
      "Next Learning Rate: 0.0007225896865411294\n",
      "465. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9836, Time Spent: 13415s\n",
      "Next Learning Rate: 0.0007220838737605507\n",
      "466. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9714, Time Spent: 13444s\n",
      "Next Learning Rate: 0.0007215784150489183\n",
      "467. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9860, Time Spent: 13472s\n",
      "Next Learning Rate: 0.000721073310158384\n",
      "468. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 2.0008, Time Spent: 13501s\n",
      "Next Learning Rate: 0.0007205685588412732\n",
      "469. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0065, Time Spent: 13530s\n",
      "Next Learning Rate: 0.0007200641608500842\n",
      "470. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9628, Time Spent: 13559s\n",
      "Next Learning Rate: 0.0007195601159374892\n",
      "471. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9926, Time Spent: 13587s\n",
      "Next Learning Rate: 0.0007190564238563329\n",
      "472. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9662, Time Spent: 13616s\n",
      "Next Learning Rate: 0.0007185530843596335\n",
      "473. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9982, Time Spent: 13645s\n",
      "Next Learning Rate: 0.0007180500972005817\n",
      "474. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9928, Time Spent: 13674s\n",
      "Next Learning Rate: 0.0007175474621325412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 2.0192, Time Spent: 13702s\n",
      "Next Learning Rate: 0.0007170451789090484\n",
      "476. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9938, Time Spent: 13731s\n",
      "Next Learning Rate: 0.0007165432472838121\n",
      "477. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9788, Time Spent: 13760s\n",
      "Next Learning Rate: 0.0007160416670107134\n",
      "478. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9928, Time Spent: 13789s\n",
      "Next Learning Rate: 0.0007155404378438059\n",
      "479. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9938, Time Spent: 13818s\n",
      "Next Learning Rate: 0.0007150395595373152\n",
      "480. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9738, Time Spent: 13847s\n",
      "Next Learning Rate: 0.0007145390318456391\n",
      "481. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9721, Time Spent: 13875s\n",
      "Next Learning Rate: 0.0007140388545233471\n",
      "482. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9762, Time Spent: 13904s\n",
      "Next Learning Rate: 0.0007135390273251808\n",
      "483. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 2.0054, Time Spent: 13933s\n",
      "Next Learning Rate: 0.0007130395500060531\n",
      "484. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9361, Time Spent: 13962s\n",
      "Next Learning Rate: 0.0007125404223210489\n",
      "485. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9708, Time Spent: 13991s\n",
      "Next Learning Rate: 0.0007120416440254242\n",
      "486. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9709, Time Spent: 14020s\n",
      "Next Learning Rate: 0.0007115432148746064\n",
      "487. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9831, Time Spent: 14048s\n",
      "Next Learning Rate: 0.0007110451346241942\n",
      "488. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9878, Time Spent: 14077s\n",
      "Next Learning Rate: 0.0007105474030299572\n",
      "489. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 1.9658, Time Spent: 14106s\n",
      "Next Learning Rate: 0.0007100500198478362\n",
      "490. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9909, Time Spent: 14135s\n",
      "Next Learning Rate: 0.0007095529848339427\n",
      "491. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9667, Time Spent: 14163s\n",
      "Next Learning Rate: 0.0007090562977445589\n",
      "492. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9793, Time Spent: 14192s\n",
      "Next Learning Rate: 0.0007085599583361378\n",
      "493. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9750, Time Spent: 14221s\n",
      "Next Learning Rate: 0.0007080639663653024\n",
      "494. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9870, Time Spent: 14250s\n",
      "Next Learning Rate: 0.0007075683215888467\n",
      "495. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9757, Time Spent: 14279s\n",
      "Next Learning Rate: 0.0007070730237637345\n",
      "496. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9536, Time Spent: 14308s\n",
      "Next Learning Rate: 0.0007065780726470998\n",
      "497. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9938, Time Spent: 14337s\n",
      "Next Learning Rate: 0.0007060834679962468\n",
      "498. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9763, Time Spent: 14365s\n",
      "Next Learning Rate: 0.0007055892095686494\n",
      "499. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9675, Time Spent: 14394s\n",
      "Next Learning Rate: 0.0007050952971219513\n",
      "500. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9629, Time Spent: 14423s\n",
      "Next Learning Rate: 0.0007046017304139659\n",
      "501. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9979, Time Spent: 14452s\n",
      "Next Learning Rate: 0.0007041085092026761\n",
      "502. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9811, Time Spent: 14481s\n",
      "Next Learning Rate: 0.0007036156332462342\n",
      "503. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9689, Time Spent: 14509s\n",
      "Next Learning Rate: 0.0007031231023029618\n",
      "504. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0142, Time Spent: 14538s\n",
      "Next Learning Rate: 0.0007026309161313497\n",
      "505. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9703, Time Spent: 14567s\n",
      "Next Learning Rate: 0.0007021390744900577\n",
      "506. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9872, Time Spent: 14595s\n",
      "Next Learning Rate: 0.0007016475771379146\n",
      "507. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9678, Time Spent: 14624s\n",
      "Next Learning Rate: 0.000701156423833918\n",
      "508. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 2.0000, Time Spent: 14653s\n",
      "Next Learning Rate: 0.0007006656143372342\n",
      "509. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9463, Time Spent: 14682s\n",
      "Next Learning Rate: 0.0007001751484071981\n",
      "510. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9911, Time Spent: 14711s\n",
      "Next Learning Rate: 0.0006996850258033131\n",
      "511. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9649, Time Spent: 14739s\n",
      "Next Learning Rate: 0.0006991952462852508\n",
      "512. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9557, Time Spent: 14768s\n",
      "Next Learning Rate: 0.000698705809612851\n",
      "513. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9686, Time Spent: 14797s\n",
      "Next Learning Rate: 0.000698216715546122\n",
      "514. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9886, Time Spent: 14826s\n",
      "Next Learning Rate: 0.0006977279638452397\n",
      "515. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9533, Time Spent: 14854s\n",
      "Next Learning Rate: 0.0006972395542705481\n",
      "516. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9597, Time Spent: 14883s\n",
      "Next Learning Rate: 0.0006967514865825587\n",
      "517. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9706, Time Spent: 14912s\n",
      "Next Learning Rate: 0.0006962637605419508\n",
      "518. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9650, Time Spent: 14941s\n",
      "Next Learning Rate: 0.0006957763759095714\n",
      "519. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9873, Time Spent: 14970s\n",
      "Next Learning Rate: 0.0006952893324464347\n",
      "520. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9691, Time Spent: 14999s\n",
      "Next Learning Rate: 0.0006948026299137222\n",
      "521. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9841, Time Spent: 15027s\n",
      "Next Learning Rate: 0.0006943162680727826\n",
      "522. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9883, Time Spent: 15056s\n",
      "Next Learning Rate: 0.0006938302466851316\n",
      "523. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9744, Time Spent: 15085s\n",
      "Next Learning Rate: 0.000693344565512452\n",
      "524. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9740, Time Spent: 15114s\n",
      "Next Learning Rate: 0.0006928592243165933\n",
      "525. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9858, Time Spent: 15143s\n",
      "Next Learning Rate: 0.0006923742228595717\n",
      "526. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9903, Time Spent: 15171s\n",
      "Next Learning Rate: 0.0006918895609035699\n",
      "527. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9572, Time Spent: 15200s\n",
      "Next Learning Rate: 0.0006914052382109374\n",
      "528. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9572, Time Spent: 15229s\n",
      "Next Learning Rate: 0.0006909212545441897\n",
      "529. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9784, Time Spent: 15257s\n",
      "Next Learning Rate: 0.0006904376096660088\n",
      "530. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9744, Time Spent: 15286s\n",
      "Next Learning Rate: 0.0006899543033392425\n",
      "531. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9608, Time Spent: 15315s\n",
      "Next Learning Rate: 0.000689471335326905\n",
      "532. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9517, Time Spent: 15344s\n",
      "Next Learning Rate: 0.0006889887053921762\n",
      "533. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9536, Time Spent: 15373s\n",
      "Next Learning Rate: 0.0006885064132984017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9788, Time Spent: 15402s\n",
      "Next Learning Rate: 0.0006880244588090928\n",
      "535. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9950, Time Spent: 15430s\n",
      "Next Learning Rate: 0.0006875428416879264\n",
      "536. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9776, Time Spent: 15459s\n",
      "Next Learning Rate: 0.0006870615616987448\n",
      "537. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9818, Time Spent: 15488s\n",
      "Next Learning Rate: 0.0006865806186055557\n",
      "538. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9386, Time Spent: 15517s\n",
      "Next Learning Rate: 0.0006861000121725318\n",
      "539. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9754, Time Spent: 15545s\n",
      "Next Learning Rate: 0.000685619742164011\n",
      "540. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9565, Time Spent: 15574s\n",
      "Next Learning Rate: 0.0006851398083444961\n",
      "541. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9946, Time Spent: 15603s\n",
      "Next Learning Rate: 0.0006846602104786549\n",
      "542. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9586, Time Spent: 15631s\n",
      "Next Learning Rate: 0.0006841809483313199\n",
      "543. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9540, Time Spent: 15660s\n",
      "Next Learning Rate: 0.0006837020216674879\n",
      "544. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9810, Time Spent: 15689s\n",
      "Next Learning Rate: 0.0006832234302523206\n",
      "545. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9620, Time Spent: 15718s\n",
      "Next Learning Rate: 0.000682745173851144\n",
      "546. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9500, Time Spent: 15746s\n",
      "Next Learning Rate: 0.0006822672522294481\n",
      "547. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9717, Time Spent: 15775s\n",
      "Next Learning Rate: 0.0006817896651528874\n",
      "548. 1000/1010 scrambles genned. Solvable algs genned: 10\n",
      "Average Loss: 1.9688, Time Spent: 15804s\n",
      "Next Learning Rate: 0.0006813124123872804\n",
      "549. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9836, Time Spent: 15832s\n",
      "Next Learning Rate: 0.0006808354936986092\n",
      "550. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9454, Time Spent: 15861s\n",
      "Next Learning Rate: 0.0006803589088530201\n",
      "551. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9699, Time Spent: 15890s\n",
      "Next Learning Rate: 0.000679882657616823\n",
      "552. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 2.0039, Time Spent: 15919s\n",
      "Next Learning Rate: 0.0006794067397564912\n",
      "553. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9691, Time Spent: 15947s\n",
      "Next Learning Rate: 0.0006789311550386616\n",
      "554. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9626, Time Spent: 15976s\n",
      "Next Learning Rate: 0.0006784559032301346\n",
      "555. 1000/1012 scrambles genned. Solvable algs genned: 12\n",
      "Average Loss: 1.9527, Time Spent: 16005s\n",
      "Next Learning Rate: 0.0006779809840978734\n",
      "556. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9567, Time Spent: 16033s\n",
      "Next Learning Rate: 0.0006775063974090048\n",
      "557. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9638, Time Spent: 16062s\n",
      "Next Learning Rate: 0.0006770321429308185\n",
      "558. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9551, Time Spent: 16091s\n",
      "Next Learning Rate: 0.0006765582204307669\n",
      "559. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9715, Time Spent: 16120s\n",
      "Next Learning Rate: 0.0006760846296764653\n",
      "560. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9745, Time Spent: 16148s\n",
      "Next Learning Rate: 0.0006756113704356918\n",
      "561. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9720, Time Spent: 16177s\n",
      "Next Learning Rate: 0.0006751384424763867\n",
      "562. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9676, Time Spent: 16206s\n",
      "Next Learning Rate: 0.0006746658455666532\n",
      "563. 1000/1010 scrambles genned. Solvable algs genned: 10\n",
      "Average Loss: 1.9696, Time Spent: 16235s\n",
      "Next Learning Rate: 0.0006741935794747565\n",
      "564. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9962, Time Spent: 16264s\n",
      "Next Learning Rate: 0.0006737216439691242\n",
      "565. 1000/1011 scrambles genned. Solvable algs genned: 11\n",
      "Average Loss: 1.9508, Time Spent: 16292s\n",
      "Next Learning Rate: 0.0006732500388183458\n",
      "566. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9705, Time Spent: 16321s\n",
      "Next Learning Rate: 0.000672778763791173\n",
      "567. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9485, Time Spent: 16350s\n",
      "Next Learning Rate: 0.0006723078186565191\n",
      "568. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9651, Time Spent: 16379s\n",
      "Next Learning Rate: 0.0006718372031834595\n",
      "569. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9510, Time Spent: 16408s\n",
      "Next Learning Rate: 0.0006713669171412311\n",
      "570. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9766, Time Spent: 16437s\n",
      "Next Learning Rate: 0.0006708969602992322\n",
      "571. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9847, Time Spent: 16465s\n",
      "Next Learning Rate: 0.0006704273324270227\n",
      "572. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9578, Time Spent: 16494s\n",
      "Next Learning Rate: 0.0006699580332943237\n",
      "573. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9437, Time Spent: 16523s\n",
      "Next Learning Rate: 0.0006694890626710177\n",
      "574. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9474, Time Spent: 16552s\n",
      "Next Learning Rate: 0.000669020420327148\n",
      "575. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9510, Time Spent: 16580s\n",
      "Next Learning Rate: 0.000668552106032919\n",
      "576. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9425, Time Spent: 16609s\n",
      "Next Learning Rate: 0.000668084119558696\n",
      "577. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9748, Time Spent: 16638s\n",
      "Next Learning Rate: 0.0006676164606750049\n",
      "578. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9758, Time Spent: 16667s\n",
      "Next Learning Rate: 0.0006671491291525323\n",
      "579. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9597, Time Spent: 16695s\n",
      "Next Learning Rate: 0.0006666821247621256\n",
      "580. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9504, Time Spent: 16724s\n",
      "Next Learning Rate: 0.000666215447274792\n",
      "581. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9843, Time Spent: 16753s\n",
      "Next Learning Rate: 0.0006657490964616996\n",
      "582. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9605, Time Spent: 16782s\n",
      "Next Learning Rate: 0.0006652830720941764\n",
      "583. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9270, Time Spent: 16811s\n",
      "Next Learning Rate: 0.0006648173739437104\n",
      "584. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9532, Time Spent: 16840s\n",
      "Next Learning Rate: 0.0006643520017819497\n",
      "585. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9689, Time Spent: 16868s\n",
      "Next Learning Rate: 0.0006638869553807023\n",
      "586. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 1.9626, Time Spent: 16897s\n",
      "Next Learning Rate: 0.0006634222345119358\n",
      "587. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9510, Time Spent: 16926s\n",
      "Next Learning Rate: 0.0006629578389477774\n",
      "588. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9419, Time Spent: 16955s\n",
      "Next Learning Rate: 0.000662493768460514\n",
      "589. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9703, Time Spent: 16984s\n",
      "Next Learning Rate: 0.0006620300228225916\n",
      "590. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9406, Time Spent: 17012s\n",
      "Next Learning Rate: 0.0006615666018066158\n",
      "591. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9691, Time Spent: 17041s\n",
      "Next Learning Rate: 0.0006611035051853511\n",
      "592. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9550, Time Spent: 17070s\n",
      "Next Learning Rate: 0.0006606407327317214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9543, Time Spent: 17098s\n",
      "Next Learning Rate: 0.0006601782842188092\n",
      "594. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9769, Time Spent: 17127s\n",
      "Next Learning Rate: 0.000659716159419856\n",
      "595. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9694, Time Spent: 17156s\n",
      "Next Learning Rate: 0.0006592543581082621\n",
      "596. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9682, Time Spent: 17185s\n",
      "Next Learning Rate: 0.0006587928800575863\n",
      "597. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9625, Time Spent: 17214s\n",
      "Next Learning Rate: 0.000658331725041546\n",
      "598. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9720, Time Spent: 17243s\n",
      "Next Learning Rate: 0.0006578708928340169\n",
      "599. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9640, Time Spent: 17272s\n",
      "Next Learning Rate: 0.0006574103832090331\n",
      "600. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9469, Time Spent: 17301s\n",
      "Next Learning Rate: 0.0006569501959407867\n",
      "601. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9370, Time Spent: 17329s\n",
      "Next Learning Rate: 0.0006564903308036281\n",
      "602. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9619, Time Spent: 17358s\n",
      "Next Learning Rate: 0.0006560307875720656\n",
      "603. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9602, Time Spent: 17387s\n",
      "Next Learning Rate: 0.0006555715660207651\n",
      "604. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9360, Time Spent: 17416s\n",
      "Next Learning Rate: 0.0006551126659245505\n",
      "605. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9261, Time Spent: 17445s\n",
      "Next Learning Rate: 0.0006546540870584033\n",
      "606. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9215, Time Spent: 17473s\n",
      "Next Learning Rate: 0.0006541958291974624\n",
      "607. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9691, Time Spent: 17502s\n",
      "Next Learning Rate: 0.0006537378921170242\n",
      "608. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9623, Time Spent: 17531s\n",
      "Next Learning Rate: 0.0006532802755925423\n",
      "609. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9492, Time Spent: 17560s\n",
      "Next Learning Rate: 0.0006528229793996274\n",
      "610. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9507, Time Spent: 17589s\n",
      "Next Learning Rate: 0.0006523660033140476\n",
      "611. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9492, Time Spent: 17618s\n",
      "Next Learning Rate: 0.0006519093471117278\n",
      "612. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9461, Time Spent: 17647s\n",
      "Next Learning Rate: 0.0006514530105687496\n",
      "613. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9564, Time Spent: 17675s\n",
      "Next Learning Rate: 0.0006509969934613515\n",
      "614. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9629, Time Spent: 17704s\n",
      "Next Learning Rate: 0.0006505412955659285\n",
      "615. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9286, Time Spent: 17733s\n",
      "Next Learning Rate: 0.0006500859166590323\n",
      "616. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9527, Time Spent: 17762s\n",
      "Next Learning Rate: 0.000649630856517371\n",
      "617. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9772, Time Spent: 17791s\n",
      "Next Learning Rate: 0.0006491761149178089\n",
      "618. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9297, Time Spent: 17820s\n",
      "Next Learning Rate: 0.0006487216916373664\n",
      "619. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9688, Time Spent: 17849s\n",
      "Next Learning Rate: 0.0006482675864532202\n",
      "620. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9655, Time Spent: 17877s\n",
      "Next Learning Rate: 0.0006478137991427029\n",
      "621. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9709, Time Spent: 17906s\n",
      "Next Learning Rate: 0.000647360329483303\n",
      "622. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9716, Time Spent: 17935s\n",
      "Next Learning Rate: 0.0006469071772526646\n",
      "623. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9498, Time Spent: 17964s\n",
      "Next Learning Rate: 0.0006464543422285878\n",
      "624. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9615, Time Spent: 17993s\n",
      "Next Learning Rate: 0.0006460018241890278\n",
      "625. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9427, Time Spent: 18022s\n",
      "Next Learning Rate: 0.0006455496229120954\n",
      "626. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9393, Time Spent: 18051s\n",
      "Next Learning Rate: 0.0006450977381760569\n",
      "627. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9389, Time Spent: 18080s\n",
      "Next Learning Rate: 0.0006446461697593337\n",
      "628. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9455, Time Spent: 18109s\n",
      "Next Learning Rate: 0.0006441949174405021\n",
      "629. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9611, Time Spent: 18137s\n",
      "Next Learning Rate: 0.0006437439809982938\n",
      "630. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9742, Time Spent: 18166s\n",
      "Next Learning Rate: 0.000643293360211595\n",
      "631. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9291, Time Spent: 18195s\n",
      "Next Learning Rate: 0.0006428430548594469\n",
      "632. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9572, Time Spent: 18224s\n",
      "Next Learning Rate: 0.0006423930647210453\n",
      "633. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9459, Time Spent: 18252s\n",
      "Next Learning Rate: 0.0006419433895757406\n",
      "634. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9597, Time Spent: 18281s\n",
      "Next Learning Rate: 0.0006414940292030375\n",
      "635. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9476, Time Spent: 18310s\n",
      "Next Learning Rate: 0.0006410449833825953\n",
      "636. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9345, Time Spent: 18339s\n",
      "Next Learning Rate: 0.0006405962518942275\n",
      "637. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9540, Time Spent: 18367s\n",
      "Next Learning Rate: 0.0006401478345179015\n",
      "638. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9431, Time Spent: 18396s\n",
      "Next Learning Rate: 0.0006396997310337389\n",
      "639. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9161, Time Spent: 18425s\n",
      "Next Learning Rate: 0.0006392519412220153\n",
      "640. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9526, Time Spent: 18453s\n",
      "Next Learning Rate: 0.0006388044648631599\n",
      "641. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9279, Time Spent: 18482s\n",
      "Next Learning Rate: 0.0006383573017377556\n",
      "642. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9314, Time Spent: 18511s\n",
      "Next Learning Rate: 0.0006379104516265392\n",
      "643. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9455, Time Spent: 18540s\n",
      "Next Learning Rate: 0.0006374639143104006\n",
      "644. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9586, Time Spent: 18568s\n",
      "Next Learning Rate: 0.0006370176895703834\n",
      "645. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9136, Time Spent: 18598s\n",
      "Next Learning Rate: 0.0006365717771876841\n",
      "646. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9274, Time Spent: 18627s\n",
      "Next Learning Rate: 0.0006361261769436526\n",
      "647. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9476, Time Spent: 18655s\n",
      "Next Learning Rate: 0.000635680888619792\n",
      "648. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9478, Time Spent: 18684s\n",
      "Next Learning Rate: 0.0006352359119977582\n",
      "649. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9262, Time Spent: 18713s\n",
      "Next Learning Rate: 0.0006347912468593598\n",
      "650. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9519, Time Spent: 18742s\n",
      "Next Learning Rate: 0.0006343468929865582\n",
      "651. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9467, Time Spent: 18771s\n",
      "Next Learning Rate: 0.0006339028501614676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9388, Time Spent: 18799s\n",
      "Next Learning Rate: 0.0006334591181663546\n",
      "653. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9761, Time Spent: 18828s\n",
      "Next Learning Rate: 0.0006330156967836381\n",
      "654. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9367, Time Spent: 18857s\n",
      "Next Learning Rate: 0.0006325725857958895\n",
      "655. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9230, Time Spent: 18886s\n",
      "Next Learning Rate: 0.0006321297849858323\n",
      "656. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9592, Time Spent: 18915s\n",
      "Next Learning Rate: 0.0006316872941363423\n",
      "657. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9274, Time Spent: 18944s\n",
      "Next Learning Rate: 0.0006312451130304468\n",
      "658. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9075, Time Spent: 18973s\n",
      "Next Learning Rate: 0.0006308032414513254\n",
      "659. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9791, Time Spent: 19002s\n",
      "Next Learning Rate: 0.0006303616791823094\n",
      "660. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9377, Time Spent: 19031s\n",
      "Next Learning Rate: 0.0006299204260068817\n",
      "661. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9430, Time Spent: 19060s\n",
      "Next Learning Rate: 0.0006294794817086769\n",
      "662. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9506, Time Spent: 19088s\n",
      "Next Learning Rate: 0.0006290388460714808\n",
      "663. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9449, Time Spent: 19117s\n",
      "Next Learning Rate: 0.0006285985188792308\n",
      "664. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9545, Time Spent: 19146s\n",
      "Next Learning Rate: 0.0006281584999160152\n",
      "665. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9448, Time Spent: 19175s\n",
      "Next Learning Rate: 0.000627718788966074\n",
      "666. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9487, Time Spent: 19204s\n",
      "Next Learning Rate: 0.0006272793858137977\n",
      "667. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9313, Time Spent: 19233s\n",
      "Next Learning Rate: 0.0006268402902437281\n",
      "668. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9550, Time Spent: 19262s\n",
      "Next Learning Rate: 0.0006264015020405574\n",
      "669. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9488, Time Spent: 19291s\n",
      "Next Learning Rate: 0.000625963020989129\n",
      "670. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9495, Time Spent: 19319s\n",
      "Next Learning Rate: 0.0006255248468744366\n",
      "671. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9346, Time Spent: 19348s\n",
      "Next Learning Rate: 0.0006250869794816245\n",
      "672. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9225, Time Spent: 19377s\n",
      "Next Learning Rate: 0.0006246494185959873\n",
      "673. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9481, Time Spent: 19406s\n",
      "Next Learning Rate: 0.0006242121640029701\n",
      "674. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9718, Time Spent: 19435s\n",
      "Next Learning Rate: 0.000623775215488168\n",
      "675. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9638, Time Spent: 19464s\n",
      "Next Learning Rate: 0.0006233385728373263\n",
      "676. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9396, Time Spent: 19492s\n",
      "Next Learning Rate: 0.0006229022358363401\n",
      "677. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9476, Time Spent: 19521s\n",
      "Next Learning Rate: 0.0006224662042712547\n",
      "678. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9406, Time Spent: 19550s\n",
      "Next Learning Rate: 0.0006220304779282648\n",
      "679. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9325, Time Spent: 19579s\n",
      "Next Learning Rate: 0.0006215950565937149\n",
      "680. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9669, Time Spent: 19607s\n",
      "Next Learning Rate: 0.0006211599400540993\n",
      "681. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9382, Time Spent: 19636s\n",
      "Next Learning Rate: 0.0006207251280960613\n",
      "682. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9643, Time Spent: 19665s\n",
      "Next Learning Rate: 0.0006202906205063941\n",
      "683. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9670, Time Spent: 19694s\n",
      "Next Learning Rate: 0.0006198564170720396\n",
      "684. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9299, Time Spent: 19722s\n",
      "Next Learning Rate: 0.0006194225175800891\n",
      "685. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9186, Time Spent: 19751s\n",
      "Next Learning Rate: 0.000618988921817783\n",
      "686. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9735, Time Spent: 19780s\n",
      "Next Learning Rate: 0.0006185556295725106\n",
      "687. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9536, Time Spent: 19809s\n",
      "Next Learning Rate: 0.0006181226406318098\n",
      "688. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9492, Time Spent: 19838s\n",
      "Next Learning Rate: 0.0006176899547833675\n",
      "689. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9253, Time Spent: 19866s\n",
      "Next Learning Rate: 0.0006172575718150191\n",
      "690. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9457, Time Spent: 19895s\n",
      "Next Learning Rate: 0.0006168254915147486\n",
      "691. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9326, Time Spent: 19924s\n",
      "Next Learning Rate: 0.0006163937136706882\n",
      "692. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9654, Time Spent: 19953s\n",
      "Next Learning Rate: 0.0006159622380711188\n",
      "693. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9576, Time Spent: 19981s\n",
      "Next Learning Rate: 0.0006155310645044689\n",
      "694. 1000/1010 scrambles genned. Solvable algs genned: 10\n",
      "Average Loss: 1.9383, Time Spent: 20010s\n",
      "Next Learning Rate: 0.0006151001927593158\n",
      "695. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9557, Time Spent: 20039s\n",
      "Next Learning Rate: 0.0006146696226243843\n",
      "696. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 1.9474, Time Spent: 20068s\n",
      "Next Learning Rate: 0.0006142393538885472\n",
      "697. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9326, Time Spent: 20097s\n",
      "Next Learning Rate: 0.0006138093863408252\n",
      "698. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9354, Time Spent: 20125s\n",
      "Next Learning Rate: 0.0006133797197703866\n",
      "699. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9339, Time Spent: 20154s\n",
      "Next Learning Rate: 0.0006129503539665473\n",
      "700. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9239, Time Spent: 20183s\n",
      "Next Learning Rate: 0.0006125212887187707\n",
      "701. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9277, Time Spent: 20212s\n",
      "Next Learning Rate: 0.0006120925238166675\n",
      "702. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9480, Time Spent: 20241s\n",
      "Next Learning Rate: 0.0006116640590499958\n",
      "703. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9415, Time Spent: 20270s\n",
      "Next Learning Rate: 0.0006112358942086608\n",
      "704. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9300, Time Spent: 20299s\n",
      "Next Learning Rate: 0.0006108080290827148\n",
      "705. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9155, Time Spent: 20328s\n",
      "Next Learning Rate: 0.0006103804634623568\n",
      "706. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9278, Time Spent: 20356s\n",
      "Next Learning Rate: 0.0006099531971379331\n",
      "707. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9356, Time Spent: 20385s\n",
      "Next Learning Rate: 0.0006095262298999365\n",
      "708. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9450, Time Spent: 20414s\n",
      "Next Learning Rate: 0.0006090995615390066\n",
      "709. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9423, Time Spent: 20443s\n",
      "Next Learning Rate: 0.0006086731918459292\n",
      "710. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9481, Time Spent: 20472s\n",
      "Next Learning Rate: 0.000608247120611637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9308, Time Spent: 20501s\n",
      "Next Learning Rate: 0.0006078213476272088\n",
      "712. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9077, Time Spent: 20530s\n",
      "Next Learning Rate: 0.0006073958726838698\n",
      "713. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9410, Time Spent: 20559s\n",
      "Next Learning Rate: 0.000606970695572991\n",
      "714. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9213, Time Spent: 20588s\n",
      "Next Learning Rate: 0.0006065458160860899\n",
      "715. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9224, Time Spent: 20617s\n",
      "Next Learning Rate: 0.0006061212340148296\n",
      "716. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9522, Time Spent: 20646s\n",
      "Next Learning Rate: 0.0006056969491510191\n",
      "717. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9279, Time Spent: 20675s\n",
      "Next Learning Rate: 0.0006052729612866134\n",
      "718. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9182, Time Spent: 20703s\n",
      "Next Learning Rate: 0.0006048492702137127\n",
      "719. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9121, Time Spent: 20732s\n",
      "Next Learning Rate: 0.0006044258757245631\n",
      "720. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9351, Time Spent: 20761s\n",
      "Next Learning Rate: 0.0006040027776115558\n",
      "721. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9198, Time Spent: 20790s\n",
      "Next Learning Rate: 0.0006035799756672277\n",
      "722. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9205, Time Spent: 20819s\n",
      "Next Learning Rate: 0.0006031574696842607\n",
      "723. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9434, Time Spent: 20848s\n",
      "Next Learning Rate: 0.0006027352594554817\n",
      "724. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9293, Time Spent: 20877s\n",
      "Next Learning Rate: 0.0006023133447738628\n",
      "725. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9332, Time Spent: 20905s\n",
      "Next Learning Rate: 0.0006018917254325211\n",
      "726. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9532, Time Spent: 20934s\n",
      "Next Learning Rate: 0.0006014704012247184\n",
      "727. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9712, Time Spent: 20963s\n",
      "Next Learning Rate: 0.000601049371943861\n",
      "728. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9171, Time Spent: 20992s\n",
      "Next Learning Rate: 0.0006006286373835003\n",
      "729. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9356, Time Spent: 21020s\n",
      "Next Learning Rate: 0.0006002081973373319\n",
      "730. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8987, Time Spent: 21049s\n",
      "Next Learning Rate: 0.0005997880515991957\n",
      "731. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9392, Time Spent: 21078s\n",
      "Next Learning Rate: 0.0005993681999630762\n",
      "732. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9118, Time Spent: 21106s\n",
      "Next Learning Rate: 0.000598948642223102\n",
      "733. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9125, Time Spent: 21135s\n",
      "Next Learning Rate: 0.0005985293781735459\n",
      "734. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9196, Time Spent: 21164s\n",
      "Next Learning Rate: 0.0005981104076088243\n",
      "735. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9271, Time Spent: 21193s\n",
      "Next Learning Rate: 0.0005976917303234981\n",
      "736. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9418, Time Spent: 21222s\n",
      "Next Learning Rate: 0.0005972733461122716\n",
      "737. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9467, Time Spent: 21251s\n",
      "Next Learning Rate: 0.000596855254769993\n",
      "738. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9292, Time Spent: 21279s\n",
      "Next Learning Rate: 0.0005964374560916539\n",
      "739. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9306, Time Spent: 21308s\n",
      "Next Learning Rate: 0.0005960199498723898\n",
      "740. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8903, Time Spent: 21337s\n",
      "Next Learning Rate: 0.000595602735907479\n",
      "741. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9489, Time Spent: 21366s\n",
      "Next Learning Rate: 0.0005951858139923438\n",
      "742. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9298, Time Spent: 21394s\n",
      "Next Learning Rate: 0.0005947691839225492\n",
      "743. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9109, Time Spent: 21423s\n",
      "Next Learning Rate: 0.0005943528454938034\n",
      "744. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9259, Time Spent: 21452s\n",
      "Next Learning Rate: 0.0005939367985019577\n",
      "745. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.8991, Time Spent: 21481s\n",
      "Next Learning Rate: 0.0005935210427430062\n",
      "746. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9451, Time Spent: 21510s\n",
      "Next Learning Rate: 0.0005931055780130861\n",
      "747. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9503, Time Spent: 21539s\n",
      "Next Learning Rate: 0.0005926904041084769\n",
      "748. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9293, Time Spent: 21568s\n",
      "Next Learning Rate: 0.0005922755208256009\n",
      "749. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9307, Time Spent: 21597s\n",
      "Next Learning Rate: 0.000591860927961023\n",
      "750. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9603, Time Spent: 21626s\n",
      "Next Learning Rate: 0.0005914466253114502\n",
      "751. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9164, Time Spent: 21655s\n",
      "Next Learning Rate: 0.0005910326126737322\n",
      "752. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9246, Time Spent: 21684s\n",
      "Next Learning Rate: 0.0005906188898448606\n",
      "753. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9197, Time Spent: 21712s\n",
      "Next Learning Rate: 0.0005902054566219692\n",
      "754. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9247, Time Spent: 21741s\n",
      "Next Learning Rate: 0.0005897923128023337\n",
      "755. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9305, Time Spent: 21770s\n",
      "Next Learning Rate: 0.0005893794581833721\n",
      "756. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9042, Time Spent: 21799s\n",
      "Next Learning Rate: 0.0005889668925626437\n",
      "757. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9121, Time Spent: 21828s\n",
      "Next Learning Rate: 0.0005885546157378498\n",
      "758. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9386, Time Spent: 21857s\n",
      "Next Learning Rate: 0.0005881426275068333\n",
      "759. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9294, Time Spent: 21886s\n",
      "Next Learning Rate: 0.0005877309276675786\n",
      "760. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9418, Time Spent: 21915s\n",
      "Next Learning Rate: 0.0005873195160182112\n",
      "761. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9282, Time Spent: 21944s\n",
      "Next Learning Rate: 0.0005869083923569985\n",
      "762. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9206, Time Spent: 21972s\n",
      "Next Learning Rate: 0.0005864975564823485\n",
      "763. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9281, Time Spent: 22001s\n",
      "Next Learning Rate: 0.0005860870081928109\n",
      "764. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9535, Time Spent: 22030s\n",
      "Next Learning Rate: 0.0005856767472870759\n",
      "765. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9148, Time Spent: 22059s\n",
      "Next Learning Rate: 0.0005852667735639749\n",
      "766. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9291, Time Spent: 22088s\n",
      "Next Learning Rate: 0.0005848570868224801\n",
      "767. 1000/1010 scrambles genned. Solvable algs genned: 10\n",
      "Average Loss: 1.9083, Time Spent: 22117s\n",
      "Next Learning Rate: 0.0005844476868617043\n",
      "768. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9637, Time Spent: 22146s\n",
      "Next Learning Rate: 0.0005840385734809012\n",
      "769. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9145, Time Spent: 22174s\n",
      "Next Learning Rate: 0.0005836297464794645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9176, Time Spent: 22203s\n",
      "Next Learning Rate: 0.0005832212056569289\n",
      "771. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9289, Time Spent: 22232s\n",
      "Next Learning Rate: 0.000582812950812969\n",
      "772. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.8797, Time Spent: 22261s\n",
      "Next Learning Rate: 0.0005824049817473999\n",
      "773. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9031, Time Spent: 22290s\n",
      "Next Learning Rate: 0.0005819972982601767\n",
      "774. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9127, Time Spent: 22319s\n",
      "Next Learning Rate: 0.0005815899001513945\n",
      "775. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9336, Time Spent: 22348s\n",
      "Next Learning Rate: 0.0005811827872212884\n",
      "776. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.8905, Time Spent: 22377s\n",
      "Next Learning Rate: 0.0005807759592702335\n",
      "777. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9266, Time Spent: 22405s\n",
      "Next Learning Rate: 0.0005803694160987443\n",
      "778. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9266, Time Spent: 22434s\n",
      "Next Learning Rate: 0.0005799631575074751\n",
      "779. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9038, Time Spent: 22463s\n",
      "Next Learning Rate: 0.0005795571832972199\n",
      "780. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9548, Time Spent: 22492s\n",
      "Next Learning Rate: 0.0005791514932689118\n",
      "781. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9068, Time Spent: 22521s\n",
      "Next Learning Rate: 0.0005787460872236235\n",
      "782. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8984, Time Spent: 22549s\n",
      "Next Learning Rate: 0.000578340964962567\n",
      "783. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9141, Time Spent: 22578s\n",
      "Next Learning Rate: 0.0005779361262870931\n",
      "784. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9148, Time Spent: 22607s\n",
      "Next Learning Rate: 0.0005775315709986922\n",
      "785. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9075, Time Spent: 22636s\n",
      "Next Learning Rate: 0.0005771272988989931\n",
      "786. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9451, Time Spent: 22664s\n",
      "Next Learning Rate: 0.0005767233097897637\n",
      "787. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9025, Time Spent: 22693s\n",
      "Next Learning Rate: 0.0005763196034729109\n",
      "788. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9288, Time Spent: 22722s\n",
      "Next Learning Rate: 0.0005759161797504799\n",
      "789. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9330, Time Spent: 22751s\n",
      "Next Learning Rate: 0.0005755130384246546\n",
      "790. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9433, Time Spent: 22780s\n",
      "Next Learning Rate: 0.0005751101792977573\n",
      "791. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9463, Time Spent: 22808s\n",
      "Next Learning Rate: 0.0005747076021722488\n",
      "792. 1000/1000 scrambles genned. Solvable algs genned: 0\n",
      "Average Loss: 1.9047, Time Spent: 22837s\n",
      "Next Learning Rate: 0.0005743053068507282\n",
      "793. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9082, Time Spent: 22865s\n",
      "Next Learning Rate: 0.0005739032931359327\n",
      "794. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9413, Time Spent: 22894s\n",
      "Next Learning Rate: 0.0005735015608307376\n",
      "795. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9460, Time Spent: 22923s\n",
      "Next Learning Rate: 0.0005731001097381561\n",
      "796. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9504, Time Spent: 22952s\n",
      "Next Learning Rate: 0.0005726989396613394\n",
      "797. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9232, Time Spent: 22981s\n",
      "Next Learning Rate: 0.0005722980504035764\n",
      "798. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9231, Time Spent: 23010s\n",
      "Next Learning Rate: 0.000571897441768294\n",
      "799. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9179, Time Spent: 23039s\n",
      "Next Learning Rate: 0.0005714971135590562\n",
      "800. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9193, Time Spent: 23068s\n",
      "Next Learning Rate: 0.0005710970655795648\n",
      "801. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9065, Time Spent: 23096s\n",
      "Next Learning Rate: 0.000570697297633659\n",
      "802. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9285, Time Spent: 23125s\n",
      "Next Learning Rate: 0.0005702978095253155\n",
      "803. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9150, Time Spent: 23154s\n",
      "Next Learning Rate: 0.0005698986010586477\n",
      "804. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9274, Time Spent: 23183s\n",
      "Next Learning Rate: 0.0005694996720379067\n",
      "805. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9218, Time Spent: 23211s\n",
      "Next Learning Rate: 0.0005691010222674801\n",
      "806. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9101, Time Spent: 23241s\n",
      "Next Learning Rate: 0.0005687026515518929\n",
      "807. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9521, Time Spent: 23270s\n",
      "Next Learning Rate: 0.0005683045596958066\n",
      "808. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.8980, Time Spent: 23298s\n",
      "Next Learning Rate: 0.0005679067465040195\n",
      "809. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9075, Time Spent: 23327s\n",
      "Next Learning Rate: 0.0005675092117814666\n",
      "810. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9159, Time Spent: 23356s\n",
      "Next Learning Rate: 0.0005671119553332196\n",
      "811. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9154, Time Spent: 23385s\n",
      "Next Learning Rate: 0.0005667149769644863\n",
      "812. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9237, Time Spent: 23414s\n",
      "Next Learning Rate: 0.0005663182764806111\n",
      "813. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9108, Time Spent: 23442s\n",
      "Next Learning Rate: 0.0005659218536870746\n",
      "814. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9027, Time Spent: 23471s\n",
      "Next Learning Rate: 0.0005655257083894936\n",
      "815. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9217, Time Spent: 23500s\n",
      "Next Learning Rate: 0.000565129840393621\n",
      "816. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9013, Time Spent: 23529s\n",
      "Next Learning Rate: 0.0005647342495053454\n",
      "817. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9150, Time Spent: 23558s\n",
      "Next Learning Rate: 0.0005643389355306917\n",
      "818. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9085, Time Spent: 23587s\n",
      "Next Learning Rate: 0.0005639438982758202\n",
      "819. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9475, Time Spent: 23615s\n",
      "Next Learning Rate: 0.000563549137547027\n",
      "820. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9111, Time Spent: 23644s\n",
      "Next Learning Rate: 0.0005631546531507441\n",
      "821. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9221, Time Spent: 23673s\n",
      "Next Learning Rate: 0.0005627604448935385\n",
      "822. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.8888, Time Spent: 23701s\n",
      "Next Learning Rate: 0.0005623665125821131\n",
      "823. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.8880, Time Spent: 23730s\n",
      "Next Learning Rate: 0.0005619728560233055\n",
      "824. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9233, Time Spent: 23759s\n",
      "Next Learning Rate: 0.0005615794750240892\n",
      "825. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.8931, Time Spent: 23788s\n",
      "Next Learning Rate: 0.0005611863693915723\n",
      "826. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9142, Time Spent: 23817s\n",
      "Next Learning Rate: 0.0005607935389329982\n",
      "827. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9149, Time Spent: 23846s\n",
      "Next Learning Rate: 0.0005604009834557451\n",
      "828. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9138, Time Spent: 23875s\n",
      "Next Learning Rate: 0.0005600087027673261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "829. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9008, Time Spent: 23903s\n",
      "Next Learning Rate: 0.0005596166966753889\n",
      "830. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9049, Time Spent: 23932s\n",
      "Next Learning Rate: 0.0005592249649877161\n",
      "831. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9055, Time Spent: 23961s\n",
      "Next Learning Rate: 0.0005588335075122247\n",
      "832. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9035, Time Spent: 23990s\n",
      "Next Learning Rate: 0.0005584423240569662\n",
      "833. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9126, Time Spent: 24019s\n",
      "Next Learning Rate: 0.0005580514144301263\n",
      "834. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9201, Time Spent: 24048s\n",
      "Next Learning Rate: 0.0005576607784400252\n",
      "835. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9304, Time Spent: 24076s\n",
      "Next Learning Rate: 0.0005572704158951172\n",
      "836. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9477, Time Spent: 24105s\n",
      "Next Learning Rate: 0.0005568803266039905\n",
      "837. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.8798, Time Spent: 24134s\n",
      "Next Learning Rate: 0.0005564905103753677\n",
      "838. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9041, Time Spent: 24163s\n",
      "Next Learning Rate: 0.0005561009670181049\n",
      "839. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9355, Time Spent: 24192s\n",
      "Next Learning Rate: 0.0005557116963411922\n",
      "840. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9101, Time Spent: 24221s\n",
      "Next Learning Rate: 0.0005553226981537534\n",
      "841. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9112, Time Spent: 24249s\n",
      "Next Learning Rate: 0.0005549339722650457\n",
      "842. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.8792, Time Spent: 24278s\n",
      "Next Learning Rate: 0.0005545455184844602\n",
      "843. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9239, Time Spent: 24307s\n",
      "Next Learning Rate: 0.000554157336621521\n",
      "844. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9036, Time Spent: 24336s\n",
      "Next Learning Rate: 0.000553769426485886\n",
      "845. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9207, Time Spent: 24365s\n",
      "Next Learning Rate: 0.0005533817878873458\n",
      "846. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9120, Time Spent: 24393s\n",
      "Next Learning Rate: 0.0005529944206358246\n",
      "847. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9071, Time Spent: 24422s\n",
      "Next Learning Rate: 0.0005526073245413794\n",
      "848. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9103, Time Spent: 24451s\n",
      "Next Learning Rate: 0.0005522204994142005\n",
      "849. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8852, Time Spent: 24480s\n",
      "Next Learning Rate: 0.0005518339450646105\n",
      "850. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.8982, Time Spent: 24509s\n",
      "Next Learning Rate: 0.0005514476613030652\n",
      "851. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9071, Time Spent: 24538s\n",
      "Next Learning Rate: 0.0005510616479401531\n",
      "852. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9010, Time Spent: 24566s\n",
      "Next Learning Rate: 0.000550675904786595\n",
      "853. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9333, Time Spent: 24595s\n",
      "Next Learning Rate: 0.0005502904316532443\n",
      "854. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8959, Time Spent: 24624s\n",
      "Next Learning Rate: 0.000549905228351087\n",
      "855. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.8900, Time Spent: 24653s\n",
      "Next Learning Rate: 0.0005495202946912412\n",
      "856. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9275, Time Spent: 24682s\n",
      "Next Learning Rate: 0.0005491356304849573\n",
      "857. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9360, Time Spent: 24711s\n",
      "Next Learning Rate: 0.0005487512355436178\n",
      "858. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9141, Time Spent: 24740s\n",
      "Next Learning Rate: 0.0005483671096787372\n",
      "859. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9135, Time Spent: 24768s\n",
      "Next Learning Rate: 0.0005479832527019622\n",
      "860. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9271, Time Spent: 24797s\n",
      "Next Learning Rate: 0.0005475996644250708\n",
      "861. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.8926, Time Spent: 24826s\n",
      "Next Learning Rate: 0.0005472163446599732\n",
      "862. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.8982, Time Spent: 24855s\n",
      "Next Learning Rate: 0.0005468332932187112\n",
      "863. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9238, Time Spent: 24884s\n",
      "Next Learning Rate: 0.0005464505099134581\n",
      "864. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9214, Time Spent: 24913s\n",
      "Next Learning Rate: 0.0005460679945565186\n",
      "865. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9158, Time Spent: 24942s\n",
      "Next Learning Rate: 0.0005456857469603291\n",
      "866. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9062, Time Spent: 24971s\n",
      "Next Learning Rate: 0.0005453037669374568\n",
      "867. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9066, Time Spent: 24999s\n",
      "Next Learning Rate: 0.0005449220543006006\n",
      "868. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9300, Time Spent: 25028s\n",
      "Next Learning Rate: 0.0005445406088625902\n",
      "869. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9123, Time Spent: 25057s\n",
      "Next Learning Rate: 0.0005441594304363863\n",
      "870. 1000/1010 scrambles genned. Solvable algs genned: 10\n",
      "Average Loss: 1.9008, Time Spent: 25086s\n",
      "Next Learning Rate: 0.0005437785188350808\n",
      "871. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9184, Time Spent: 25114s\n",
      "Next Learning Rate: 0.0005433978738718962\n",
      "872. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.8973, Time Spent: 25143s\n",
      "Next Learning Rate: 0.0005430174953601858\n",
      "873. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9071, Time Spent: 25172s\n",
      "Next Learning Rate: 0.0005426373831134337\n",
      "874. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9216, Time Spent: 25201s\n",
      "Next Learning Rate: 0.0005422575369452542\n",
      "875. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9081, Time Spent: 25230s\n",
      "Next Learning Rate: 0.0005418779566693925\n",
      "876. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8965, Time Spent: 25259s\n",
      "Next Learning Rate: 0.0005414986420997239\n",
      "877. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.8860, Time Spent: 25287s\n",
      "Next Learning Rate: 0.0005411195930502541\n",
      "878. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9031, Time Spent: 25316s\n",
      "Next Learning Rate: 0.000540740809335119\n",
      "879. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9036, Time Spent: 25345s\n",
      "Next Learning Rate: 0.0005403622907685843\n",
      "880. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9305, Time Spent: 25374s\n",
      "Next Learning Rate: 0.0005399840371650463\n",
      "881. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9204, Time Spent: 25403s\n",
      "Next Learning Rate: 0.0005396060483390308\n",
      "882. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8935, Time Spent: 25432s\n",
      "Next Learning Rate: 0.0005392283241051934\n",
      "883. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9138, Time Spent: 25460s\n",
      "Next Learning Rate: 0.0005388508642783197\n",
      "884. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9104, Time Spent: 25489s\n",
      "Next Learning Rate: 0.0005384736686733249\n",
      "885. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9190, Time Spent: 25518s\n",
      "Next Learning Rate: 0.0005380967371052535\n",
      "886. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9080, Time Spent: 25547s\n",
      "Next Learning Rate: 0.0005377200693892798\n",
      "887. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9395, Time Spent: 25576s\n",
      "Next Learning Rate: 0.0005373436653407072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9270, Time Spent: 25604s\n",
      "Next Learning Rate: 0.0005369675247749688\n",
      "889. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9326, Time Spent: 25634s\n",
      "Next Learning Rate: 0.0005365916475076263\n",
      "890. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9345, Time Spent: 25663s\n",
      "Next Learning Rate: 0.000536216033354371\n",
      "891. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.8828, Time Spent: 25692s\n",
      "Next Learning Rate: 0.0005358406821310229\n",
      "892. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.8805, Time Spent: 25721s\n",
      "Next Learning Rate: 0.0005354655936535311\n",
      "893. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.8954, Time Spent: 25750s\n",
      "Next Learning Rate: 0.0005350907677379737\n",
      "894. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.9239, Time Spent: 25778s\n",
      "Next Learning Rate: 0.000534716204200557\n",
      "895. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.8899, Time Spent: 25807s\n",
      "Next Learning Rate: 0.0005343419028576166\n",
      "896. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9005, Time Spent: 25836s\n",
      "Next Learning Rate: 0.0005339678635256162\n",
      "897. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9045, Time Spent: 25865s\n",
      "Next Learning Rate: 0.0005335940860211483\n",
      "898. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9225, Time Spent: 25894s\n",
      "Next Learning Rate: 0.0005332205701609335\n",
      "899. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9447, Time Spent: 25924s\n",
      "Next Learning Rate: 0.0005328473157618208\n",
      "900. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9082, Time Spent: 25953s\n",
      "Next Learning Rate: 0.0005324743226407875\n",
      "901. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.9107, Time Spent: 25982s\n",
      "Next Learning Rate: 0.0005321015906149389\n",
      "902. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9090, Time Spent: 26011s\n",
      "Next Learning Rate: 0.0005317291195015084\n",
      "903. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.9080, Time Spent: 26040s\n",
      "Next Learning Rate: 0.0005313569091178574\n",
      "904. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.8839, Time Spent: 26069s\n",
      "Next Learning Rate: 0.0005309849592814749\n",
      "905. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9183, Time Spent: 26098s\n",
      "Next Learning Rate: 0.0005306132698099778\n",
      "906. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.8893, Time Spent: 26126s\n",
      "Next Learning Rate: 0.0005302418405211108\n",
      "907. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9033, Time Spent: 26155s\n",
      "Next Learning Rate: 0.000529870671232746\n",
      "908. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9189, Time Spent: 26184s\n",
      "Next Learning Rate: 0.0005294997617628831\n",
      "909. 1000/1011 scrambles genned. Solvable algs genned: 11\n",
      "Average Loss: 1.8928, Time Spent: 26213s\n",
      "Next Learning Rate: 0.0005291291119296491\n",
      "910. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.8962, Time Spent: 26242s\n",
      "Next Learning Rate: 0.0005287587215512983\n",
      "911. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.8973, Time Spent: 26271s\n",
      "Next Learning Rate: 0.0005283885904462123\n",
      "912. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9117, Time Spent: 26300s\n",
      "Next Learning Rate: 0.0005280187184329\n",
      "913. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.9103, Time Spent: 26329s\n",
      "Next Learning Rate: 0.0005276491053299969\n",
      "914. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8945, Time Spent: 26358s\n",
      "Next Learning Rate: 0.0005272797509562659\n",
      "915. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.8989, Time Spent: 26387s\n",
      "Next Learning Rate: 0.0005269106551305965\n",
      "916. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.9010, Time Spent: 26416s\n",
      "Next Learning Rate: 0.000526541817672005\n",
      "917. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.8989, Time Spent: 26445s\n",
      "Next Learning Rate: 0.0005261732383996346\n",
      "918. 1000/1009 scrambles genned. Solvable algs genned: 9\n",
      "Average Loss: 1.9225, Time Spent: 26474s\n",
      "Next Learning Rate: 0.0005258049171327548\n",
      "919. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.8923, Time Spent: 26503s\n",
      "Next Learning Rate: 0.0005254368536907619\n",
      "920. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.8653, Time Spent: 26532s\n",
      "Next Learning Rate: 0.0005250690478931783\n",
      "921. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9108, Time Spent: 26560s\n",
      "Next Learning Rate: 0.0005247014995596531\n",
      "922. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8965, Time Spent: 26589s\n",
      "Next Learning Rate: 0.0005243342085099614\n",
      "923. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9066, Time Spent: 26618s\n",
      "Next Learning Rate: 0.0005239671745640044\n",
      "924. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9245, Time Spent: 26647s\n",
      "Next Learning Rate: 0.0005236003975418095\n",
      "925. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.8943, Time Spent: 26676s\n",
      "Next Learning Rate: 0.0005232338772635303\n",
      "926. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.8902, Time Spent: 26705s\n",
      "Next Learning Rate: 0.0005228676135494458\n",
      "927. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9312, Time Spent: 26734s\n",
      "Next Learning Rate: 0.0005225016062199613\n",
      "928. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9187, Time Spent: 26763s\n",
      "Next Learning Rate: 0.0005221358550956072\n",
      "929. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.8527, Time Spent: 26792s\n",
      "Next Learning Rate: 0.0005217703599970403\n",
      "930. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9105, Time Spent: 26822s\n",
      "Next Learning Rate: 0.0005214051207450423\n",
      "931. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.8974, Time Spent: 26851s\n",
      "Next Learning Rate: 0.0005210401371605207\n",
      "932. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9107, Time Spent: 26880s\n",
      "Next Learning Rate: 0.0005206754090645084\n",
      "933. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8953, Time Spent: 26908s\n",
      "Next Learning Rate: 0.0005203109362781631\n",
      "934. 1000/1007 scrambles genned. Solvable algs genned: 7\n",
      "Average Loss: 1.8934, Time Spent: 26938s\n",
      "Next Learning Rate: 0.0005199467186227684\n",
      "935. 1000/1001 scrambles genned. Solvable algs genned: 1\n",
      "Average Loss: 1.8961, Time Spent: 26966s\n",
      "Next Learning Rate: 0.0005195827559197324\n",
      "936. 1000/1003 scrambles genned. Solvable algs genned: 3\n",
      "Average Loss: 1.8950, Time Spent: 26995s\n",
      "Next Learning Rate: 0.0005192190479905885\n",
      "937. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.8761, Time Spent: 27024s\n",
      "Next Learning Rate: 0.0005188555946569951\n",
      "938. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8982, Time Spent: 27053s\n",
      "Next Learning Rate: 0.0005184923957407352\n",
      "939. 1000/1002 scrambles genned. Solvable algs genned: 2\n",
      "Average Loss: 1.8870, Time Spent: 27082s\n",
      "Next Learning Rate: 0.0005181294510637166\n",
      "940. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.9037, Time Spent: 27111s\n",
      "Next Learning Rate: 0.000517766760447972\n",
      "941. 1000/1006 scrambles genned. Solvable algs genned: 6\n",
      "Average Loss: 1.9282, Time Spent: 27140s\n",
      "Next Learning Rate: 0.0005174043237156584\n",
      "942. 1000/1004 scrambles genned. Solvable algs genned: 4\n",
      "Average Loss: 1.9137, Time Spent: 27169s\n",
      "Next Learning Rate: 0.0005170421406890574\n",
      "943. 1000/1005 scrambles genned. Solvable algs genned: 5\n",
      "Average Loss: 1.8820, Time Spent: 27200s\n",
      "Next Learning Rate: 0.0005166802111905751\n",
      "944. 1000/1008 scrambles genned. Solvable algs genned: 8\n",
      "Average Loss: 1.8922, Time Spent: 27230s\n",
      "Next Learning Rate: 0.0005163185350427417\n",
      "945. 1000/1004 scrambles genned. Solvable algs genned: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10772\\506507111.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Compute the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Backpropagation and optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1121\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2822\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2824\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "n = 12 # nr of moves we want the training data on\n",
    "\n",
    "k = 6 # max length in table lookup\n",
    "# treshold = 0.75 # acceptance rate for the network being correct\n",
    "nr_of_algs = 1000 # number of algs we want to train on\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #CrossEntropyLoss MSELoss\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001) #SGD   0.0005\n",
    "u = 0.9993\n",
    "\n",
    "correct_rate = 0\n",
    "t = time.time()\n",
    "its = 0\n",
    "while True:\n",
    "    network.eval()\n",
    "    algs = []\n",
    "    algs_genned = 0\n",
    "    while len(algs) < nr_of_algs:\n",
    "        # gen alg\n",
    "        alg = gen_random_alg(n)\n",
    "        algs_genned += 1\n",
    "        \n",
    "        # check if it can be solved using n-k-1 AI iterations + lookup (i.e. if it can be solved in fewer moves than n)\n",
    "        cube = Cube()\n",
    "        cube.apply_moves(alg)\n",
    "        sol = AI_linear_solver(cube, n-k-1)\n",
    "        # if no solution, append to algs so we can train on it\n",
    "        if not sol:\n",
    "            algs.append(alg)\n",
    "    non_n_scrambles = algs_genned - nr_of_algs\n",
    "    its += 1\n",
    "    print(f\"{its}. {nr_of_algs}/{algs_genned} scrambles genned. Solvable algs genned: {non_n_scrambles}\")\n",
    "    dataset = algs_to_dataset(algs, k+1)\n",
    "    \n",
    "    network.train()\n",
    "    # Training loop\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    for input_data, target in dataset:\n",
    "        # Forward pass\n",
    "        output = network(input_data).unsqueeze(0)\n",
    "\n",
    "        # Encode the target as class probabilities for MSELoss\n",
    "#         target_onehot = F.one_hot(target, num_classes=18).float().squeeze() \n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        i+=1  \n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(dataset)\n",
    "    print(f\"Average Loss: {average_loss:.4f}, Time Spent: {int(time.time()-t)}s\")\n",
    "    optimizer.param_groups[0]['lr'] *= u\n",
    "    print(\"Next Learning Rate:\",optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "network.eval()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1: ResnetModel(240, 512, 256, 5, 18, False)\n",
    "2: ResnetModel(240, 1024, 512, 5, 18, False)\n",
    "3: ResnetModel(240, 4096, 1024, 5, 18, False) MSELoss\n",
    "4: ResnetModel(240, 4096, 1024, 5, 18, False) CrossEntropyLoss\n",
    "'''\n",
    "\n",
    "# # Specify a path\n",
    "PATH = \"nn_s12_4.pt\" #replace after use to not overwrite\n",
    "\n",
    "# Save\n",
    "# torch.save(network.state_dict(), PATH)\n",
    "# Load\n",
    "# network = SimpleNetwork()\n",
    "network.load_state_dict(torch.load(PATH))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.9 %\n",
      "2.4360759258270264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFIX! R L gets removed, only L R stays. For example: Check if next last move is opposite of this move, and add to training data.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.eval()\n",
    "def evalute_cube(cube):\n",
    "    state_tensor = cube_to_tensor(cube)\n",
    "    output = network(state_tensor)\n",
    "#     print(output)\n",
    "    _, predicted_class = torch.max(output, dim=0)\n",
    "    return predicted_class.item()\n",
    "\n",
    "n = 1000\n",
    "correct = 0\n",
    "t = time.time()\n",
    "for i in range(n):\n",
    "    length = gen_random_length(1,1,18)\n",
    "    alg = gen_random_alg(length)\n",
    "    cube = Cube()\n",
    "    cube.apply_moves(alg)\n",
    "    ev = evalute_cube(cube)\n",
    "#     print(alg,\"-\",moves[ev])\n",
    "    correct += int(ev==moves.index(inverse_alg(alg.split(\" \")[-1])))\n",
    "print(100*correct/n,\"%\")\n",
    "print(time.time()-t)\n",
    "\n",
    "'''\n",
    "FIX! R L gets removed, only L R stays. For example: Check if next last move is opposite of this move, and add to training data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1: 100.0%', '2: 90.7%', '3: 90.0%', '4: 87.9%', '5: 84.4%', '6: 76.9%', '7: 65.5%', '8: 57.6%', '9: 46.4%', '10: 35.3%', '11: 30.3%', '12: 22.3%', '13: 18.7%', '14: 14.7%', '15: 13.7%', '16: 11.3%', '17: 9.3%', '18: 9.7%', '19: 8.9%', '20: 8.7%']\n"
     ]
    }
   ],
   "source": [
    "percentages = []\n",
    "n = 1000\n",
    "for j in range(20):\n",
    "    correct = 0\n",
    "    t = time.time()\n",
    "    for i in range(n):\n",
    "        length = j+1\n",
    "        alg = gen_random_alg(length)\n",
    "        cube = Cube()\n",
    "        cube.apply_moves(alg)\n",
    "        ev = evalute_cube(cube)\n",
    "        correct += int(ev==moves.index(inverse_alg(alg.split(\" \")[-1])))\n",
    "    percentages.append(f\"{j+1}: {100*correct/n}%\")\n",
    "\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResnetModel(nn.Module):\n",
    "    def __init__(self, state_dim: int, h1_dim: int, resnet_dim: int, num_resnet_blocks: int,\n",
    "                 out_dim: int, batch_norm: bool):\n",
    "        super().__init__()\n",
    "        self.state_dim: int = state_dim\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.num_resnet_blocks: int = num_resnet_blocks\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        # first two hidden layers\n",
    "        self.fc1 = nn.Linear(self.state_dim, h1_dim)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(h1_dim)\n",
    "\n",
    "        self.fc2 = nn.Linear(h1_dim, resnet_dim)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(resnet_dim)\n",
    "\n",
    "        # resnet blocks\n",
    "        for block_num in range(self.num_resnet_blocks):\n",
    "            if self.batch_norm:\n",
    "                res_fc1 = nn.Linear(resnet_dim, resnet_dim)\n",
    "                res_bn1 = nn.BatchNorm1d(resnet_dim)\n",
    "                res_fc2 = nn.Linear(resnet_dim, resnet_dim)\n",
    "                res_bn2 = nn.BatchNorm1d(resnet_dim)\n",
    "                self.blocks.append(nn.ModuleList([res_fc1, res_bn1, res_fc2, res_bn2]))\n",
    "            else:\n",
    "                res_fc1 = nn.Linear(resnet_dim, resnet_dim)\n",
    "                res_fc2 = nn.Linear(resnet_dim, resnet_dim)\n",
    "                self.blocks.append(nn.ModuleList([res_fc1, res_fc2]))\n",
    "\n",
    "        # output\n",
    "        self.fc_out = nn.Linear(resnet_dim, out_dim)\n",
    "\n",
    "    def forward(self, states_nnet):\n",
    "        x = states_nnet\n",
    "\n",
    "        # preprocess input\n",
    "        x = x.float()\n",
    "\n",
    "        # first two hidden layers\n",
    "        x = self.fc1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # resnet blocks\n",
    "        for block_num in range(self.num_resnet_blocks):\n",
    "            res_inp = x\n",
    "            if self.batch_norm:\n",
    "                x = self.blocks[block_num][0](x)\n",
    "                x = self.blocks[block_num][1](x)\n",
    "                x = F.relu(x)\n",
    "                x = self.blocks[block_num][2](x)\n",
    "                x = self.blocks[block_num][3](x)\n",
    "            else:\n",
    "                x = self.blocks[block_num][0](x)\n",
    "                x = F.relu(x)\n",
    "                x = self.blocks[block_num][1](x)\n",
    "\n",
    "            x = F.relu(x + res_inp)\n",
    "\n",
    "        # output\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ResnetModel(240, 4096, 1024, 5, 18, False)\n",
    "network = network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14152\\3416771361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "network(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_dataset(dataset_size, data_exp, min_alg_length, max_alg_length):\n",
    "    # Define a dataset (input-output pairs)\n",
    "    dataset = []\n",
    "    for i in range(dataset_size):\n",
    "        length = gen_random_length(data_exp, min_alg_length, max_alg_length)\n",
    "        alg = gen_random_alg(length)\n",
    "        cube = Cube(0)\n",
    "        cube.apply_moves(alg)\n",
    "        dataset.append((cube_to_tensor_2(cube),torch.tensor([moves.index(inverse_alg(alg.split(\" \")[-1]))])))\n",
    "    return dataset\n",
    "\n",
    "def smart_dataset(nr_of_algs, data_exp, min_alg_length, max_alg_length):\n",
    "    # total number of data points: nr_of_algs*(max_alg_length - min_alg_length + 1)\n",
    "    dataset = []\n",
    "    for i in range(nr_of_algs):\n",
    "        alg = gen_random_alg(max_alg_length)\n",
    "        sub_algs = []\n",
    "        cube = Cube()\n",
    "        cube.apply_moves(alg)\n",
    "        for j in range(max_alg_length-1,min_alg_length-2,-1):\n",
    "            inv = inverse_alg(alg.split(\" \")[j])\n",
    "            dataset.append((cube_to_tensor_2(cube),torch.tensor([moves.index(inv)])))\n",
    "            cube.apply_moves(inv)\n",
    "    return dataset\n",
    "\n",
    "def smart_dataset_2(nr_of_algs, data_exp, min_alg_length, max_alg_length):\n",
    "    # total number of data points: nr_of_algs*(max_alg_length - min_alg_length + 1)\n",
    "    dataset = []\n",
    "    for i in range(nr_of_algs):\n",
    "        alg = gen_random_alg(max_alg_length)\n",
    "        sub_algs = []\n",
    "        cube = Cube()\n",
    "        cube.apply_moves(alg)\n",
    "        for j in range(max_alg_length-1,min_alg_length-2,-1):\n",
    "            inv = inverse_alg(alg.split(\" \")[j])\n",
    "            dataset.append((cube_to_tensor_2(cube).unsqueeze(0),torch.tensor([moves.index(inv)])))\n",
    "            cube.apply_moves(inv)\n",
    "    return dataset\n",
    "\n",
    "# r_dataset = random_dataset(dataset_size, data_exp, min_alg_length, max_alg_length)\n",
    "# s_dataset = smart_dataset(nr_of_algs, data_exp, min_alg_length, max_alg_length)\n",
    "# len(r_dataset),len(s_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.resnet = nn.Sequential(\n",
    "            nn.Linear(240, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 18)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.resnet(x)\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "network = ResNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resnet 2\n",
    "\n",
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "#         self.downsample = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "#             nn.BatchNorm1d(out_channels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         if x.shape != out.shape:\n",
    "#             identity = self.downsample(x)\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.conv = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.bn = nn.BatchNorm1d(64)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.layer1 = self._make_layer(64, 64, num_blocks=2)\n",
    "#         self.layer2 = self._make_layer(64, 128, num_blocks=2)\n",
    "#         self.layer3 = self._make_layer(128, 256, num_blocks=2)\n",
    "#         self.layer4 = self._make_layer(256, 512, num_blocks=2)\n",
    "\n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "#     def _make_layer(self, in_channels, out_channels, num_blocks):\n",
    "#         layers = []\n",
    "#         layers.append(ResidualBlock(in_channels, out_channels))\n",
    "#         for _ in range(1, num_blocks):\n",
    "#             layers.append(ResidualBlock(out_channels, out_channels))\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Create the ResNet model\n",
    "# input_size = 240\n",
    "# num_classes = 18  # Including 0 to 17 (total 18 classes)\n",
    "# network = ResNet(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 5000*5000 data points 1 times, totaling 25000000 data points.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# trains by making a \"small\" dataset and training on it 5 times. Repeat N times\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.0001) #SGD\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.005)\n",
    "# scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01, step_size_up=100, cycle_momentum=False)\n",
    "u = 0.999\n",
    "\n",
    "\n",
    "N = 5000\n",
    "dataset_size = 5000\n",
    "num_epochs = 1\n",
    "data_exp = 1\n",
    "min_alg_length = 7\n",
    "max_alg_length = 14\n",
    "nr_of_algs = dataset_size//(max_alg_length - min_alg_length + 1) # gives approx. dataset_size nr of data points\n",
    "print(f\"Training on {N}*{dataset_size} data points {num_epochs} times, totaling {N*dataset_size*num_epochs} data points.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 created\n",
      "Epoch [1/1], Average Loss: 2.1017, Time Spent: 8s\n",
      "9.99e-05\n",
      "1/5000 done, time elapsed since training start: 8s\n",
      "Dataset 2 created\n",
      "Epoch [1/1], Average Loss: 2.0755, Time Spent: 18s\n",
      "9.980010000000001e-05\n",
      "2/5000 done, time elapsed since training start: 18s\n",
      "Dataset 3 created\n",
      "Epoch [1/1], Average Loss: 2.1148, Time Spent: 26s\n",
      "9.970029990000001e-05\n",
      "3/5000 done, time elapsed since training start: 26s\n",
      "Dataset 4 created\n",
      "Epoch [1/1], Average Loss: 2.1150, Time Spent: 36s\n",
      "9.960059960010001e-05\n",
      "4/5000 done, time elapsed since training start: 36s\n",
      "Dataset 5 created\n",
      "Epoch [1/1], Average Loss: 2.1039, Time Spent: 46s\n",
      "9.950099900049991e-05\n",
      "5/5000 done, time elapsed since training start: 46s\n",
      "Dataset 6 created\n",
      "Epoch [1/1], Average Loss: 2.0986, Time Spent: 55s\n",
      "9.94014980014994e-05\n",
      "6/5000 done, time elapsed since training start: 55s\n",
      "Dataset 7 created\n",
      "Epoch [1/1], Average Loss: 2.1312, Time Spent: 64s\n",
      "9.930209650349791e-05\n",
      "7/5000 done, time elapsed since training start: 64s\n",
      "Dataset 8 created\n",
      "Epoch [1/1], Average Loss: 2.0995, Time Spent: 73s\n",
      "9.920279440699441e-05\n",
      "8/5000 done, time elapsed since training start: 73s\n",
      "Dataset 9 created\n",
      "Epoch [1/1], Average Loss: 2.0975, Time Spent: 81s\n",
      "9.910359161258741e-05\n",
      "9/5000 done, time elapsed since training start: 81s\n",
      "Dataset 10 created\n",
      "Epoch [1/1], Average Loss: 2.0910, Time Spent: 90s\n",
      "9.900448802097483e-05\n",
      "10/5000 done, time elapsed since training start: 90s\n",
      "Dataset 11 created\n",
      "Epoch [1/1], Average Loss: 2.1005, Time Spent: 99s\n",
      "9.890548353295386e-05\n",
      "11/5000 done, time elapsed since training start: 99s\n",
      "Dataset 12 created\n",
      "Epoch [1/1], Average Loss: 2.1069, Time Spent: 108s\n",
      "9.88065780494209e-05\n",
      "12/5000 done, time elapsed since training start: 108s\n",
      "Dataset 13 created\n",
      "Epoch [1/1], Average Loss: 2.0925, Time Spent: 118s\n",
      "9.870777147137147e-05\n",
      "13/5000 done, time elapsed since training start: 118s\n",
      "Dataset 14 created\n",
      "Epoch [1/1], Average Loss: 2.0926, Time Spent: 126s\n",
      "9.860906369990011e-05\n",
      "14/5000 done, time elapsed since training start: 126s\n",
      "Dataset 15 created\n",
      "Epoch [1/1], Average Loss: 2.1194, Time Spent: 135s\n",
      "9.85104546362002e-05\n",
      "15/5000 done, time elapsed since training start: 135s\n",
      "Dataset 16 created\n",
      "Epoch [1/1], Average Loss: 2.0834, Time Spent: 144s\n",
      "9.8411944181564e-05\n",
      "16/5000 done, time elapsed since training start: 144s\n",
      "Dataset 17 created\n",
      "Epoch [1/1], Average Loss: 2.1011, Time Spent: 152s\n",
      "9.831353223738244e-05\n",
      "17/5000 done, time elapsed since training start: 152s\n",
      "Dataset 18 created\n",
      "Epoch [1/1], Average Loss: 2.1048, Time Spent: 160s\n",
      "9.821521870514505e-05\n",
      "18/5000 done, time elapsed since training start: 160s\n",
      "Dataset 19 created\n",
      "Epoch [1/1], Average Loss: 2.1185, Time Spent: 169s\n",
      "9.81170034864399e-05\n",
      "19/5000 done, time elapsed since training start: 169s\n",
      "Dataset 20 created\n",
      "Epoch [1/1], Average Loss: 2.1086, Time Spent: 178s\n",
      "9.801888648295346e-05\n",
      "20/5000 done, time elapsed since training start: 178s\n",
      "Dataset 21 created\n",
      "Epoch [1/1], Average Loss: 2.1183, Time Spent: 186s\n",
      "9.792086759647051e-05\n",
      "21/5000 done, time elapsed since training start: 186s\n",
      "Dataset 22 created\n",
      "Epoch [1/1], Average Loss: 2.0775, Time Spent: 194s\n",
      "9.782294672887403e-05\n",
      "22/5000 done, time elapsed since training start: 194s\n",
      "Dataset 23 created\n",
      "Epoch [1/1], Average Loss: 2.0973, Time Spent: 201s\n",
      "9.772512378214515e-05\n",
      "23/5000 done, time elapsed since training start: 201s\n",
      "Dataset 24 created\n",
      "Epoch [1/1], Average Loss: 2.1058, Time Spent: 209s\n",
      "9.762739865836301e-05\n",
      "24/5000 done, time elapsed since training start: 209s\n",
      "Dataset 25 created\n",
      "Epoch [1/1], Average Loss: 2.0672, Time Spent: 217s\n",
      "9.752977125970466e-05\n",
      "25/5000 done, time elapsed since training start: 217s\n",
      "Dataset 26 created\n",
      "Epoch [1/1], Average Loss: 2.1197, Time Spent: 225s\n",
      "9.743224148844495e-05\n",
      "26/5000 done, time elapsed since training start: 225s\n",
      "Dataset 27 created\n",
      "Epoch [1/1], Average Loss: 2.0793, Time Spent: 233s\n",
      "9.73348092469565e-05\n",
      "27/5000 done, time elapsed since training start: 233s\n",
      "Dataset 28 created\n",
      "Epoch [1/1], Average Loss: 2.0990, Time Spent: 241s\n",
      "9.723747443770954e-05\n",
      "28/5000 done, time elapsed since training start: 241s\n",
      "Dataset 29 created\n",
      "Epoch [1/1], Average Loss: 2.0732, Time Spent: 249s\n",
      "9.714023696327183e-05\n",
      "29/5000 done, time elapsed since training start: 249s\n",
      "Dataset 30 created\n",
      "Epoch [1/1], Average Loss: 2.0670, Time Spent: 257s\n",
      "9.704309672630856e-05\n",
      "30/5000 done, time elapsed since training start: 257s\n",
      "Dataset 31 created\n",
      "Epoch [1/1], Average Loss: 2.1085, Time Spent: 265s\n",
      "9.694605362958225e-05\n",
      "31/5000 done, time elapsed since training start: 265s\n",
      "Dataset 32 created\n",
      "Epoch [1/1], Average Loss: 2.1234, Time Spent: 273s\n",
      "9.684910757595267e-05\n",
      "32/5000 done, time elapsed since training start: 273s\n",
      "Dataset 33 created\n",
      "Epoch [1/1], Average Loss: 2.0928, Time Spent: 281s\n",
      "9.675225846837672e-05\n",
      "33/5000 done, time elapsed since training start: 281s\n",
      "Dataset 34 created\n",
      "Epoch [1/1], Average Loss: 2.1036, Time Spent: 289s\n",
      "9.665550620990834e-05\n",
      "34/5000 done, time elapsed since training start: 289s\n",
      "Dataset 35 created\n",
      "Epoch [1/1], Average Loss: 2.0886, Time Spent: 297s\n",
      "9.655885070369844e-05\n",
      "35/5000 done, time elapsed since training start: 297s\n",
      "Dataset 36 created\n",
      "Epoch [1/1], Average Loss: 2.0631, Time Spent: 305s\n",
      "9.646229185299473e-05\n",
      "36/5000 done, time elapsed since training start: 305s\n",
      "Dataset 37 created\n",
      "Epoch [1/1], Average Loss: 2.0724, Time Spent: 313s\n",
      "9.636582956114173e-05\n",
      "37/5000 done, time elapsed since training start: 313s\n",
      "Dataset 38 created\n",
      "Epoch [1/1], Average Loss: 2.0925, Time Spent: 321s\n",
      "9.62694637315806e-05\n",
      "38/5000 done, time elapsed since training start: 321s\n",
      "Dataset 39 created\n",
      "Epoch [1/1], Average Loss: 2.0986, Time Spent: 329s\n",
      "9.617319426784902e-05\n",
      "39/5000 done, time elapsed since training start: 329s\n",
      "Dataset 40 created\n",
      "Epoch [1/1], Average Loss: 2.1117, Time Spent: 337s\n",
      "9.607702107358116e-05\n",
      "40/5000 done, time elapsed since training start: 337s\n",
      "Dataset 41 created\n",
      "Epoch [1/1], Average Loss: 2.0755, Time Spent: 345s\n",
      "9.598094405250758e-05\n",
      "41/5000 done, time elapsed since training start: 345s\n",
      "Dataset 42 created\n",
      "Epoch [1/1], Average Loss: 2.0922, Time Spent: 353s\n",
      "9.588496310845507e-05\n",
      "42/5000 done, time elapsed since training start: 353s\n",
      "Dataset 43 created\n",
      "Epoch [1/1], Average Loss: 2.0884, Time Spent: 361s\n",
      "9.578907814534661e-05\n",
      "43/5000 done, time elapsed since training start: 361s\n",
      "Dataset 44 created\n",
      "Epoch [1/1], Average Loss: 2.1239, Time Spent: 369s\n",
      "9.569328906720127e-05\n",
      "44/5000 done, time elapsed since training start: 369s\n",
      "Dataset 45 created\n",
      "Epoch [1/1], Average Loss: 2.0705, Time Spent: 378s\n",
      "9.559759577813406e-05\n",
      "45/5000 done, time elapsed since training start: 378s\n",
      "Dataset 46 created\n",
      "Epoch [1/1], Average Loss: 2.1262, Time Spent: 386s\n",
      "9.550199818235592e-05\n",
      "46/5000 done, time elapsed since training start: 386s\n",
      "Dataset 47 created\n",
      "Epoch [1/1], Average Loss: 2.1056, Time Spent: 395s\n",
      "9.540649618417357e-05\n",
      "47/5000 done, time elapsed since training start: 395s\n",
      "Dataset 48 created\n",
      "Epoch [1/1], Average Loss: 2.1096, Time Spent: 403s\n",
      "9.53110896879894e-05\n",
      "48/5000 done, time elapsed since training start: 403s\n",
      "Dataset 49 created\n",
      "Epoch [1/1], Average Loss: 2.0952, Time Spent: 411s\n",
      "9.521577859830141e-05\n",
      "49/5000 done, time elapsed since training start: 411s\n",
      "Dataset 50 created\n",
      "Epoch [1/1], Average Loss: 2.0939, Time Spent: 419s\n",
      "9.512056281970311e-05\n",
      "50/5000 done, time elapsed since training start: 419s\n",
      "Dataset 51 created\n",
      "Epoch [1/1], Average Loss: 2.0896, Time Spent: 427s\n",
      "9.502544225688341e-05\n",
      "51/5000 done, time elapsed since training start: 427s\n",
      "Dataset 52 created\n",
      "Epoch [1/1], Average Loss: 2.0966, Time Spent: 436s\n",
      "9.493041681462653e-05\n",
      "52/5000 done, time elapsed since training start: 436s\n",
      "Dataset 53 created\n",
      "Epoch [1/1], Average Loss: 2.1148, Time Spent: 443s\n",
      "9.48354863978119e-05\n",
      "53/5000 done, time elapsed since training start: 443s\n",
      "Dataset 54 created\n",
      "Epoch [1/1], Average Loss: 2.0854, Time Spent: 451s\n",
      "9.474065091141409e-05\n",
      "54/5000 done, time elapsed since training start: 451s\n",
      "Dataset 55 created\n",
      "Epoch [1/1], Average Loss: 2.1081, Time Spent: 459s\n",
      "9.464591026050267e-05\n",
      "55/5000 done, time elapsed since training start: 459s\n",
      "Dataset 56 created\n",
      "Epoch [1/1], Average Loss: 2.0989, Time Spent: 467s\n",
      "9.455126435024217e-05\n",
      "56/5000 done, time elapsed since training start: 467s\n",
      "Dataset 57 created\n",
      "Epoch [1/1], Average Loss: 2.0931, Time Spent: 475s\n",
      "9.445671308589193e-05\n",
      "57/5000 done, time elapsed since training start: 475s\n",
      "Dataset 58 created\n",
      "Epoch [1/1], Average Loss: 2.1239, Time Spent: 483s\n",
      "9.436225637280604e-05\n",
      "58/5000 done, time elapsed since training start: 483s\n",
      "Dataset 59 created\n",
      "Epoch [1/1], Average Loss: 2.0886, Time Spent: 491s\n",
      "9.426789411643323e-05\n",
      "59/5000 done, time elapsed since training start: 491s\n",
      "Dataset 60 created\n",
      "Epoch [1/1], Average Loss: 2.0774, Time Spent: 498s\n",
      "9.41736262223168e-05\n",
      "60/5000 done, time elapsed since training start: 498s\n",
      "Dataset 61 created\n",
      "Epoch [1/1], Average Loss: 2.0844, Time Spent: 506s\n",
      "9.407945259609448e-05\n",
      "61/5000 done, time elapsed since training start: 506s\n",
      "Dataset 62 created\n",
      "Epoch [1/1], Average Loss: 2.1049, Time Spent: 514s\n",
      "9.398537314349839e-05\n",
      "62/5000 done, time elapsed since training start: 514s\n",
      "Dataset 63 created\n",
      "Epoch [1/1], Average Loss: 2.0994, Time Spent: 522s\n",
      "9.389138777035489e-05\n",
      "63/5000 done, time elapsed since training start: 522s\n",
      "Dataset 64 created\n",
      "Epoch [1/1], Average Loss: 2.0859, Time Spent: 530s\n",
      "9.379749638258453e-05\n",
      "64/5000 done, time elapsed since training start: 530s\n",
      "Dataset 65 created\n",
      "Epoch [1/1], Average Loss: 2.1350, Time Spent: 538s\n",
      "9.370369888620194e-05\n",
      "65/5000 done, time elapsed since training start: 538s\n",
      "Dataset 66 created\n",
      "Epoch [1/1], Average Loss: 2.1141, Time Spent: 546s\n",
      "9.360999518731574e-05\n",
      "66/5000 done, time elapsed since training start: 546s\n",
      "Dataset 67 created\n",
      "Epoch [1/1], Average Loss: 2.0918, Time Spent: 554s\n",
      "9.351638519212843e-05\n",
      "67/5000 done, time elapsed since training start: 554s\n",
      "Dataset 68 created\n",
      "Epoch [1/1], Average Loss: 2.0889, Time Spent: 561s\n",
      "9.34228688069363e-05\n",
      "68/5000 done, time elapsed since training start: 561s\n",
      "Dataset 69 created\n",
      "Epoch [1/1], Average Loss: 2.0807, Time Spent: 570s\n",
      "9.332944593812936e-05\n",
      "69/5000 done, time elapsed since training start: 570s\n",
      "Dataset 70 created\n",
      "Epoch [1/1], Average Loss: 2.1108, Time Spent: 577s\n",
      "9.323611649219123e-05\n",
      "70/5000 done, time elapsed since training start: 577s\n",
      "Dataset 71 created\n",
      "Epoch [1/1], Average Loss: 2.1328, Time Spent: 585s\n",
      "9.314288037569904e-05\n",
      "71/5000 done, time elapsed since training start: 585s\n",
      "Dataset 72 created\n",
      "Epoch [1/1], Average Loss: 2.1076, Time Spent: 593s\n",
      "9.304973749532334e-05\n",
      "72/5000 done, time elapsed since training start: 593s\n",
      "Dataset 73 created\n",
      "Epoch [1/1], Average Loss: 2.0960, Time Spent: 601s\n",
      "9.295668775782802e-05\n",
      "73/5000 done, time elapsed since training start: 601s\n",
      "Dataset 74 created\n",
      "Epoch [1/1], Average Loss: 2.0976, Time Spent: 608s\n",
      "9.286373107007019e-05\n",
      "74/5000 done, time elapsed since training start: 609s\n",
      "Dataset 75 created\n",
      "Epoch [1/1], Average Loss: 2.0776, Time Spent: 616s\n",
      "9.277086733900011e-05\n",
      "75/5000 done, time elapsed since training start: 616s\n",
      "Dataset 76 created\n",
      "Epoch [1/1], Average Loss: 2.0796, Time Spent: 624s\n",
      "9.267809647166111e-05\n",
      "76/5000 done, time elapsed since training start: 624s\n",
      "Dataset 77 created\n",
      "Epoch [1/1], Average Loss: 2.0784, Time Spent: 632s\n",
      "9.258541837518945e-05\n",
      "77/5000 done, time elapsed since training start: 632s\n",
      "Dataset 78 created\n",
      "Epoch [1/1], Average Loss: 2.1112, Time Spent: 640s\n",
      "9.249283295681426e-05\n",
      "78/5000 done, time elapsed since training start: 640s\n",
      "Dataset 79 created\n",
      "Epoch [1/1], Average Loss: 2.1034, Time Spent: 648s\n",
      "9.240034012385745e-05\n",
      "79/5000 done, time elapsed since training start: 648s\n",
      "Dataset 80 created\n",
      "Epoch [1/1], Average Loss: 2.1017, Time Spent: 655s\n",
      "9.230793978373359e-05\n",
      "80/5000 done, time elapsed since training start: 655s\n",
      "Dataset 81 created\n",
      "Epoch [1/1], Average Loss: 2.0697, Time Spent: 663s\n",
      "9.221563184394985e-05\n",
      "81/5000 done, time elapsed since training start: 663s\n",
      "Dataset 82 created\n",
      "Epoch [1/1], Average Loss: 2.1081, Time Spent: 671s\n",
      "9.21234162121059e-05\n",
      "82/5000 done, time elapsed since training start: 671s\n",
      "Dataset 83 created\n",
      "Epoch [1/1], Average Loss: 2.0981, Time Spent: 679s\n",
      "9.20312927958938e-05\n",
      "83/5000 done, time elapsed since training start: 679s\n",
      "Dataset 84 created\n",
      "Epoch [1/1], Average Loss: 2.0945, Time Spent: 687s\n",
      "9.19392615030979e-05\n",
      "84/5000 done, time elapsed since training start: 687s\n",
      "Dataset 85 created\n",
      "Epoch [1/1], Average Loss: 2.1086, Time Spent: 695s\n",
      "9.184732224159481e-05\n",
      "85/5000 done, time elapsed since training start: 695s\n",
      "Dataset 86 created\n",
      "Epoch [1/1], Average Loss: 2.1095, Time Spent: 704s\n",
      "9.175547491935321e-05\n",
      "86/5000 done, time elapsed since training start: 704s\n",
      "Dataset 87 created\n",
      "Epoch [1/1], Average Loss: 2.1144, Time Spent: 711s\n",
      "9.166371944443386e-05\n",
      "87/5000 done, time elapsed since training start: 711s\n",
      "Dataset 88 created\n",
      "Epoch [1/1], Average Loss: 2.0898, Time Spent: 719s\n",
      "9.157205572498943e-05\n",
      "88/5000 done, time elapsed since training start: 719s\n",
      "Dataset 89 created\n",
      "Epoch [1/1], Average Loss: 2.1266, Time Spent: 727s\n",
      "9.148048366926444e-05\n",
      "89/5000 done, time elapsed since training start: 727s\n",
      "Dataset 90 created\n",
      "Epoch [1/1], Average Loss: 2.0859, Time Spent: 735s\n",
      "9.138900318559518e-05\n",
      "90/5000 done, time elapsed since training start: 735s\n",
      "Dataset 91 created\n",
      "Epoch [1/1], Average Loss: 2.1247, Time Spent: 743s\n",
      "9.129761418240959e-05\n",
      "91/5000 done, time elapsed since training start: 743s\n",
      "Dataset 92 created\n",
      "Epoch [1/1], Average Loss: 2.1031, Time Spent: 751s\n",
      "9.120631656822718e-05\n",
      "92/5000 done, time elapsed since training start: 751s\n",
      "Dataset 93 created\n",
      "Epoch [1/1], Average Loss: 2.1134, Time Spent: 759s\n",
      "9.111511025165896e-05\n",
      "93/5000 done, time elapsed since training start: 759s\n",
      "Dataset 94 created\n",
      "Epoch [1/1], Average Loss: 2.1080, Time Spent: 767s\n",
      "9.102399514140729e-05\n",
      "94/5000 done, time elapsed since training start: 767s\n",
      "Dataset 95 created\n",
      "Epoch [1/1], Average Loss: 2.0770, Time Spent: 774s\n",
      "9.093297114626589e-05\n",
      "95/5000 done, time elapsed since training start: 774s\n",
      "Dataset 96 created\n",
      "Epoch [1/1], Average Loss: 2.1159, Time Spent: 782s\n",
      "9.084203817511962e-05\n",
      "96/5000 done, time elapsed since training start: 782s\n",
      "Dataset 97 created\n",
      "Epoch [1/1], Average Loss: 2.0884, Time Spent: 790s\n",
      "9.075119613694451e-05\n",
      "97/5000 done, time elapsed since training start: 790s\n",
      "Dataset 98 created\n",
      "Epoch [1/1], Average Loss: 2.0947, Time Spent: 798s\n",
      "9.066044494080756e-05\n",
      "98/5000 done, time elapsed since training start: 798s\n",
      "Dataset 99 created\n",
      "Epoch [1/1], Average Loss: 2.1050, Time Spent: 806s\n",
      "9.056978449586675e-05\n",
      "99/5000 done, time elapsed since training start: 806s\n",
      "Dataset 100 created\n",
      "Epoch [1/1], Average Loss: 2.0616, Time Spent: 814s\n",
      "9.047921471137089e-05\n",
      "100/5000 done, time elapsed since training start: 814s\n",
      "Dataset 101 created\n",
      "Epoch [1/1], Average Loss: 2.1009, Time Spent: 822s\n",
      "9.038873549665951e-05\n",
      "101/5000 done, time elapsed since training start: 822s\n",
      "Dataset 102 created\n",
      "Epoch [1/1], Average Loss: 2.0781, Time Spent: 829s\n",
      "9.029834676116286e-05\n",
      "102/5000 done, time elapsed since training start: 829s\n",
      "Dataset 103 created\n",
      "Epoch [1/1], Average Loss: 2.0751, Time Spent: 837s\n",
      "9.02080484144017e-05\n",
      "103/5000 done, time elapsed since training start: 837s\n",
      "Dataset 104 created\n",
      "Epoch [1/1], Average Loss: 2.1091, Time Spent: 845s\n",
      "9.011784036598729e-05\n",
      "104/5000 done, time elapsed since training start: 845s\n",
      "Dataset 105 created\n",
      "Epoch [1/1], Average Loss: 2.1137, Time Spent: 853s\n",
      "9.002772252562131e-05\n",
      "105/5000 done, time elapsed since training start: 853s\n",
      "Dataset 106 created\n",
      "Epoch [1/1], Average Loss: 2.0738, Time Spent: 861s\n",
      "8.993769480309569e-05\n",
      "106/5000 done, time elapsed since training start: 861s\n",
      "Dataset 107 created\n",
      "Epoch [1/1], Average Loss: 2.0757, Time Spent: 869s\n",
      "8.98477571082926e-05\n",
      "107/5000 done, time elapsed since training start: 869s\n",
      "Dataset 108 created\n",
      "Epoch [1/1], Average Loss: 2.1315, Time Spent: 876s\n",
      "8.97579093511843e-05\n",
      "108/5000 done, time elapsed since training start: 876s\n",
      "Dataset 109 created\n",
      "Epoch [1/1], Average Loss: 2.0961, Time Spent: 884s\n",
      "8.966815144183312e-05\n",
      "109/5000 done, time elapsed since training start: 884s\n",
      "Dataset 110 created\n",
      "Epoch [1/1], Average Loss: 2.0837, Time Spent: 892s\n",
      "8.957848329039128e-05\n",
      "110/5000 done, time elapsed since training start: 892s\n",
      "Dataset 111 created\n",
      "Epoch [1/1], Average Loss: 2.1231, Time Spent: 900s\n",
      "8.948890480710089e-05\n",
      "111/5000 done, time elapsed since training start: 900s\n",
      "Dataset 112 created\n",
      "Epoch [1/1], Average Loss: 2.1140, Time Spent: 908s\n",
      "8.939941590229378e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/5000 done, time elapsed since training start: 908s\n",
      "Dataset 113 created\n",
      "Epoch [1/1], Average Loss: 2.0989, Time Spent: 916s\n",
      "8.931001648639149e-05\n",
      "113/5000 done, time elapsed since training start: 916s\n",
      "Dataset 114 created\n",
      "Epoch [1/1], Average Loss: 2.1007, Time Spent: 924s\n",
      "8.92207064699051e-05\n",
      "114/5000 done, time elapsed since training start: 924s\n",
      "Dataset 115 created\n",
      "Epoch [1/1], Average Loss: 2.1139, Time Spent: 932s\n",
      "8.913148576343518e-05\n",
      "115/5000 done, time elapsed since training start: 932s\n",
      "Dataset 116 created\n",
      "Epoch [1/1], Average Loss: 2.0787, Time Spent: 940s\n",
      "8.904235427767175e-05\n",
      "116/5000 done, time elapsed since training start: 940s\n",
      "Dataset 117 created\n",
      "Epoch [1/1], Average Loss: 2.1398, Time Spent: 948s\n",
      "8.895331192339408e-05\n",
      "117/5000 done, time elapsed since training start: 948s\n",
      "Dataset 118 created\n",
      "Epoch [1/1], Average Loss: 2.0914, Time Spent: 956s\n",
      "8.886435861147069e-05\n",
      "118/5000 done, time elapsed since training start: 956s\n",
      "Dataset 119 created\n",
      "Epoch [1/1], Average Loss: 2.1003, Time Spent: 965s\n",
      "8.877549425285921e-05\n",
      "119/5000 done, time elapsed since training start: 965s\n",
      "Dataset 120 created\n",
      "Epoch [1/1], Average Loss: 2.0749, Time Spent: 974s\n",
      "8.868671875860635e-05\n",
      "120/5000 done, time elapsed since training start: 974s\n",
      "Dataset 121 created\n",
      "Epoch [1/1], Average Loss: 2.0979, Time Spent: 982s\n",
      "8.859803203984775e-05\n",
      "121/5000 done, time elapsed since training start: 982s\n",
      "Dataset 122 created\n",
      "Epoch [1/1], Average Loss: 2.0717, Time Spent: 990s\n",
      "8.85094340078079e-05\n",
      "122/5000 done, time elapsed since training start: 990s\n",
      "Dataset 123 created\n",
      "Epoch [1/1], Average Loss: 2.0787, Time Spent: 998s\n",
      "8.842092457380009e-05\n",
      "123/5000 done, time elapsed since training start: 998s\n",
      "Dataset 124 created\n",
      "Epoch [1/1], Average Loss: 2.1091, Time Spent: 1005s\n",
      "8.833250364922628e-05\n",
      "124/5000 done, time elapsed since training start: 1005s\n",
      "Dataset 125 created\n",
      "Epoch [1/1], Average Loss: 2.0995, Time Spent: 1013s\n",
      "8.824417114557706e-05\n",
      "125/5000 done, time elapsed since training start: 1013s\n",
      "Dataset 126 created\n",
      "Epoch [1/1], Average Loss: 2.0907, Time Spent: 1021s\n",
      "8.815592697443149e-05\n",
      "126/5000 done, time elapsed since training start: 1021s\n",
      "Dataset 127 created\n",
      "Epoch [1/1], Average Loss: 2.1109, Time Spent: 1029s\n",
      "8.806777104745705e-05\n",
      "127/5000 done, time elapsed since training start: 1029s\n",
      "Dataset 128 created\n",
      "Epoch [1/1], Average Loss: 2.1100, Time Spent: 1037s\n",
      "8.79797032764096e-05\n",
      "128/5000 done, time elapsed since training start: 1037s\n",
      "Dataset 129 created\n",
      "Epoch [1/1], Average Loss: 2.1130, Time Spent: 1044s\n",
      "8.789172357313319e-05\n",
      "129/5000 done, time elapsed since training start: 1044s\n",
      "Dataset 130 created\n",
      "Epoch [1/1], Average Loss: 2.0825, Time Spent: 1052s\n",
      "8.780383184956006e-05\n",
      "130/5000 done, time elapsed since training start: 1052s\n",
      "Dataset 131 created\n",
      "Epoch [1/1], Average Loss: 2.0830, Time Spent: 1060s\n",
      "8.77160280177105e-05\n",
      "131/5000 done, time elapsed since training start: 1060s\n",
      "Dataset 132 created\n",
      "Epoch [1/1], Average Loss: 2.0948, Time Spent: 1068s\n",
      "8.76283119896928e-05\n",
      "132/5000 done, time elapsed since training start: 1068s\n",
      "Dataset 133 created\n",
      "Epoch [1/1], Average Loss: 2.1131, Time Spent: 1075s\n",
      "8.75406836777031e-05\n",
      "133/5000 done, time elapsed since training start: 1075s\n",
      "Dataset 134 created\n",
      "Epoch [1/1], Average Loss: 2.1037, Time Spent: 1083s\n",
      "8.74531429940254e-05\n",
      "134/5000 done, time elapsed since training start: 1083s\n",
      "Dataset 135 created\n",
      "Epoch [1/1], Average Loss: 2.0476, Time Spent: 1091s\n",
      "8.736568985103138e-05\n",
      "135/5000 done, time elapsed since training start: 1091s\n",
      "Dataset 136 created\n",
      "Epoch [1/1], Average Loss: 2.0771, Time Spent: 1099s\n",
      "8.727832416118035e-05\n",
      "136/5000 done, time elapsed since training start: 1099s\n",
      "Dataset 137 created\n",
      "Epoch [1/1], Average Loss: 2.0703, Time Spent: 1107s\n",
      "8.719104583701917e-05\n",
      "137/5000 done, time elapsed since training start: 1107s\n",
      "Dataset 138 created\n",
      "Epoch [1/1], Average Loss: 2.0609, Time Spent: 1115s\n",
      "8.710385479118215e-05\n",
      "138/5000 done, time elapsed since training start: 1115s\n",
      "Dataset 139 created\n",
      "Epoch [1/1], Average Loss: 2.1115, Time Spent: 1123s\n",
      "8.701675093639097e-05\n",
      "139/5000 done, time elapsed since training start: 1123s\n",
      "Dataset 140 created\n",
      "Epoch [1/1], Average Loss: 2.0892, Time Spent: 1130s\n",
      "8.692973418545459e-05\n",
      "140/5000 done, time elapsed since training start: 1130s\n",
      "Dataset 141 created\n",
      "Epoch [1/1], Average Loss: 2.1124, Time Spent: 1138s\n",
      "8.684280445126913e-05\n",
      "141/5000 done, time elapsed since training start: 1138s\n",
      "Dataset 142 created\n",
      "Epoch [1/1], Average Loss: 2.1344, Time Spent: 1146s\n",
      "8.675596164681786e-05\n",
      "142/5000 done, time elapsed since training start: 1146s\n",
      "Dataset 143 created\n",
      "Epoch [1/1], Average Loss: 2.1012, Time Spent: 1154s\n",
      "8.666920568517104e-05\n",
      "143/5000 done, time elapsed since training start: 1154s\n",
      "Dataset 144 created\n",
      "Epoch [1/1], Average Loss: 2.0884, Time Spent: 1162s\n",
      "8.658253647948587e-05\n",
      "144/5000 done, time elapsed since training start: 1162s\n",
      "Dataset 145 created\n",
      "Epoch [1/1], Average Loss: 2.0677, Time Spent: 1169s\n",
      "8.649595394300638e-05\n",
      "145/5000 done, time elapsed since training start: 1169s\n",
      "Dataset 146 created\n",
      "Epoch [1/1], Average Loss: 2.1055, Time Spent: 1177s\n",
      "8.640945798906337e-05\n",
      "146/5000 done, time elapsed since training start: 1177s\n",
      "Dataset 147 created\n",
      "Epoch [1/1], Average Loss: 2.1094, Time Spent: 1185s\n",
      "8.632304853107431e-05\n",
      "147/5000 done, time elapsed since training start: 1185s\n",
      "Dataset 148 created\n",
      "Epoch [1/1], Average Loss: 2.0573, Time Spent: 1192s\n",
      "8.623672548254323e-05\n",
      "148/5000 done, time elapsed since training start: 1192s\n",
      "Dataset 149 created\n",
      "Epoch [1/1], Average Loss: 2.0792, Time Spent: 1200s\n",
      "8.615048875706068e-05\n",
      "149/5000 done, time elapsed since training start: 1200s\n",
      "Dataset 150 created\n",
      "Epoch [1/1], Average Loss: 2.0973, Time Spent: 1208s\n",
      "8.606433826830362e-05\n",
      "150/5000 done, time elapsed since training start: 1208s\n",
      "Dataset 151 created\n",
      "Epoch [1/1], Average Loss: 2.1170, Time Spent: 1216s\n",
      "8.597827393003532e-05\n",
      "151/5000 done, time elapsed since training start: 1216s\n",
      "Dataset 152 created\n",
      "Epoch [1/1], Average Loss: 2.0927, Time Spent: 1223s\n",
      "8.589229565610528e-05\n",
      "152/5000 done, time elapsed since training start: 1223s\n",
      "Dataset 153 created\n",
      "Epoch [1/1], Average Loss: 2.1068, Time Spent: 1231s\n",
      "8.580640336044918e-05\n",
      "153/5000 done, time elapsed since training start: 1231s\n",
      "Dataset 154 created\n",
      "Epoch [1/1], Average Loss: 2.0812, Time Spent: 1239s\n",
      "8.572059695708873e-05\n",
      "154/5000 done, time elapsed since training start: 1239s\n",
      "Dataset 155 created\n",
      "Epoch [1/1], Average Loss: 2.0920, Time Spent: 1247s\n",
      "8.563487636013164e-05\n",
      "155/5000 done, time elapsed since training start: 1247s\n",
      "Dataset 156 created\n",
      "Epoch [1/1], Average Loss: 2.0895, Time Spent: 1254s\n",
      "8.554924148377151e-05\n",
      "156/5000 done, time elapsed since training start: 1254s\n",
      "Dataset 157 created\n",
      "Epoch [1/1], Average Loss: 2.0697, Time Spent: 1263s\n",
      "8.546369224228775e-05\n",
      "157/5000 done, time elapsed since training start: 1263s\n",
      "Dataset 158 created\n",
      "Epoch [1/1], Average Loss: 2.0925, Time Spent: 1270s\n",
      "8.537822855004546e-05\n",
      "158/5000 done, time elapsed since training start: 1270s\n",
      "Dataset 159 created\n",
      "Epoch [1/1], Average Loss: 2.1132, Time Spent: 1278s\n",
      "8.529285032149542e-05\n",
      "159/5000 done, time elapsed since training start: 1278s\n",
      "Dataset 160 created\n",
      "Epoch [1/1], Average Loss: 2.1029, Time Spent: 1286s\n",
      "8.520755747117393e-05\n",
      "160/5000 done, time elapsed since training start: 1286s\n",
      "Dataset 161 created\n",
      "Epoch [1/1], Average Loss: 2.1191, Time Spent: 1294s\n",
      "8.512234991370275e-05\n",
      "161/5000 done, time elapsed since training start: 1294s\n",
      "Dataset 162 created\n",
      "Epoch [1/1], Average Loss: 2.0851, Time Spent: 1302s\n",
      "8.503722756378905e-05\n",
      "162/5000 done, time elapsed since training start: 1302s\n",
      "Dataset 163 created\n",
      "Epoch [1/1], Average Loss: 2.0934, Time Spent: 1310s\n",
      "8.495219033622527e-05\n",
      "163/5000 done, time elapsed since training start: 1310s\n",
      "Dataset 164 created\n",
      "Epoch [1/1], Average Loss: 2.1275, Time Spent: 1318s\n",
      "8.486723814588903e-05\n",
      "164/5000 done, time elapsed since training start: 1318s\n",
      "Dataset 165 created\n",
      "Epoch [1/1], Average Loss: 2.0936, Time Spent: 1326s\n",
      "8.478237090774315e-05\n",
      "165/5000 done, time elapsed since training start: 1326s\n",
      "Dataset 166 created\n",
      "Epoch [1/1], Average Loss: 2.0777, Time Spent: 1334s\n",
      "8.46975885368354e-05\n",
      "166/5000 done, time elapsed since training start: 1334s\n",
      "Dataset 167 created\n",
      "Epoch [1/1], Average Loss: 2.1190, Time Spent: 1342s\n",
      "8.461289094829857e-05\n",
      "167/5000 done, time elapsed since training start: 1342s\n",
      "Dataset 168 created\n",
      "Epoch [1/1], Average Loss: 2.1055, Time Spent: 1350s\n",
      "8.452827805735027e-05\n",
      "168/5000 done, time elapsed since training start: 1350s\n",
      "Dataset 169 created\n",
      "Epoch [1/1], Average Loss: 2.1065, Time Spent: 1358s\n",
      "8.444374977929292e-05\n",
      "169/5000 done, time elapsed since training start: 1358s\n",
      "Dataset 170 created\n",
      "Epoch [1/1], Average Loss: 2.0626, Time Spent: 1365s\n",
      "8.435930602951363e-05\n",
      "170/5000 done, time elapsed since training start: 1365s\n",
      "Dataset 171 created\n",
      "Epoch [1/1], Average Loss: 2.1139, Time Spent: 1373s\n",
      "8.427494672348411e-05\n",
      "171/5000 done, time elapsed since training start: 1373s\n",
      "Dataset 172 created\n",
      "Epoch [1/1], Average Loss: 2.1093, Time Spent: 1381s\n",
      "8.419067177676063e-05\n",
      "172/5000 done, time elapsed since training start: 1381s\n",
      "Dataset 173 created\n",
      "Epoch [1/1], Average Loss: 2.0942, Time Spent: 1389s\n",
      "8.410648110498388e-05\n",
      "173/5000 done, time elapsed since training start: 1389s\n",
      "Dataset 174 created\n",
      "Epoch [1/1], Average Loss: 2.0932, Time Spent: 1397s\n",
      "8.402237462387889e-05\n",
      "174/5000 done, time elapsed since training start: 1397s\n",
      "Dataset 175 created\n",
      "Epoch [1/1], Average Loss: 2.1062, Time Spent: 1405s\n",
      "8.393835224925501e-05\n",
      "175/5000 done, time elapsed since training start: 1405s\n",
      "Dataset 176 created\n",
      "Epoch [1/1], Average Loss: 2.0980, Time Spent: 1412s\n",
      "8.385441389700575e-05\n",
      "176/5000 done, time elapsed since training start: 1412s\n",
      "Dataset 177 created\n",
      "Epoch [1/1], Average Loss: 2.1029, Time Spent: 1420s\n",
      "8.377055948310875e-05\n",
      "177/5000 done, time elapsed since training start: 1420s\n",
      "Dataset 178 created\n",
      "Epoch [1/1], Average Loss: 2.1150, Time Spent: 1429s\n",
      "8.368678892362564e-05\n",
      "178/5000 done, time elapsed since training start: 1429s\n",
      "Dataset 179 created\n",
      "Epoch [1/1], Average Loss: 2.0931, Time Spent: 1437s\n",
      "8.360310213470201e-05\n",
      "179/5000 done, time elapsed since training start: 1437s\n",
      "Dataset 180 created\n",
      "Epoch [1/1], Average Loss: 2.1058, Time Spent: 1445s\n",
      "8.351949903256732e-05\n",
      "180/5000 done, time elapsed since training start: 1445s\n",
      "Dataset 181 created\n",
      "Epoch [1/1], Average Loss: 2.0805, Time Spent: 1453s\n",
      "8.343597953353475e-05\n",
      "181/5000 done, time elapsed since training start: 1453s\n",
      "Dataset 182 created\n",
      "Epoch [1/1], Average Loss: 2.1089, Time Spent: 1460s\n",
      "8.33525435540012e-05\n",
      "182/5000 done, time elapsed since training start: 1460s\n",
      "Dataset 183 created\n",
      "Epoch [1/1], Average Loss: 2.0992, Time Spent: 1468s\n",
      "8.32691910104472e-05\n",
      "183/5000 done, time elapsed since training start: 1468s\n",
      "Dataset 184 created\n",
      "Epoch [1/1], Average Loss: 2.0923, Time Spent: 1476s\n",
      "8.318592181943675e-05\n",
      "184/5000 done, time elapsed since training start: 1476s\n",
      "Dataset 185 created\n",
      "Epoch [1/1], Average Loss: 2.1111, Time Spent: 1484s\n",
      "8.310273589761732e-05\n",
      "185/5000 done, time elapsed since training start: 1484s\n",
      "Dataset 186 created\n",
      "Epoch [1/1], Average Loss: 2.1045, Time Spent: 1492s\n",
      "8.30196331617197e-05\n",
      "186/5000 done, time elapsed since training start: 1492s\n",
      "Dataset 187 created\n",
      "Epoch [1/1], Average Loss: 2.0802, Time Spent: 1499s\n",
      "8.293661352855798e-05\n",
      "187/5000 done, time elapsed since training start: 1499s\n",
      "Dataset 188 created\n",
      "Epoch [1/1], Average Loss: 2.1251, Time Spent: 1507s\n",
      "8.285367691502942e-05\n",
      "188/5000 done, time elapsed since training start: 1507s\n",
      "Dataset 189 created\n",
      "Epoch [1/1], Average Loss: 2.1103, Time Spent: 1515s\n",
      "8.277082323811439e-05\n",
      "189/5000 done, time elapsed since training start: 1515s\n",
      "Dataset 190 created\n",
      "Epoch [1/1], Average Loss: 2.1043, Time Spent: 1523s\n",
      "8.268805241487628e-05\n",
      "190/5000 done, time elapsed since training start: 1523s\n",
      "Dataset 191 created\n",
      "Epoch [1/1], Average Loss: 2.0782, Time Spent: 1531s\n",
      "8.26053643624614e-05\n",
      "191/5000 done, time elapsed since training start: 1531s\n",
      "Dataset 192 created\n",
      "Epoch [1/1], Average Loss: 2.0991, Time Spent: 1539s\n",
      "8.252275899809894e-05\n",
      "192/5000 done, time elapsed since training start: 1539s\n",
      "Dataset 193 created\n",
      "Epoch [1/1], Average Loss: 2.0989, Time Spent: 1546s\n",
      "8.244023623910085e-05\n",
      "193/5000 done, time elapsed since training start: 1546s\n",
      "Dataset 194 created\n",
      "Epoch [1/1], Average Loss: 2.0679, Time Spent: 1554s\n",
      "8.235779600286175e-05\n",
      "194/5000 done, time elapsed since training start: 1554s\n",
      "Dataset 195 created\n",
      "Epoch [1/1], Average Loss: 2.0863, Time Spent: 1562s\n",
      "8.22754382068589e-05\n",
      "195/5000 done, time elapsed since training start: 1562s\n",
      "Dataset 196 created\n",
      "Epoch [1/1], Average Loss: 2.0933, Time Spent: 1570s\n",
      "8.219316276865204e-05\n",
      "196/5000 done, time elapsed since training start: 1570s\n",
      "Dataset 197 created\n",
      "Epoch [1/1], Average Loss: 2.0972, Time Spent: 1578s\n",
      "8.211096960588339e-05\n",
      "197/5000 done, time elapsed since training start: 1578s\n",
      "Dataset 198 created\n",
      "Epoch [1/1], Average Loss: 2.0916, Time Spent: 1585s\n",
      "8.20288586362775e-05\n",
      "198/5000 done, time elapsed since training start: 1585s\n",
      "Dataset 199 created\n",
      "Epoch [1/1], Average Loss: 2.1029, Time Spent: 1593s\n",
      "8.194682977764122e-05\n",
      "199/5000 done, time elapsed since training start: 1593s\n",
      "Dataset 200 created\n",
      "Epoch [1/1], Average Loss: 2.1090, Time Spent: 1601s\n",
      "8.186488294786357e-05\n",
      "200/5000 done, time elapsed since training start: 1601s\n",
      "Dataset 201 created\n",
      "Epoch [1/1], Average Loss: 2.0891, Time Spent: 1609s\n",
      "8.17830180649157e-05\n",
      "201/5000 done, time elapsed since training start: 1609s\n",
      "Dataset 202 created\n",
      "Epoch [1/1], Average Loss: 2.0736, Time Spent: 1616s\n",
      "8.17012350468508e-05\n",
      "202/5000 done, time elapsed since training start: 1616s\n",
      "Dataset 203 created\n",
      "Epoch [1/1], Average Loss: 2.1018, Time Spent: 1625s\n",
      "8.161953381180394e-05\n",
      "203/5000 done, time elapsed since training start: 1625s\n",
      "Dataset 204 created\n",
      "Epoch [1/1], Average Loss: 2.0863, Time Spent: 1633s\n",
      "8.153791427799214e-05\n",
      "204/5000 done, time elapsed since training start: 1633s\n",
      "Dataset 205 created\n",
      "Epoch [1/1], Average Loss: 2.0800, Time Spent: 1641s\n",
      "8.145637636371415e-05\n",
      "205/5000 done, time elapsed since training start: 1641s\n",
      "Dataset 206 created\n",
      "Epoch [1/1], Average Loss: 2.0916, Time Spent: 1649s\n",
      "8.137491998735043e-05\n",
      "206/5000 done, time elapsed since training start: 1649s\n",
      "Dataset 207 created\n",
      "Epoch [1/1], Average Loss: 2.0926, Time Spent: 1656s\n",
      "8.129354506736308e-05\n",
      "207/5000 done, time elapsed since training start: 1656s\n",
      "Dataset 208 created\n",
      "Epoch [1/1], Average Loss: 2.0928, Time Spent: 1664s\n",
      "8.121225152229572e-05\n",
      "208/5000 done, time elapsed since training start: 1664s\n",
      "Dataset 209 created\n",
      "Epoch [1/1], Average Loss: 2.0992, Time Spent: 1672s\n",
      "8.113103927077342e-05\n",
      "209/5000 done, time elapsed since training start: 1672s\n",
      "Dataset 210 created\n",
      "Epoch [1/1], Average Loss: 2.0747, Time Spent: 1680s\n",
      "8.104990823150265e-05\n",
      "210/5000 done, time elapsed since training start: 1680s\n",
      "Dataset 211 created\n",
      "Epoch [1/1], Average Loss: 2.0890, Time Spent: 1688s\n",
      "8.096885832327114e-05\n",
      "211/5000 done, time elapsed since training start: 1688s\n",
      "Dataset 212 created\n",
      "Epoch [1/1], Average Loss: 2.0920, Time Spent: 1696s\n",
      "8.088788946494788e-05\n",
      "212/5000 done, time elapsed since training start: 1696s\n",
      "Dataset 213 created\n",
      "Epoch [1/1], Average Loss: 2.0851, Time Spent: 1703s\n",
      "8.080700157548292e-05\n",
      "213/5000 done, time elapsed since training start: 1703s\n",
      "Dataset 214 created\n",
      "Epoch [1/1], Average Loss: 2.0839, Time Spent: 1711s\n",
      "8.072619457390744e-05\n",
      "214/5000 done, time elapsed since training start: 1711s\n",
      "Dataset 215 created\n",
      "Epoch [1/1], Average Loss: 2.1050, Time Spent: 1719s\n",
      "8.064546837933354e-05\n",
      "215/5000 done, time elapsed since training start: 1719s\n",
      "Dataset 216 created\n",
      "Epoch [1/1], Average Loss: 2.1258, Time Spent: 1727s\n",
      "8.05648229109542e-05\n",
      "216/5000 done, time elapsed since training start: 1727s\n",
      "Dataset 217 created\n",
      "Epoch [1/1], Average Loss: 2.0957, Time Spent: 1735s\n",
      "8.048425808804326e-05\n",
      "217/5000 done, time elapsed since training start: 1735s\n",
      "Dataset 218 created\n",
      "Epoch [1/1], Average Loss: 2.1048, Time Spent: 1742s\n",
      "8.040377382995522e-05\n",
      "218/5000 done, time elapsed since training start: 1742s\n",
      "Dataset 219 created\n",
      "Epoch [1/1], Average Loss: 2.0814, Time Spent: 1750s\n",
      "8.032337005612527e-05\n",
      "219/5000 done, time elapsed since training start: 1750s\n",
      "Dataset 220 created\n",
      "Epoch [1/1], Average Loss: 2.0945, Time Spent: 1758s\n",
      "8.024304668606914e-05\n",
      "220/5000 done, time elapsed since training start: 1758s\n",
      "Dataset 221 created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Average Loss: 2.0854, Time Spent: 1766s\n",
      "8.016280363938306e-05\n",
      "221/5000 done, time elapsed since training start: 1766s\n",
      "Dataset 222 created\n",
      "Epoch [1/1], Average Loss: 2.0970, Time Spent: 1773s\n",
      "8.008264083574368e-05\n",
      "222/5000 done, time elapsed since training start: 1773s\n",
      "Dataset 223 created\n",
      "Epoch [1/1], Average Loss: 2.0815, Time Spent: 1781s\n",
      "8.000255819490794e-05\n",
      "223/5000 done, time elapsed since training start: 1781s\n",
      "Dataset 224 created\n",
      "Epoch [1/1], Average Loss: 2.0824, Time Spent: 1789s\n",
      "7.992255563671303e-05\n",
      "224/5000 done, time elapsed since training start: 1789s\n",
      "Dataset 225 created\n",
      "Epoch [1/1], Average Loss: 2.1049, Time Spent: 1797s\n",
      "7.984263308107632e-05\n",
      "225/5000 done, time elapsed since training start: 1797s\n",
      "Dataset 226 created\n",
      "Epoch [1/1], Average Loss: 2.0974, Time Spent: 1805s\n",
      "7.976279044799524e-05\n",
      "226/5000 done, time elapsed since training start: 1805s\n",
      "Dataset 227 created\n",
      "Epoch [1/1], Average Loss: 2.0997, Time Spent: 1813s\n",
      "7.968302765754725e-05\n",
      "227/5000 done, time elapsed since training start: 1813s\n",
      "Dataset 228 created\n",
      "Epoch [1/1], Average Loss: 2.0859, Time Spent: 1821s\n",
      "7.96033446298897e-05\n",
      "228/5000 done, time elapsed since training start: 1821s\n",
      "Dataset 229 created\n",
      "Epoch [1/1], Average Loss: 2.0888, Time Spent: 1828s\n",
      "7.952374128525981e-05\n",
      "229/5000 done, time elapsed since training start: 1828s\n",
      "Dataset 230 created\n",
      "Epoch [1/1], Average Loss: 2.1016, Time Spent: 1836s\n",
      "7.944421754397455e-05\n",
      "230/5000 done, time elapsed since training start: 1836s\n",
      "Dataset 231 created\n",
      "Epoch [1/1], Average Loss: 2.0963, Time Spent: 1844s\n",
      "7.936477332643058e-05\n",
      "231/5000 done, time elapsed since training start: 1844s\n",
      "Dataset 232 created\n",
      "Epoch [1/1], Average Loss: 2.0836, Time Spent: 1852s\n",
      "7.928540855310415e-05\n",
      "232/5000 done, time elapsed since training start: 1852s\n",
      "Dataset 233 created\n",
      "Epoch [1/1], Average Loss: 2.1240, Time Spent: 1860s\n",
      "7.920612314455105e-05\n",
      "233/5000 done, time elapsed since training start: 1860s\n",
      "Dataset 234 created\n",
      "Epoch [1/1], Average Loss: 2.0859, Time Spent: 1868s\n",
      "7.91269170214065e-05\n",
      "234/5000 done, time elapsed since training start: 1868s\n",
      "Dataset 235 created\n",
      "Epoch [1/1], Average Loss: 2.0846, Time Spent: 1875s\n",
      "7.904779010438509e-05\n",
      "235/5000 done, time elapsed since training start: 1875s\n",
      "Dataset 236 created\n",
      "Epoch [1/1], Average Loss: 2.1125, Time Spent: 1883s\n",
      "7.89687423142807e-05\n",
      "236/5000 done, time elapsed since training start: 1883s\n",
      "Dataset 237 created\n",
      "Epoch [1/1], Average Loss: 2.0955, Time Spent: 1891s\n",
      "7.888977357196642e-05\n",
      "237/5000 done, time elapsed since training start: 1891s\n",
      "Dataset 238 created\n",
      "Epoch [1/1], Average Loss: 2.0998, Time Spent: 1899s\n",
      "7.881088379839445e-05\n",
      "238/5000 done, time elapsed since training start: 1899s\n",
      "Dataset 239 created\n",
      "Epoch [1/1], Average Loss: 2.0952, Time Spent: 1907s\n",
      "7.873207291459606e-05\n",
      "239/5000 done, time elapsed since training start: 1907s\n",
      "Dataset 240 created\n",
      "Epoch [1/1], Average Loss: 2.1132, Time Spent: 1914s\n",
      "7.865334084168146e-05\n",
      "240/5000 done, time elapsed since training start: 1914s\n",
      "Dataset 241 created\n",
      "Epoch [1/1], Average Loss: 2.1216, Time Spent: 1922s\n",
      "7.857468750083978e-05\n",
      "241/5000 done, time elapsed since training start: 1922s\n",
      "Dataset 242 created\n",
      "Epoch [1/1], Average Loss: 2.1090, Time Spent: 1930s\n",
      "7.849611281333894e-05\n",
      "242/5000 done, time elapsed since training start: 1930s\n",
      "Dataset 243 created\n",
      "Epoch [1/1], Average Loss: 2.0997, Time Spent: 1938s\n",
      "7.84176167005256e-05\n",
      "243/5000 done, time elapsed since training start: 1938s\n",
      "Dataset 244 created\n",
      "Epoch [1/1], Average Loss: 2.1135, Time Spent: 1946s\n",
      "7.833919908382508e-05\n",
      "244/5000 done, time elapsed since training start: 1946s\n",
      "Dataset 245 created\n",
      "Epoch [1/1], Average Loss: 2.0845, Time Spent: 1954s\n",
      "7.826085988474125e-05\n",
      "245/5000 done, time elapsed since training start: 1954s\n",
      "Dataset 246 created\n",
      "Epoch [1/1], Average Loss: 2.1160, Time Spent: 1962s\n",
      "7.818259902485651e-05\n",
      "246/5000 done, time elapsed since training start: 1962s\n",
      "Dataset 247 created\n",
      "Epoch [1/1], Average Loss: 2.1171, Time Spent: 1969s\n",
      "7.810441642583165e-05\n",
      "247/5000 done, time elapsed since training start: 1969s\n",
      "Dataset 248 created\n",
      "Epoch [1/1], Average Loss: 2.0839, Time Spent: 1977s\n",
      "7.802631200940582e-05\n",
      "248/5000 done, time elapsed since training start: 1977s\n",
      "Dataset 249 created\n",
      "Epoch [1/1], Average Loss: 2.0814, Time Spent: 1985s\n",
      "7.794828569739641e-05\n",
      "249/5000 done, time elapsed since training start: 1985s\n",
      "Dataset 250 created\n",
      "Epoch [1/1], Average Loss: 2.0925, Time Spent: 1993s\n",
      "7.787033741169902e-05\n",
      "250/5000 done, time elapsed since training start: 1993s\n",
      "Dataset 251 created\n",
      "Epoch [1/1], Average Loss: 2.1126, Time Spent: 2001s\n",
      "7.779246707428732e-05\n",
      "251/5000 done, time elapsed since training start: 2001s\n",
      "Dataset 252 created\n",
      "Epoch [1/1], Average Loss: 2.0605, Time Spent: 2009s\n",
      "7.771467460721304e-05\n",
      "252/5000 done, time elapsed since training start: 2009s\n",
      "Dataset 253 created\n",
      "Epoch [1/1], Average Loss: 2.1279, Time Spent: 2016s\n",
      "7.763695993260583e-05\n",
      "253/5000 done, time elapsed since training start: 2016s\n",
      "Dataset 254 created\n",
      "Epoch [1/1], Average Loss: 2.0719, Time Spent: 2024s\n",
      "7.755932297267323e-05\n",
      "254/5000 done, time elapsed since training start: 2024s\n",
      "Dataset 255 created\n",
      "Epoch [1/1], Average Loss: 2.1190, Time Spent: 2032s\n",
      "7.748176364970056e-05\n",
      "255/5000 done, time elapsed since training start: 2032s\n",
      "Dataset 256 created\n",
      "Epoch [1/1], Average Loss: 2.1306, Time Spent: 2040s\n",
      "7.740428188605086e-05\n",
      "256/5000 done, time elapsed since training start: 2040s\n",
      "Dataset 257 created\n",
      "Epoch [1/1], Average Loss: 2.0912, Time Spent: 2048s\n",
      "7.73268776041648e-05\n",
      "257/5000 done, time elapsed since training start: 2048s\n",
      "Dataset 258 created\n",
      "Epoch [1/1], Average Loss: 2.0985, Time Spent: 2056s\n",
      "7.724955072656065e-05\n",
      "258/5000 done, time elapsed since training start: 2056s\n",
      "Dataset 259 created\n",
      "Epoch [1/1], Average Loss: 2.1266, Time Spent: 2063s\n",
      "7.717230117583408e-05\n",
      "259/5000 done, time elapsed since training start: 2063s\n",
      "Dataset 260 created\n",
      "Epoch [1/1], Average Loss: 2.0885, Time Spent: 2071s\n",
      "7.709512887465825e-05\n",
      "260/5000 done, time elapsed since training start: 2071s\n",
      "Dataset 261 created\n",
      "Epoch [1/1], Average Loss: 2.0981, Time Spent: 2079s\n",
      "7.701803374578359e-05\n",
      "261/5000 done, time elapsed since training start: 2079s\n",
      "Dataset 262 created\n",
      "Epoch [1/1], Average Loss: 2.1179, Time Spent: 2086s\n",
      "7.694101571203781e-05\n",
      "262/5000 done, time elapsed since training start: 2086s\n",
      "Dataset 263 created\n",
      "Epoch [1/1], Average Loss: 2.0958, Time Spent: 2094s\n",
      "7.686407469632577e-05\n",
      "263/5000 done, time elapsed since training start: 2094s\n",
      "Dataset 264 created\n",
      "Epoch [1/1], Average Loss: 2.1035, Time Spent: 2102s\n",
      "7.678721062162945e-05\n",
      "264/5000 done, time elapsed since training start: 2102s\n",
      "Dataset 265 created\n",
      "Epoch [1/1], Average Loss: 2.0932, Time Spent: 2110s\n",
      "7.671042341100782e-05\n",
      "265/5000 done, time elapsed since training start: 2110s\n",
      "Dataset 266 created\n",
      "Epoch [1/1], Average Loss: 2.0918, Time Spent: 2118s\n",
      "7.663371298759682e-05\n",
      "266/5000 done, time elapsed since training start: 2118s\n",
      "Dataset 267 created\n",
      "Epoch [1/1], Average Loss: 2.1197, Time Spent: 2125s\n",
      "7.655707927460922e-05\n",
      "267/5000 done, time elapsed since training start: 2125s\n",
      "Dataset 268 created\n",
      "Epoch [1/1], Average Loss: 2.0909, Time Spent: 2133s\n",
      "7.648052219533461e-05\n",
      "268/5000 done, time elapsed since training start: 2133s\n",
      "Dataset 269 created\n",
      "Epoch [1/1], Average Loss: 2.0856, Time Spent: 2141s\n",
      "7.640404167313927e-05\n",
      "269/5000 done, time elapsed since training start: 2141s\n",
      "Dataset 270 created\n",
      "Epoch [1/1], Average Loss: 2.1057, Time Spent: 2149s\n",
      "7.632763763146613e-05\n",
      "270/5000 done, time elapsed since training start: 2149s\n",
      "Dataset 271 created\n",
      "Epoch [1/1], Average Loss: 2.0842, Time Spent: 2157s\n",
      "7.625130999383466e-05\n",
      "271/5000 done, time elapsed since training start: 2157s\n",
      "Dataset 272 created\n",
      "Epoch [1/1], Average Loss: 2.0980, Time Spent: 2165s\n",
      "7.617505868384084e-05\n",
      "272/5000 done, time elapsed since training start: 2165s\n",
      "Dataset 273 created\n",
      "Epoch [1/1], Average Loss: 2.0645, Time Spent: 2174s\n",
      "7.6098883625157e-05\n",
      "273/5000 done, time elapsed since training start: 2174s\n",
      "Dataset 274 created\n",
      "Epoch [1/1], Average Loss: 2.0958, Time Spent: 2182s\n",
      "7.602278474153185e-05\n",
      "274/5000 done, time elapsed since training start: 2182s\n",
      "Dataset 275 created\n",
      "Epoch [1/1], Average Loss: 2.0856, Time Spent: 2190s\n",
      "7.594676195679031e-05\n",
      "275/5000 done, time elapsed since training start: 2190s\n",
      "Dataset 276 created\n",
      "Epoch [1/1], Average Loss: 2.1181, Time Spent: 2198s\n",
      "7.587081519483353e-05\n",
      "276/5000 done, time elapsed since training start: 2198s\n",
      "Dataset 277 created\n",
      "Epoch [1/1], Average Loss: 2.1267, Time Spent: 2206s\n",
      "7.57949443796387e-05\n",
      "277/5000 done, time elapsed since training start: 2206s\n",
      "Dataset 278 created\n",
      "Epoch [1/1], Average Loss: 2.1064, Time Spent: 2214s\n",
      "7.571914943525906e-05\n",
      "278/5000 done, time elapsed since training start: 2214s\n",
      "Dataset 279 created\n",
      "Epoch [1/1], Average Loss: 2.0861, Time Spent: 2222s\n",
      "7.56434302858238e-05\n",
      "279/5000 done, time elapsed since training start: 2222s\n",
      "Dataset 280 created\n",
      "Epoch [1/1], Average Loss: 2.0562, Time Spent: 2229s\n",
      "7.556778685553798e-05\n",
      "280/5000 done, time elapsed since training start: 2229s\n",
      "Dataset 281 created\n",
      "Epoch [1/1], Average Loss: 2.0843, Time Spent: 2237s\n",
      "7.549221906868244e-05\n",
      "281/5000 done, time elapsed since training start: 2237s\n",
      "Dataset 282 created\n",
      "Epoch [1/1], Average Loss: 2.0787, Time Spent: 2245s\n",
      "7.541672684961375e-05\n",
      "282/5000 done, time elapsed since training start: 2245s\n",
      "Dataset 283 created\n",
      "Epoch [1/1], Average Loss: 2.1118, Time Spent: 2253s\n",
      "7.534131012276413e-05\n",
      "283/5000 done, time elapsed since training start: 2253s\n",
      "Dataset 284 created\n",
      "Epoch [1/1], Average Loss: 2.0866, Time Spent: 2261s\n",
      "7.526596881264138e-05\n",
      "284/5000 done, time elapsed since training start: 2261s\n",
      "Dataset 285 created\n",
      "Epoch [1/1], Average Loss: 2.0891, Time Spent: 2269s\n",
      "7.519070284382874e-05\n",
      "285/5000 done, time elapsed since training start: 2269s\n",
      "Dataset 286 created\n",
      "Epoch [1/1], Average Loss: 2.1031, Time Spent: 2277s\n",
      "7.511551214098491e-05\n",
      "286/5000 done, time elapsed since training start: 2277s\n",
      "Dataset 287 created\n",
      "Epoch [1/1], Average Loss: 2.0775, Time Spent: 2285s\n",
      "7.504039662884393e-05\n",
      "287/5000 done, time elapsed since training start: 2285s\n",
      "Dataset 288 created\n",
      "Epoch [1/1], Average Loss: 2.1173, Time Spent: 2293s\n",
      "7.496535623221508e-05\n",
      "288/5000 done, time elapsed since training start: 2293s\n",
      "Dataset 289 created\n",
      "Epoch [1/1], Average Loss: 2.1020, Time Spent: 2300s\n",
      "7.489039087598286e-05\n",
      "289/5000 done, time elapsed since training start: 2300s\n",
      "Dataset 290 created\n",
      "Epoch [1/1], Average Loss: 2.1010, Time Spent: 2308s\n",
      "7.481550048510688e-05\n",
      "290/5000 done, time elapsed since training start: 2308s\n",
      "Dataset 291 created\n",
      "Epoch [1/1], Average Loss: 2.0919, Time Spent: 2316s\n",
      "7.474068498462178e-05\n",
      "291/5000 done, time elapsed since training start: 2316s\n",
      "Dataset 292 created\n",
      "Epoch [1/1], Average Loss: 2.0559, Time Spent: 2324s\n",
      "7.466594429963716e-05\n",
      "292/5000 done, time elapsed since training start: 2324s\n",
      "Dataset 293 created\n",
      "Epoch [1/1], Average Loss: 2.0795, Time Spent: 2331s\n",
      "7.459127835533752e-05\n",
      "293/5000 done, time elapsed since training start: 2331s\n",
      "Dataset 294 created\n",
      "Epoch [1/1], Average Loss: 2.0907, Time Spent: 2339s\n",
      "7.451668707698218e-05\n",
      "294/5000 done, time elapsed since training start: 2339s\n",
      "Dataset 295 created\n",
      "Epoch [1/1], Average Loss: 2.0742, Time Spent: 2347s\n",
      "7.44421703899052e-05\n",
      "295/5000 done, time elapsed since training start: 2347s\n",
      "Dataset 296 created\n",
      "Epoch [1/1], Average Loss: 2.0994, Time Spent: 2355s\n",
      "7.43677282195153e-05\n",
      "296/5000 done, time elapsed since training start: 2355s\n",
      "Dataset 297 created\n",
      "Epoch [1/1], Average Loss: 2.1035, Time Spent: 2363s\n",
      "7.429336049129579e-05\n",
      "297/5000 done, time elapsed since training start: 2363s\n",
      "Dataset 298 created\n",
      "Epoch [1/1], Average Loss: 2.0887, Time Spent: 2370s\n",
      "7.42190671308045e-05\n",
      "298/5000 done, time elapsed since training start: 2370s\n",
      "Dataset 299 created\n",
      "Epoch [1/1], Average Loss: 2.1012, Time Spent: 2378s\n",
      "7.414484806367369e-05\n",
      "299/5000 done, time elapsed since training start: 2378s\n",
      "Dataset 300 created\n",
      "Epoch [1/1], Average Loss: 2.0849, Time Spent: 2386s\n",
      "7.407070321561001e-05\n",
      "300/5000 done, time elapsed since training start: 2386s\n",
      "Dataset 301 created\n",
      "Epoch [1/1], Average Loss: 2.0981, Time Spent: 2394s\n",
      "7.39966325123944e-05\n",
      "301/5000 done, time elapsed since training start: 2394s\n",
      "Dataset 302 created\n",
      "Epoch [1/1], Average Loss: 2.0996, Time Spent: 2402s\n",
      "7.392263587988201e-05\n",
      "302/5000 done, time elapsed since training start: 2402s\n",
      "Dataset 303 created\n",
      "Epoch [1/1], Average Loss: 2.1061, Time Spent: 2410s\n",
      "7.384871324400212e-05\n",
      "303/5000 done, time elapsed since training start: 2410s\n",
      "Dataset 304 created\n",
      "Epoch [1/1], Average Loss: 2.0884, Time Spent: 2418s\n",
      "7.377486453075813e-05\n",
      "304/5000 done, time elapsed since training start: 2418s\n",
      "Dataset 305 created\n",
      "Epoch [1/1], Average Loss: 2.1072, Time Spent: 2425s\n",
      "7.370108966622737e-05\n",
      "305/5000 done, time elapsed since training start: 2425s\n",
      "Dataset 306 created\n",
      "Epoch [1/1], Average Loss: 2.1160, Time Spent: 2433s\n",
      "7.362738857656114e-05\n",
      "306/5000 done, time elapsed since training start: 2433s\n",
      "Dataset 307 created\n",
      "Epoch [1/1], Average Loss: 2.0896, Time Spent: 2441s\n",
      "7.355376118798457e-05\n",
      "307/5000 done, time elapsed since training start: 2441s\n",
      "Dataset 308 created\n",
      "Epoch [1/1], Average Loss: 2.1283, Time Spent: 2449s\n",
      "7.348020742679659e-05\n",
      "308/5000 done, time elapsed since training start: 2449s\n",
      "Dataset 309 created\n",
      "Epoch [1/1], Average Loss: 2.0876, Time Spent: 2457s\n",
      "7.340672721936979e-05\n",
      "309/5000 done, time elapsed since training start: 2457s\n",
      "Dataset 310 created\n",
      "Epoch [1/1], Average Loss: 2.1100, Time Spent: 2465s\n",
      "7.333332049215042e-05\n",
      "310/5000 done, time elapsed since training start: 2465s\n",
      "Dataset 311 created\n",
      "Epoch [1/1], Average Loss: 2.0780, Time Spent: 2472s\n",
      "7.325998717165827e-05\n",
      "311/5000 done, time elapsed since training start: 2472s\n",
      "Dataset 312 created\n",
      "Epoch [1/1], Average Loss: 2.1132, Time Spent: 2480s\n",
      "7.318672718448662e-05\n",
      "312/5000 done, time elapsed since training start: 2480s\n",
      "Dataset 313 created\n",
      "Epoch [1/1], Average Loss: 2.0936, Time Spent: 2488s\n",
      "7.311354045730212e-05\n",
      "313/5000 done, time elapsed since training start: 2488s\n",
      "Dataset 314 created\n",
      "Epoch [1/1], Average Loss: 2.0650, Time Spent: 2496s\n",
      "7.304042691684482e-05\n",
      "314/5000 done, time elapsed since training start: 2496s\n",
      "Dataset 315 created\n",
      "Epoch [1/1], Average Loss: 2.0802, Time Spent: 2504s\n",
      "7.296738648992797e-05\n",
      "315/5000 done, time elapsed since training start: 2504s\n",
      "Dataset 316 created\n",
      "Epoch [1/1], Average Loss: 2.0885, Time Spent: 2511s\n",
      "7.289441910343804e-05\n",
      "316/5000 done, time elapsed since training start: 2511s\n",
      "Dataset 317 created\n",
      "Epoch [1/1], Average Loss: 2.1043, Time Spent: 2519s\n",
      "7.282152468433461e-05\n",
      "317/5000 done, time elapsed since training start: 2519s\n",
      "Dataset 318 created\n",
      "Epoch [1/1], Average Loss: 2.1144, Time Spent: 2528s\n",
      "7.274870315965027e-05\n",
      "318/5000 done, time elapsed since training start: 2528s\n",
      "Dataset 319 created\n",
      "Epoch [1/1], Average Loss: 2.1031, Time Spent: 2536s\n",
      "7.267595445649063e-05\n",
      "319/5000 done, time elapsed since training start: 2536s\n",
      "Dataset 320 created\n",
      "Epoch [1/1], Average Loss: 2.1049, Time Spent: 2544s\n",
      "7.260327850203413e-05\n",
      "320/5000 done, time elapsed since training start: 2544s\n",
      "Dataset 321 created\n",
      "Epoch [1/1], Average Loss: 2.1196, Time Spent: 2552s\n",
      "7.25306752235321e-05\n",
      "321/5000 done, time elapsed since training start: 2552s\n",
      "Dataset 322 created\n",
      "Epoch [1/1], Average Loss: 2.0844, Time Spent: 2560s\n",
      "7.245814454830857e-05\n",
      "322/5000 done, time elapsed since training start: 2560s\n",
      "Dataset 323 created\n",
      "Epoch [1/1], Average Loss: 2.0977, Time Spent: 2568s\n",
      "7.238568640376027e-05\n",
      "323/5000 done, time elapsed since training start: 2568s\n",
      "Dataset 324 created\n",
      "Epoch [1/1], Average Loss: 2.0586, Time Spent: 2576s\n",
      "7.23133007173565e-05\n",
      "324/5000 done, time elapsed since training start: 2576s\n",
      "Dataset 325 created\n",
      "Epoch [1/1], Average Loss: 2.0984, Time Spent: 2584s\n",
      "7.224098741663915e-05\n",
      "325/5000 done, time elapsed since training start: 2584s\n",
      "Dataset 326 created\n",
      "Epoch [1/1], Average Loss: 2.0901, Time Spent: 2592s\n",
      "7.216874642922251e-05\n",
      "326/5000 done, time elapsed since training start: 2592s\n",
      "Dataset 327 created\n",
      "Epoch [1/1], Average Loss: 2.0970, Time Spent: 2600s\n",
      "7.20965776827933e-05\n",
      "327/5000 done, time elapsed since training start: 2600s\n",
      "Dataset 328 created\n",
      "Epoch [1/1], Average Loss: 2.0889, Time Spent: 2608s\n",
      "7.20244811051105e-05\n",
      "328/5000 done, time elapsed since training start: 2608s\n",
      "Dataset 329 created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Average Loss: 2.0976, Time Spent: 2616s\n",
      "7.195245662400539e-05\n",
      "329/5000 done, time elapsed since training start: 2616s\n",
      "Dataset 330 created\n",
      "Epoch [1/1], Average Loss: 2.1205, Time Spent: 2623s\n",
      "7.188050416738138e-05\n",
      "330/5000 done, time elapsed since training start: 2623s\n",
      "Dataset 331 created\n",
      "Epoch [1/1], Average Loss: 2.0850, Time Spent: 2631s\n",
      "7.1808623663214e-05\n",
      "331/5000 done, time elapsed since training start: 2631s\n",
      "Dataset 332 created\n",
      "Epoch [1/1], Average Loss: 2.0940, Time Spent: 2639s\n",
      "7.173681503955078e-05\n",
      "332/5000 done, time elapsed since training start: 2639s\n",
      "Dataset 333 created\n",
      "Epoch [1/1], Average Loss: 2.1049, Time Spent: 2647s\n",
      "7.166507822451122e-05\n",
      "333/5000 done, time elapsed since training start: 2647s\n",
      "Dataset 334 created\n",
      "Epoch [1/1], Average Loss: 2.0969, Time Spent: 2654s\n",
      "7.15934131462867e-05\n",
      "334/5000 done, time elapsed since training start: 2654s\n",
      "Dataset 335 created\n",
      "Epoch [1/1], Average Loss: 2.0886, Time Spent: 2662s\n",
      "7.152181973314041e-05\n",
      "335/5000 done, time elapsed since training start: 2662s\n",
      "Dataset 336 created\n",
      "Epoch [1/1], Average Loss: 2.1054, Time Spent: 2670s\n",
      "7.145029791340727e-05\n",
      "336/5000 done, time elapsed since training start: 2670s\n",
      "Dataset 337 created\n",
      "Epoch [1/1], Average Loss: 2.0878, Time Spent: 2678s\n",
      "7.137884761549386e-05\n",
      "337/5000 done, time elapsed since training start: 2678s\n",
      "Dataset 338 created\n",
      "Epoch [1/1], Average Loss: 2.1008, Time Spent: 2686s\n",
      "7.130746876787837e-05\n",
      "338/5000 done, time elapsed since training start: 2686s\n",
      "Dataset 339 created\n",
      "Epoch [1/1], Average Loss: 2.1080, Time Spent: 2693s\n",
      "7.12361612991105e-05\n",
      "339/5000 done, time elapsed since training start: 2693s\n",
      "Dataset 340 created\n",
      "Epoch [1/1], Average Loss: 2.0845, Time Spent: 2701s\n",
      "7.116492513781138e-05\n",
      "340/5000 done, time elapsed since training start: 2701s\n",
      "Dataset 341 created\n",
      "Epoch [1/1], Average Loss: 2.0981, Time Spent: 2709s\n",
      "7.109376021267357e-05\n",
      "341/5000 done, time elapsed since training start: 2709s\n",
      "Dataset 342 created\n",
      "Epoch [1/1], Average Loss: 2.0583, Time Spent: 2717s\n",
      "7.10226664524609e-05\n",
      "342/5000 done, time elapsed since training start: 2717s\n",
      "Dataset 343 created\n",
      "Epoch [1/1], Average Loss: 2.1295, Time Spent: 2725s\n",
      "7.095164378600844e-05\n",
      "343/5000 done, time elapsed since training start: 2725s\n",
      "Dataset 344 created\n",
      "Epoch [1/1], Average Loss: 2.0815, Time Spent: 2733s\n",
      "7.088069214222243e-05\n",
      "344/5000 done, time elapsed since training start: 2733s\n",
      "Dataset 345 created\n",
      "Epoch [1/1], Average Loss: 2.0486, Time Spent: 2741s\n",
      "7.08098114500802e-05\n",
      "345/5000 done, time elapsed since training start: 2741s\n",
      "Dataset 346 created\n",
      "Epoch [1/1], Average Loss: 2.1200, Time Spent: 2748s\n",
      "7.073900163863012e-05\n",
      "346/5000 done, time elapsed since training start: 2748s\n",
      "Dataset 347 created\n",
      "Epoch [1/1], Average Loss: 2.1134, Time Spent: 2757s\n",
      "7.06682626369915e-05\n",
      "347/5000 done, time elapsed since training start: 2757s\n",
      "Dataset 348 created\n",
      "Epoch [1/1], Average Loss: 2.1270, Time Spent: 2765s\n",
      "7.05975943743545e-05\n",
      "348/5000 done, time elapsed since training start: 2765s\n",
      "Dataset 349 created\n",
      "Epoch [1/1], Average Loss: 2.0744, Time Spent: 2773s\n",
      "7.052699677998015e-05\n",
      "349/5000 done, time elapsed since training start: 2773s\n",
      "Dataset 350 created\n",
      "Epoch [1/1], Average Loss: 2.0904, Time Spent: 2781s\n",
      "7.045646978320017e-05\n",
      "350/5000 done, time elapsed since training start: 2781s\n",
      "Dataset 351 created\n",
      "Epoch [1/1], Average Loss: 2.1125, Time Spent: 2789s\n",
      "7.038601331341697e-05\n",
      "351/5000 done, time elapsed since training start: 2789s\n",
      "Dataset 352 created\n",
      "Epoch [1/1], Average Loss: 2.1024, Time Spent: 2796s\n",
      "7.031562730010355e-05\n",
      "352/5000 done, time elapsed since training start: 2796s\n",
      "Dataset 353 created\n",
      "Epoch [1/1], Average Loss: 2.1087, Time Spent: 2804s\n",
      "7.024531167280345e-05\n",
      "353/5000 done, time elapsed since training start: 2804s\n",
      "Dataset 354 created\n",
      "Epoch [1/1], Average Loss: 2.0799, Time Spent: 2812s\n",
      "7.017506636113065e-05\n",
      "354/5000 done, time elapsed since training start: 2812s\n",
      "Dataset 355 created\n",
      "Epoch [1/1], Average Loss: 2.0907, Time Spent: 2820s\n",
      "7.010489129476952e-05\n",
      "355/5000 done, time elapsed since training start: 2820s\n",
      "Dataset 356 created\n",
      "Epoch [1/1], Average Loss: 2.0972, Time Spent: 2827s\n",
      "7.003478640347476e-05\n",
      "356/5000 done, time elapsed since training start: 2827s\n",
      "Dataset 357 created\n",
      "Epoch [1/1], Average Loss: 2.0877, Time Spent: 2835s\n",
      "6.996475161707129e-05\n",
      "357/5000 done, time elapsed since training start: 2835s\n",
      "Dataset 358 created\n",
      "Epoch [1/1], Average Loss: 2.0977, Time Spent: 2843s\n",
      "6.989478686545422e-05\n",
      "358/5000 done, time elapsed since training start: 2843s\n",
      "Dataset 359 created\n",
      "Epoch [1/1], Average Loss: 2.0937, Time Spent: 2851s\n",
      "6.982489207858876e-05\n",
      "359/5000 done, time elapsed since training start: 2851s\n",
      "Dataset 360 created\n",
      "Epoch [1/1], Average Loss: 2.0938, Time Spent: 2859s\n",
      "6.975506718651018e-05\n",
      "360/5000 done, time elapsed since training start: 2859s\n",
      "Dataset 361 created\n",
      "Epoch [1/1], Average Loss: 2.1089, Time Spent: 2866s\n",
      "6.968531211932367e-05\n",
      "361/5000 done, time elapsed since training start: 2866s\n",
      "Dataset 362 created\n",
      "Epoch [1/1], Average Loss: 2.1086, Time Spent: 2874s\n",
      "6.961562680720434e-05\n",
      "362/5000 done, time elapsed since training start: 2874s\n",
      "Dataset 363 created\n",
      "Epoch [1/1], Average Loss: 2.0747, Time Spent: 2882s\n",
      "6.954601118039714e-05\n",
      "363/5000 done, time elapsed since training start: 2882s\n",
      "Dataset 364 created\n",
      "Epoch [1/1], Average Loss: 2.0824, Time Spent: 2890s\n",
      "6.947646516921674e-05\n",
      "364/5000 done, time elapsed since training start: 2890s\n",
      "Dataset 365 created\n",
      "Epoch [1/1], Average Loss: 2.1070, Time Spent: 2898s\n",
      "6.940698870404753e-05\n",
      "365/5000 done, time elapsed since training start: 2898s\n",
      "Dataset 366 created\n",
      "Epoch [1/1], Average Loss: 2.0685, Time Spent: 2906s\n",
      "6.933758171534347e-05\n",
      "366/5000 done, time elapsed since training start: 2906s\n",
      "Dataset 367 created\n",
      "Epoch [1/1], Average Loss: 2.1090, Time Spent: 2914s\n",
      "6.926824413362814e-05\n",
      "367/5000 done, time elapsed since training start: 2914s\n",
      "Dataset 368 created\n",
      "Epoch [1/1], Average Loss: 2.1084, Time Spent: 2921s\n",
      "6.919897588949451e-05\n",
      "368/5000 done, time elapsed since training start: 2921s\n",
      "Dataset 369 created\n",
      "Epoch [1/1], Average Loss: 2.0860, Time Spent: 2929s\n",
      "6.912977691360502e-05\n",
      "369/5000 done, time elapsed since training start: 2929s\n",
      "Dataset 370 created\n",
      "Epoch [1/1], Average Loss: 2.0831, Time Spent: 2937s\n",
      "6.906064713669141e-05\n",
      "370/5000 done, time elapsed since training start: 2937s\n",
      "Dataset 371 created\n",
      "Epoch [1/1], Average Loss: 2.0972, Time Spent: 2945s\n",
      "6.899158648955472e-05\n",
      "371/5000 done, time elapsed since training start: 2945s\n",
      "Dataset 372 created\n",
      "Epoch [1/1], Average Loss: 2.0864, Time Spent: 2953s\n",
      "6.892259490306517e-05\n",
      "372/5000 done, time elapsed since training start: 2953s\n",
      "Dataset 373 created\n",
      "Epoch [1/1], Average Loss: 2.1262, Time Spent: 2961s\n",
      "6.88536723081621e-05\n",
      "373/5000 done, time elapsed since training start: 2961s\n",
      "Dataset 374 created\n",
      "Epoch [1/1], Average Loss: 2.0790, Time Spent: 2969s\n",
      "6.878481863585394e-05\n",
      "374/5000 done, time elapsed since training start: 2969s\n",
      "Dataset 375 created\n",
      "Epoch [1/1], Average Loss: 2.1137, Time Spent: 2976s\n",
      "6.871603381721808e-05\n",
      "375/5000 done, time elapsed since training start: 2976s\n",
      "Dataset 376 created\n",
      "Epoch [1/1], Average Loss: 2.1093, Time Spent: 2984s\n",
      "6.864731778340087e-05\n",
      "376/5000 done, time elapsed since training start: 2984s\n",
      "Dataset 377 created\n",
      "Epoch [1/1], Average Loss: 2.0948, Time Spent: 2992s\n",
      "6.857867046561747e-05\n",
      "377/5000 done, time elapsed since training start: 2992s\n",
      "Dataset 378 created\n",
      "Epoch [1/1], Average Loss: 2.1068, Time Spent: 3000s\n",
      "6.851009179515185e-05\n",
      "378/5000 done, time elapsed since training start: 3000s\n",
      "Dataset 379 created\n",
      "Epoch [1/1], Average Loss: 2.0839, Time Spent: 3008s\n",
      "6.84415817033567e-05\n",
      "379/5000 done, time elapsed since training start: 3008s\n",
      "Dataset 380 created\n",
      "Epoch [1/1], Average Loss: 2.0934, Time Spent: 3015s\n",
      "6.837314012165334e-05\n",
      "380/5000 done, time elapsed since training start: 3015s\n",
      "Dataset 381 created\n",
      "Epoch [1/1], Average Loss: 2.1116, Time Spent: 3023s\n",
      "6.830476698153169e-05\n",
      "381/5000 done, time elapsed since training start: 3023s\n",
      "Dataset 382 created\n",
      "Epoch [1/1], Average Loss: 2.1041, Time Spent: 3031s\n",
      "6.823646221455015e-05\n",
      "382/5000 done, time elapsed since training start: 3031s\n",
      "Dataset 383 created\n",
      "Epoch [1/1], Average Loss: 2.0828, Time Spent: 3039s\n",
      "6.81682257523356e-05\n",
      "383/5000 done, time elapsed since training start: 3039s\n",
      "Dataset 384 created\n",
      "Epoch [1/1], Average Loss: 2.1104, Time Spent: 3047s\n",
      "6.810005752658326e-05\n",
      "384/5000 done, time elapsed since training start: 3047s\n",
      "Dataset 385 created\n",
      "Epoch [1/1], Average Loss: 2.0750, Time Spent: 3054s\n",
      "6.803195746905668e-05\n",
      "385/5000 done, time elapsed since training start: 3055s\n",
      "Dataset 386 created\n",
      "Epoch [1/1], Average Loss: 2.0930, Time Spent: 3063s\n",
      "6.796392551158762e-05\n",
      "386/5000 done, time elapsed since training start: 3063s\n",
      "Dataset 387 created\n",
      "Epoch [1/1], Average Loss: 2.1057, Time Spent: 3071s\n",
      "6.789596158607602e-05\n",
      "387/5000 done, time elapsed since training start: 3071s\n",
      "Dataset 388 created\n",
      "Epoch [1/1], Average Loss: 2.0706, Time Spent: 3079s\n",
      "6.782806562448995e-05\n",
      "388/5000 done, time elapsed since training start: 3079s\n",
      "Dataset 389 created\n",
      "Epoch [1/1], Average Loss: 2.0670, Time Spent: 3087s\n",
      "6.776023755886546e-05\n",
      "389/5000 done, time elapsed since training start: 3087s\n",
      "Dataset 390 created\n",
      "Epoch [1/1], Average Loss: 2.0981, Time Spent: 3094s\n",
      "6.769247732130659e-05\n",
      "390/5000 done, time elapsed since training start: 3094s\n",
      "Dataset 391 created\n",
      "Epoch [1/1], Average Loss: 2.0946, Time Spent: 3102s\n",
      "6.762478484398528e-05\n",
      "391/5000 done, time elapsed since training start: 3102s\n",
      "Dataset 392 created\n",
      "Epoch [1/1], Average Loss: 2.0697, Time Spent: 3110s\n",
      "6.755716005914129e-05\n",
      "392/5000 done, time elapsed since training start: 3110s\n",
      "Dataset 393 created\n",
      "Epoch [1/1], Average Loss: 2.0991, Time Spent: 3118s\n",
      "6.748960289908216e-05\n",
      "393/5000 done, time elapsed since training start: 3118s\n",
      "Dataset 394 created\n",
      "Epoch [1/1], Average Loss: 2.0916, Time Spent: 3126s\n",
      "6.742211329618307e-05\n",
      "394/5000 done, time elapsed since training start: 3126s\n",
      "Dataset 395 created\n",
      "Epoch [1/1], Average Loss: 2.1008, Time Spent: 3134s\n",
      "6.735469118288689e-05\n",
      "395/5000 done, time elapsed since training start: 3134s\n",
      "Dataset 396 created\n",
      "Epoch [1/1], Average Loss: 2.0978, Time Spent: 3142s\n",
      "6.7287336491704e-05\n",
      "396/5000 done, time elapsed since training start: 3142s\n",
      "Dataset 397 created\n",
      "Epoch [1/1], Average Loss: 2.1067, Time Spent: 3150s\n",
      "6.72200491552123e-05\n",
      "397/5000 done, time elapsed since training start: 3150s\n",
      "Dataset 398 created\n",
      "Epoch [1/1], Average Loss: 2.0735, Time Spent: 3158s\n",
      "6.715282910605708e-05\n",
      "398/5000 done, time elapsed since training start: 3158s\n",
      "Dataset 399 created\n",
      "Epoch [1/1], Average Loss: 2.0983, Time Spent: 3166s\n",
      "6.708567627695102e-05\n",
      "399/5000 done, time elapsed since training start: 3166s\n",
      "Dataset 400 created\n",
      "Epoch [1/1], Average Loss: 2.0705, Time Spent: 3174s\n",
      "6.701859060067407e-05\n",
      "400/5000 done, time elapsed since training start: 3174s\n",
      "Dataset 401 created\n",
      "Epoch [1/1], Average Loss: 2.1054, Time Spent: 3182s\n",
      "6.69515720100734e-05\n",
      "401/5000 done, time elapsed since training start: 3182s\n",
      "Dataset 402 created\n",
      "Epoch [1/1], Average Loss: 2.0818, Time Spent: 3190s\n",
      "6.688462043806333e-05\n",
      "402/5000 done, time elapsed since training start: 3190s\n",
      "Dataset 403 created\n",
      "Epoch [1/1], Average Loss: 2.0702, Time Spent: 3198s\n",
      "6.681773581762526e-05\n",
      "403/5000 done, time elapsed since training start: 3198s\n",
      "Dataset 404 created\n",
      "Epoch [1/1], Average Loss: 2.1041, Time Spent: 3206s\n",
      "6.675091808180764e-05\n",
      "404/5000 done, time elapsed since training start: 3206s\n",
      "Dataset 405 created\n",
      "Epoch [1/1], Average Loss: 2.0561, Time Spent: 3214s\n",
      "6.668416716372583e-05\n",
      "405/5000 done, time elapsed since training start: 3214s\n",
      "Dataset 406 created\n",
      "Epoch [1/1], Average Loss: 2.0966, Time Spent: 3222s\n",
      "6.66174829965621e-05\n",
      "406/5000 done, time elapsed since training start: 3222s\n",
      "Dataset 407 created\n",
      "Epoch [1/1], Average Loss: 2.1123, Time Spent: 3230s\n",
      "6.655086551356554e-05\n",
      "407/5000 done, time elapsed since training start: 3230s\n",
      "Dataset 408 created\n",
      "Epoch [1/1], Average Loss: 2.0860, Time Spent: 3238s\n",
      "6.648431464805198e-05\n",
      "408/5000 done, time elapsed since training start: 3238s\n",
      "Dataset 409 created\n",
      "Epoch [1/1], Average Loss: 2.1163, Time Spent: 3246s\n",
      "6.641783033340393e-05\n",
      "409/5000 done, time elapsed since training start: 3246s\n",
      "Dataset 410 created\n",
      "Epoch [1/1], Average Loss: 2.0703, Time Spent: 3254s\n",
      "6.635141250307054e-05\n",
      "410/5000 done, time elapsed since training start: 3254s\n",
      "Dataset 411 created\n",
      "Epoch [1/1], Average Loss: 2.0814, Time Spent: 3262s\n",
      "6.628506109056747e-05\n",
      "411/5000 done, time elapsed since training start: 3262s\n",
      "Dataset 412 created\n",
      "Epoch [1/1], Average Loss: 2.1011, Time Spent: 3269s\n",
      "6.62187760294769e-05\n",
      "412/5000 done, time elapsed since training start: 3269s\n",
      "Dataset 413 created\n",
      "Epoch [1/1], Average Loss: 2.0880, Time Spent: 3277s\n",
      "6.615255725344742e-05\n",
      "413/5000 done, time elapsed since training start: 3277s\n",
      "Dataset 414 created\n",
      "Epoch [1/1], Average Loss: 2.1193, Time Spent: 3285s\n",
      "6.608640469619397e-05\n",
      "414/5000 done, time elapsed since training start: 3285s\n",
      "Dataset 415 created\n",
      "Epoch [1/1], Average Loss: 2.1027, Time Spent: 3293s\n",
      "6.602031829149778e-05\n",
      "415/5000 done, time elapsed since training start: 3293s\n",
      "Dataset 416 created\n",
      "Epoch [1/1], Average Loss: 2.1181, Time Spent: 3300s\n",
      "6.595429797320629e-05\n",
      "416/5000 done, time elapsed since training start: 3300s\n",
      "Dataset 417 created\n",
      "Epoch [1/1], Average Loss: 2.1058, Time Spent: 3308s\n",
      "6.588834367523308e-05\n",
      "417/5000 done, time elapsed since training start: 3308s\n",
      "Dataset 418 created\n",
      "Epoch [1/1], Average Loss: 2.0943, Time Spent: 3316s\n",
      "6.582245533155784e-05\n",
      "418/5000 done, time elapsed since training start: 3316s\n",
      "Dataset 419 created\n",
      "Epoch [1/1], Average Loss: 2.1212, Time Spent: 3323s\n",
      "6.575663287622628e-05\n",
      "419/5000 done, time elapsed since training start: 3323s\n",
      "Dataset 420 created\n",
      "Epoch [1/1], Average Loss: 2.0967, Time Spent: 3331s\n",
      "6.569087624335006e-05\n",
      "420/5000 done, time elapsed since training start: 3331s\n",
      "Dataset 421 created\n",
      "Epoch [1/1], Average Loss: 2.0724, Time Spent: 3339s\n",
      "6.562518536710671e-05\n",
      "421/5000 done, time elapsed since training start: 3339s\n",
      "Dataset 422 created\n",
      "Epoch [1/1], Average Loss: 2.1235, Time Spent: 3347s\n",
      "6.55595601817396e-05\n",
      "422/5000 done, time elapsed since training start: 3347s\n",
      "Dataset 423 created\n",
      "Epoch [1/1], Average Loss: 2.1230, Time Spent: 3354s\n",
      "6.549400062155787e-05\n",
      "423/5000 done, time elapsed since training start: 3354s\n",
      "Dataset 424 created\n",
      "Epoch [1/1], Average Loss: 2.0744, Time Spent: 3362s\n",
      "6.542850662093631e-05\n",
      "424/5000 done, time elapsed since training start: 3362s\n",
      "Dataset 425 created\n",
      "Epoch [1/1], Average Loss: 2.1073, Time Spent: 3370s\n",
      "6.536307811431537e-05\n",
      "425/5000 done, time elapsed since training start: 3370s\n",
      "Dataset 426 created\n",
      "Epoch [1/1], Average Loss: 2.0748, Time Spent: 3378s\n",
      "6.529771503620105e-05\n",
      "426/5000 done, time elapsed since training start: 3378s\n",
      "Dataset 427 created\n",
      "Epoch [1/1], Average Loss: 2.1160, Time Spent: 3386s\n",
      "6.523241732116485e-05\n",
      "427/5000 done, time elapsed since training start: 3386s\n",
      "Dataset 428 created\n",
      "Epoch [1/1], Average Loss: 2.0772, Time Spent: 3393s\n",
      "6.516718490384368e-05\n",
      "428/5000 done, time elapsed since training start: 3393s\n",
      "Dataset 429 created\n",
      "Epoch [1/1], Average Loss: 2.0801, Time Spent: 3401s\n",
      "6.510201771893984e-05\n",
      "429/5000 done, time elapsed since training start: 3401s\n",
      "Dataset 430 created\n",
      "Epoch [1/1], Average Loss: 2.0759, Time Spent: 3409s\n",
      "6.50369157012209e-05\n",
      "430/5000 done, time elapsed since training start: 3409s\n",
      "Dataset 431 created\n",
      "Epoch [1/1], Average Loss: 2.0890, Time Spent: 3417s\n",
      "6.497187878551968e-05\n",
      "431/5000 done, time elapsed since training start: 3417s\n",
      "Dataset 432 created\n",
      "Epoch [1/1], Average Loss: 2.0939, Time Spent: 3426s\n",
      "6.490690690673416e-05\n",
      "432/5000 done, time elapsed since training start: 3426s\n",
      "Dataset 433 created\n",
      "Epoch [1/1], Average Loss: 2.0858, Time Spent: 3440s\n",
      "6.484199999982744e-05\n",
      "433/5000 done, time elapsed since training start: 3440s\n",
      "Dataset 434 created\n",
      "Epoch [1/1], Average Loss: 2.0629, Time Spent: 3451s\n",
      "6.477715799982761e-05\n",
      "434/5000 done, time elapsed since training start: 3451s\n",
      "Dataset 435 created\n",
      "Epoch [1/1], Average Loss: 2.0959, Time Spent: 3460s\n",
      "6.471238084182779e-05\n",
      "435/5000 done, time elapsed since training start: 3460s\n",
      "Dataset 436 created\n",
      "Epoch [1/1], Average Loss: 2.0882, Time Spent: 3469s\n",
      "6.464766846098597e-05\n",
      "436/5000 done, time elapsed since training start: 3469s\n",
      "Dataset 437 created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Average Loss: 2.0716, Time Spent: 3477s\n",
      "6.458302079252498e-05\n",
      "437/5000 done, time elapsed since training start: 3477s\n",
      "Dataset 438 created\n",
      "Epoch [1/1], Average Loss: 2.1013, Time Spent: 3486s\n",
      "6.451843777173246e-05\n",
      "438/5000 done, time elapsed since training start: 3486s\n",
      "Dataset 439 created\n",
      "Epoch [1/1], Average Loss: 2.0737, Time Spent: 3494s\n",
      "6.445391933396073e-05\n",
      "439/5000 done, time elapsed since training start: 3494s\n",
      "Dataset 440 created\n",
      "Epoch [1/1], Average Loss: 2.0808, Time Spent: 3502s\n",
      "6.438946541462677e-05\n",
      "440/5000 done, time elapsed since training start: 3502s\n",
      "Dataset 441 created\n",
      "Epoch [1/1], Average Loss: 2.1018, Time Spent: 3512s\n",
      "6.432507594921215e-05\n",
      "441/5000 done, time elapsed since training start: 3512s\n",
      "Dataset 442 created\n",
      "Epoch [1/1], Average Loss: 2.0979, Time Spent: 3521s\n",
      "6.426075087326294e-05\n",
      "442/5000 done, time elapsed since training start: 3521s\n",
      "Dataset 443 created\n",
      "Epoch [1/1], Average Loss: 2.0980, Time Spent: 3529s\n",
      "6.419649012238967e-05\n",
      "443/5000 done, time elapsed since training start: 3529s\n",
      "Dataset 444 created\n",
      "Epoch [1/1], Average Loss: 2.0784, Time Spent: 3537s\n",
      "6.413229363226728e-05\n",
      "444/5000 done, time elapsed since training start: 3537s\n",
      "Dataset 445 created\n",
      "Epoch [1/1], Average Loss: 2.0795, Time Spent: 3545s\n",
      "6.406816133863501e-05\n",
      "445/5000 done, time elapsed since training start: 3545s\n",
      "Dataset 446 created\n",
      "Epoch [1/1], Average Loss: 2.0967, Time Spent: 3553s\n",
      "6.400409317729637e-05\n",
      "446/5000 done, time elapsed since training start: 3553s\n",
      "Dataset 447 created\n",
      "Epoch [1/1], Average Loss: 2.0911, Time Spent: 3561s\n",
      "6.394008908411907e-05\n",
      "447/5000 done, time elapsed since training start: 3561s\n",
      "Dataset 448 created\n",
      "Epoch [1/1], Average Loss: 2.0912, Time Spent: 3570s\n",
      "6.387614899503495e-05\n",
      "448/5000 done, time elapsed since training start: 3570s\n",
      "Dataset 449 created\n",
      "Epoch [1/1], Average Loss: 2.0938, Time Spent: 3578s\n",
      "6.381227284603992e-05\n",
      "449/5000 done, time elapsed since training start: 3578s\n",
      "Dataset 450 created\n",
      "Epoch [1/1], Average Loss: 2.1042, Time Spent: 3586s\n",
      "6.374846057319388e-05\n",
      "450/5000 done, time elapsed since training start: 3586s\n",
      "Dataset 451 created\n",
      "Epoch [1/1], Average Loss: 2.0880, Time Spent: 3594s\n",
      "6.368471211262068e-05\n",
      "451/5000 done, time elapsed since training start: 3594s\n",
      "Dataset 452 created\n",
      "Epoch [1/1], Average Loss: 2.1013, Time Spent: 3602s\n",
      "6.362102740050806e-05\n",
      "452/5000 done, time elapsed since training start: 3602s\n",
      "Dataset 453 created\n",
      "Epoch [1/1], Average Loss: 2.0968, Time Spent: 3610s\n",
      "6.355740637310754e-05\n",
      "453/5000 done, time elapsed since training start: 3610s\n",
      "Dataset 454 created\n",
      "Epoch [1/1], Average Loss: 2.0877, Time Spent: 3619s\n",
      "6.349384896673443e-05\n",
      "454/5000 done, time elapsed since training start: 3619s\n",
      "Dataset 455 created\n",
      "Epoch [1/1], Average Loss: 2.1001, Time Spent: 3629s\n",
      "6.34303551177677e-05\n",
      "455/5000 done, time elapsed since training start: 3629s\n",
      "Dataset 456 created\n",
      "Epoch [1/1], Average Loss: 2.0895, Time Spent: 3638s\n",
      "6.336692476264994e-05\n",
      "456/5000 done, time elapsed since training start: 3638s\n",
      "Dataset 457 created\n",
      "Epoch [1/1], Average Loss: 2.0877, Time Spent: 3647s\n",
      "6.330355783788729e-05\n",
      "457/5000 done, time elapsed since training start: 3647s\n",
      "Dataset 458 created\n",
      "Epoch [1/1], Average Loss: 2.0704, Time Spent: 3656s\n",
      "6.32402542800494e-05\n",
      "458/5000 done, time elapsed since training start: 3656s\n",
      "Dataset 459 created\n",
      "Epoch [1/1], Average Loss: 2.1423, Time Spent: 3666s\n",
      "6.317701402576935e-05\n",
      "459/5000 done, time elapsed since training start: 3666s\n",
      "Dataset 460 created\n",
      "Epoch [1/1], Average Loss: 2.0977, Time Spent: 3676s\n",
      "6.311383701174358e-05\n",
      "460/5000 done, time elapsed since training start: 3676s\n",
      "Dataset 461 created\n",
      "Epoch [1/1], Average Loss: 2.1060, Time Spent: 3685s\n",
      "6.305072317473184e-05\n",
      "461/5000 done, time elapsed since training start: 3685s\n",
      "Dataset 462 created\n",
      "Epoch [1/1], Average Loss: 2.1221, Time Spent: 3694s\n",
      "6.298767245155711e-05\n",
      "462/5000 done, time elapsed since training start: 3694s\n",
      "Dataset 463 created\n",
      "Epoch [1/1], Average Loss: 2.1138, Time Spent: 3703s\n",
      "6.292468477910555e-05\n",
      "463/5000 done, time elapsed since training start: 3703s\n",
      "Dataset 464 created\n",
      "Epoch [1/1], Average Loss: 2.1188, Time Spent: 3711s\n",
      "6.286176009432645e-05\n",
      "464/5000 done, time elapsed since training start: 3711s\n",
      "Dataset 465 created\n",
      "Epoch [1/1], Average Loss: 2.1000, Time Spent: 3719s\n",
      "6.279889833423212e-05\n",
      "465/5000 done, time elapsed since training start: 3719s\n",
      "Dataset 466 created\n",
      "Epoch [1/1], Average Loss: 2.1040, Time Spent: 3727s\n",
      "6.273609943589789e-05\n",
      "466/5000 done, time elapsed since training start: 3727s\n",
      "Dataset 467 created\n",
      "Epoch [1/1], Average Loss: 2.1059, Time Spent: 3735s\n",
      "6.267336333646199e-05\n",
      "467/5000 done, time elapsed since training start: 3735s\n",
      "Dataset 468 created\n",
      "Epoch [1/1], Average Loss: 2.0641, Time Spent: 3742s\n",
      "6.261068997312553e-05\n",
      "468/5000 done, time elapsed since training start: 3742s\n",
      "Dataset 469 created\n",
      "Epoch [1/1], Average Loss: 2.0572, Time Spent: 3750s\n",
      "6.254807928315242e-05\n",
      "469/5000 done, time elapsed since training start: 3750s\n",
      "Dataset 470 created\n",
      "Epoch [1/1], Average Loss: 2.1059, Time Spent: 3758s\n",
      "6.248553120386926e-05\n",
      "470/5000 done, time elapsed since training start: 3758s\n",
      "Dataset 471 created\n",
      "Epoch [1/1], Average Loss: 2.0554, Time Spent: 3766s\n",
      "6.242304567266539e-05\n",
      "471/5000 done, time elapsed since training start: 3766s\n",
      "Dataset 472 created\n",
      "Epoch [1/1], Average Loss: 2.1084, Time Spent: 3774s\n",
      "6.236062262699272e-05\n",
      "472/5000 done, time elapsed since training start: 3774s\n",
      "Dataset 473 created\n",
      "Epoch [1/1], Average Loss: 2.0877, Time Spent: 3782s\n",
      "6.229826200436573e-05\n",
      "473/5000 done, time elapsed since training start: 3782s\n",
      "Dataset 474 created\n",
      "Epoch [1/1], Average Loss: 2.1027, Time Spent: 3791s\n",
      "6.223596374236136e-05\n",
      "474/5000 done, time elapsed since training start: 3791s\n",
      "Dataset 475 created\n",
      "Epoch [1/1], Average Loss: 2.1011, Time Spent: 3799s\n",
      "6.2173727778619e-05\n",
      "475/5000 done, time elapsed since training start: 3799s\n",
      "Dataset 476 created\n",
      "Epoch [1/1], Average Loss: 2.1257, Time Spent: 3808s\n",
      "6.211155405084038e-05\n",
      "476/5000 done, time elapsed since training start: 3808s\n",
      "Dataset 477 created\n",
      "Epoch [1/1], Average Loss: 2.0952, Time Spent: 3817s\n",
      "6.204944249678954e-05\n",
      "477/5000 done, time elapsed since training start: 3817s\n",
      "Dataset 478 created\n",
      "Epoch [1/1], Average Loss: 2.0846, Time Spent: 3825s\n",
      "6.198739305429274e-05\n",
      "478/5000 done, time elapsed since training start: 3825s\n",
      "Dataset 479 created\n",
      "Epoch [1/1], Average Loss: 2.0554, Time Spent: 3834s\n",
      "6.192540566123845e-05\n",
      "479/5000 done, time elapsed since training start: 3834s\n",
      "Dataset 480 created\n",
      "Epoch [1/1], Average Loss: 2.0979, Time Spent: 3843s\n",
      "6.186348025557721e-05\n",
      "480/5000 done, time elapsed since training start: 3843s\n",
      "Dataset 481 created\n",
      "Epoch [1/1], Average Loss: 2.0808, Time Spent: 3851s\n",
      "6.180161677532164e-05\n",
      "481/5000 done, time elapsed since training start: 3851s\n",
      "Dataset 482 created\n",
      "Epoch [1/1], Average Loss: 2.0723, Time Spent: 3860s\n",
      "6.173981515854632e-05\n",
      "482/5000 done, time elapsed since training start: 3860s\n",
      "Dataset 483 created\n",
      "Epoch [1/1], Average Loss: 2.0857, Time Spent: 3868s\n",
      "6.167807534338778e-05\n",
      "483/5000 done, time elapsed since training start: 3868s\n",
      "Dataset 484 created\n",
      "Epoch [1/1], Average Loss: 2.0847, Time Spent: 3875s\n",
      "6.161639726804439e-05\n",
      "484/5000 done, time elapsed since training start: 3875s\n",
      "Dataset 485 created\n",
      "Epoch [1/1], Average Loss: 2.0822, Time Spent: 3883s\n",
      "6.155478087077634e-05\n",
      "485/5000 done, time elapsed since training start: 3883s\n",
      "Dataset 486 created\n",
      "Epoch [1/1], Average Loss: 2.0665, Time Spent: 3891s\n",
      "6.149322608990556e-05\n",
      "486/5000 done, time elapsed since training start: 3891s\n",
      "Dataset 487 created\n",
      "Epoch [1/1], Average Loss: 2.0952, Time Spent: 3900s\n",
      "6.143173286381566e-05\n",
      "487/5000 done, time elapsed since training start: 3900s\n",
      "Dataset 488 created\n",
      "Epoch [1/1], Average Loss: 2.0806, Time Spent: 3909s\n",
      "6.137030113095184e-05\n",
      "488/5000 done, time elapsed since training start: 3909s\n",
      "Dataset 489 created\n",
      "Epoch [1/1], Average Loss: 2.0693, Time Spent: 3919s\n",
      "6.130893082982089e-05\n",
      "489/5000 done, time elapsed since training start: 3919s\n",
      "Dataset 490 created\n",
      "Epoch [1/1], Average Loss: 2.0881, Time Spent: 3928s\n",
      "6.124762189899106e-05\n",
      "490/5000 done, time elapsed since training start: 3928s\n",
      "Dataset 491 created\n",
      "Epoch [1/1], Average Loss: 2.0968, Time Spent: 3936s\n",
      "6.118637427709207e-05\n",
      "491/5000 done, time elapsed since training start: 3936s\n",
      "Dataset 492 created\n",
      "Epoch [1/1], Average Loss: 2.1006, Time Spent: 3945s\n",
      "6.112518790281498e-05\n",
      "492/5000 done, time elapsed since training start: 3945s\n",
      "Dataset 493 created\n",
      "Epoch [1/1], Average Loss: 2.0958, Time Spent: 3953s\n",
      "6.106406271491216e-05\n",
      "493/5000 done, time elapsed since training start: 3953s\n",
      "Dataset 494 created\n",
      "Epoch [1/1], Average Loss: 2.0957, Time Spent: 3961s\n",
      "6.100299865219725e-05\n",
      "494/5000 done, time elapsed since training start: 3961s\n",
      "Dataset 495 created\n",
      "Epoch [1/1], Average Loss: 2.1102, Time Spent: 3970s\n",
      "6.0941995653545055e-05\n",
      "495/5000 done, time elapsed since training start: 3970s\n",
      "Dataset 496 created\n",
      "Epoch [1/1], Average Loss: 2.1209, Time Spent: 3979s\n",
      "6.088105365789151e-05\n",
      "496/5000 done, time elapsed since training start: 3979s\n",
      "Dataset 497 created\n",
      "Epoch [1/1], Average Loss: 2.0857, Time Spent: 3988s\n",
      "6.082017260423362e-05\n",
      "497/5000 done, time elapsed since training start: 3988s\n",
      "Dataset 498 created\n",
      "Epoch [1/1], Average Loss: 2.0754, Time Spent: 3998s\n",
      "6.0759352431629384e-05\n",
      "498/5000 done, time elapsed since training start: 3998s\n",
      "Dataset 499 created\n",
      "Epoch [1/1], Average Loss: 2.0979, Time Spent: 4006s\n",
      "6.069859307919775e-05\n",
      "499/5000 done, time elapsed since training start: 4006s\n",
      "Dataset 500 created\n",
      "Epoch [1/1], Average Loss: 2.0739, Time Spent: 4014s\n",
      "6.063789448611855e-05\n",
      "500/5000 done, time elapsed since training start: 4014s\n",
      "Dataset 501 created\n",
      "Epoch [1/1], Average Loss: 2.1218, Time Spent: 4022s\n",
      "6.0577256591632435e-05\n",
      "501/5000 done, time elapsed since training start: 4022s\n",
      "Dataset 502 created\n",
      "Epoch [1/1], Average Loss: 2.0775, Time Spent: 4031s\n",
      "6.0516679335040805e-05\n",
      "502/5000 done, time elapsed since training start: 4031s\n",
      "Dataset 503 created\n",
      "Epoch [1/1], Average Loss: 2.0868, Time Spent: 4039s\n",
      "6.0456162655705763e-05\n",
      "503/5000 done, time elapsed since training start: 4039s\n",
      "Dataset 504 created\n",
      "Epoch [1/1], Average Loss: 2.0786, Time Spent: 4048s\n",
      "6.039570649305006e-05\n",
      "504/5000 done, time elapsed since training start: 4048s\n",
      "Dataset 505 created\n",
      "Epoch [1/1], Average Loss: 2.1014, Time Spent: 4056s\n",
      "6.033531078655701e-05\n",
      "505/5000 done, time elapsed since training start: 4056s\n",
      "Dataset 506 created\n",
      "Epoch [1/1], Average Loss: 2.0998, Time Spent: 4065s\n",
      "6.027497547577045e-05\n",
      "506/5000 done, time elapsed since training start: 4065s\n",
      "Dataset 507 created\n",
      "Epoch [1/1], Average Loss: 2.0878, Time Spent: 4074s\n",
      "6.021470050029468e-05\n",
      "507/5000 done, time elapsed since training start: 4074s\n",
      "Dataset 508 created\n",
      "Epoch [1/1], Average Loss: 2.0959, Time Spent: 4082s\n",
      "6.015448579979439e-05\n",
      "508/5000 done, time elapsed since training start: 4082s\n",
      "Dataset 509 created\n",
      "Epoch [1/1], Average Loss: 2.0897, Time Spent: 4090s\n",
      "6.0094331313994595e-05\n",
      "509/5000 done, time elapsed since training start: 4090s\n",
      "Dataset 510 created\n",
      "Epoch [1/1], Average Loss: 2.1187, Time Spent: 4098s\n",
      "6.00342369826806e-05\n",
      "510/5000 done, time elapsed since training start: 4098s\n",
      "Dataset 511 created\n",
      "Epoch [1/1], Average Loss: 2.1114, Time Spent: 4109s\n",
      "5.9974202745697915e-05\n",
      "511/5000 done, time elapsed since training start: 4109s\n",
      "Dataset 512 created\n",
      "Epoch [1/1], Average Loss: 2.0907, Time Spent: 4118s\n",
      "5.991422854295222e-05\n",
      "512/5000 done, time elapsed since training start: 4118s\n",
      "Dataset 513 created\n",
      "Epoch [1/1], Average Loss: 2.0467, Time Spent: 4127s\n",
      "5.9854314314409264e-05\n",
      "513/5000 done, time elapsed since training start: 4127s\n",
      "Dataset 514 created\n",
      "Epoch [1/1], Average Loss: 2.0965, Time Spent: 4136s\n",
      "5.9794460000094856e-05\n",
      "514/5000 done, time elapsed since training start: 4136s\n",
      "Dataset 515 created\n",
      "Epoch [1/1], Average Loss: 2.0901, Time Spent: 4147s\n",
      "5.9734665540094763e-05\n",
      "515/5000 done, time elapsed since training start: 4147s\n",
      "Dataset 516 created\n",
      "Epoch [1/1], Average Loss: 2.0741, Time Spent: 4157s\n",
      "5.967493087455467e-05\n",
      "516/5000 done, time elapsed since training start: 4157s\n",
      "Dataset 517 created\n",
      "Epoch [1/1], Average Loss: 2.1060, Time Spent: 4165s\n",
      "5.961525594368011e-05\n",
      "517/5000 done, time elapsed since training start: 4165s\n",
      "Dataset 518 created\n",
      "Epoch [1/1], Average Loss: 2.0971, Time Spent: 4175s\n",
      "5.9555640687736434e-05\n",
      "518/5000 done, time elapsed since training start: 4175s\n",
      "Dataset 519 created\n",
      "Epoch [1/1], Average Loss: 2.0974, Time Spent: 4185s\n",
      "5.9496085047048696e-05\n",
      "519/5000 done, time elapsed since training start: 4185s\n",
      "Dataset 520 created\n",
      "Epoch [1/1], Average Loss: 2.1020, Time Spent: 4196s\n",
      "5.9436588962001644e-05\n",
      "520/5000 done, time elapsed since training start: 4196s\n",
      "Dataset 521 created\n",
      "Epoch [1/1], Average Loss: 2.0764, Time Spent: 4208s\n",
      "5.937715237303964e-05\n",
      "521/5000 done, time elapsed since training start: 4208s\n",
      "Dataset 522 created\n",
      "Epoch [1/1], Average Loss: 2.0679, Time Spent: 4218s\n",
      "5.93177752206666e-05\n",
      "522/5000 done, time elapsed since training start: 4218s\n",
      "Dataset 523 created\n",
      "Epoch [1/1], Average Loss: 2.1110, Time Spent: 4228s\n",
      "5.925845744544593e-05\n",
      "523/5000 done, time elapsed since training start: 4228s\n",
      "Dataset 524 created\n",
      "Epoch [1/1], Average Loss: 2.0793, Time Spent: 4238s\n",
      "5.919919898800048e-05\n",
      "524/5000 done, time elapsed since training start: 4238s\n",
      "Dataset 525 created\n",
      "Epoch [1/1], Average Loss: 2.0895, Time Spent: 4247s\n",
      "5.913999978901248e-05\n",
      "525/5000 done, time elapsed since training start: 4247s\n",
      "Dataset 526 created\n",
      "Epoch [1/1], Average Loss: 2.0772, Time Spent: 4257s\n",
      "5.908085978922347e-05\n",
      "526/5000 done, time elapsed since training start: 4257s\n",
      "Dataset 527 created\n",
      "Epoch [1/1], Average Loss: 2.0935, Time Spent: 4267s\n",
      "5.902177892943425e-05\n",
      "527/5000 done, time elapsed since training start: 4267s\n",
      "Dataset 528 created\n",
      "Epoch [1/1], Average Loss: 2.0707, Time Spent: 4275s\n",
      "5.896275715050481e-05\n",
      "528/5000 done, time elapsed since training start: 4275s\n",
      "Dataset 529 created\n",
      "Epoch [1/1], Average Loss: 2.0921, Time Spent: 4283s\n",
      "5.890379439335431e-05\n",
      "529/5000 done, time elapsed since training start: 4283s\n",
      "Dataset 530 created\n",
      "Epoch [1/1], Average Loss: 2.0832, Time Spent: 4293s\n",
      "5.8844890598960953e-05\n",
      "530/5000 done, time elapsed since training start: 4293s\n",
      "Dataset 531 created\n",
      "Epoch [1/1], Average Loss: 2.1011, Time Spent: 4303s\n",
      "5.878604570836199e-05\n",
      "531/5000 done, time elapsed since training start: 4303s\n",
      "Dataset 532 created\n",
      "Epoch [1/1], Average Loss: 2.0966, Time Spent: 4314s\n",
      "5.872725966265363e-05\n",
      "532/5000 done, time elapsed since training start: 4314s\n",
      "Dataset 533 created\n",
      "Epoch [1/1], Average Loss: 2.0916, Time Spent: 4324s\n",
      "5.866853240299098e-05\n",
      "533/5000 done, time elapsed since training start: 4324s\n",
      "Dataset 534 created\n",
      "Epoch [1/1], Average Loss: 2.1027, Time Spent: 4333s\n",
      "5.8609863870587986e-05\n",
      "534/5000 done, time elapsed since training start: 4333s\n",
      "Dataset 535 created\n",
      "Epoch [1/1], Average Loss: 2.0922, Time Spent: 4341s\n",
      "5.8551254006717395e-05\n",
      "535/5000 done, time elapsed since training start: 4341s\n",
      "Dataset 536 created\n",
      "Epoch [1/1], Average Loss: 2.0745, Time Spent: 4350s\n",
      "5.849270275271068e-05\n",
      "536/5000 done, time elapsed since training start: 4350s\n",
      "Dataset 537 created\n",
      "Epoch [1/1], Average Loss: 2.0836, Time Spent: 4358s\n",
      "5.843421004995797e-05\n",
      "537/5000 done, time elapsed since training start: 4358s\n",
      "Dataset 538 created\n",
      "Epoch [1/1], Average Loss: 2.0695, Time Spent: 4366s\n",
      "5.837577583990801e-05\n",
      "538/5000 done, time elapsed since training start: 4366s\n",
      "Dataset 539 created\n",
      "Epoch [1/1], Average Loss: 2.0994, Time Spent: 4374s\n",
      "5.83174000640681e-05\n",
      "539/5000 done, time elapsed since training start: 4374s\n",
      "Dataset 540 created\n",
      "Epoch [1/1], Average Loss: 2.0819, Time Spent: 4383s\n",
      "5.825908266400403e-05\n",
      "540/5000 done, time elapsed since training start: 4383s\n",
      "Dataset 541 created\n",
      "Epoch [1/1], Average Loss: 2.0926, Time Spent: 4391s\n",
      "5.8200823581340026e-05\n",
      "541/5000 done, time elapsed since training start: 4391s\n",
      "Dataset 542 created\n",
      "Epoch [1/1], Average Loss: 2.1031, Time Spent: 4400s\n",
      "5.8142622757758685e-05\n",
      "542/5000 done, time elapsed since training start: 4400s\n",
      "Dataset 543 created\n",
      "Epoch [1/1], Average Loss: 2.0787, Time Spent: 4410s\n",
      "5.8084480135000926e-05\n",
      "543/5000 done, time elapsed since training start: 4410s\n",
      "Dataset 544 created\n",
      "Epoch [1/1], Average Loss: 2.0715, Time Spent: 4419s\n",
      "5.802639565486592e-05\n",
      "544/5000 done, time elapsed since training start: 4419s\n",
      "Dataset 545 created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Average Loss: 2.0594, Time Spent: 4429s\n",
      "5.7968369259211056e-05\n",
      "545/5000 done, time elapsed since training start: 4429s\n",
      "Dataset 546 created\n",
      "Epoch [1/1], Average Loss: 2.1020, Time Spent: 4439s\n",
      "5.791040088995184e-05\n",
      "546/5000 done, time elapsed since training start: 4439s\n",
      "Dataset 547 created\n",
      "Epoch [1/1], Average Loss: 2.1019, Time Spent: 4448s\n",
      "5.785249048906189e-05\n",
      "547/5000 done, time elapsed since training start: 4448s\n",
      "Dataset 548 created\n",
      "Epoch [1/1], Average Loss: 2.0671, Time Spent: 4456s\n",
      "5.7794637998572825e-05\n",
      "548/5000 done, time elapsed since training start: 4456s\n",
      "Dataset 549 created\n",
      "Epoch [1/1], Average Loss: 2.1072, Time Spent: 4464s\n",
      "5.773684336057425e-05\n",
      "549/5000 done, time elapsed since training start: 4464s\n",
      "Dataset 550 created\n",
      "Epoch [1/1], Average Loss: 2.0934, Time Spent: 4472s\n",
      "5.767910651721368e-05\n",
      "550/5000 done, time elapsed since training start: 4472s\n",
      "Dataset 551 created\n",
      "Epoch [1/1], Average Loss: 2.0949, Time Spent: 4480s\n",
      "5.7621427410696465e-05\n",
      "551/5000 done, time elapsed since training start: 4480s\n",
      "Dataset 552 created\n",
      "Epoch [1/1], Average Loss: 2.0905, Time Spent: 4488s\n",
      "5.7563805983285766e-05\n",
      "552/5000 done, time elapsed since training start: 4488s\n",
      "Dataset 553 created\n",
      "Epoch [1/1], Average Loss: 2.0575, Time Spent: 4496s\n",
      "5.750624217730248e-05\n",
      "553/5000 done, time elapsed since training start: 4496s\n",
      "Dataset 554 created\n",
      "Epoch [1/1], Average Loss: 2.1043, Time Spent: 4504s\n",
      "5.7448735935125173e-05\n",
      "554/5000 done, time elapsed since training start: 4504s\n",
      "Dataset 555 created\n",
      "Epoch [1/1], Average Loss: 2.0734, Time Spent: 4512s\n",
      "5.739128719919005e-05\n",
      "555/5000 done, time elapsed since training start: 4512s\n",
      "Dataset 556 created\n",
      "Epoch [1/1], Average Loss: 2.1073, Time Spent: 4519s\n",
      "5.733389591199086e-05\n",
      "556/5000 done, time elapsed since training start: 4519s\n",
      "Dataset 557 created\n",
      "Epoch [1/1], Average Loss: 2.1103, Time Spent: 4528s\n",
      "5.727656201607886e-05\n",
      "557/5000 done, time elapsed since training start: 4528s\n",
      "Dataset 558 created\n",
      "Epoch [1/1], Average Loss: 2.1086, Time Spent: 4537s\n",
      "5.721928545406279e-05\n",
      "558/5000 done, time elapsed since training start: 4537s\n",
      "Dataset 559 created\n",
      "Epoch [1/1], Average Loss: 2.0977, Time Spent: 4546s\n",
      "5.7162066168608725e-05\n",
      "559/5000 done, time elapsed since training start: 4546s\n",
      "Dataset 560 created\n",
      "Epoch [1/1], Average Loss: 2.1023, Time Spent: 4554s\n",
      "5.710490410244012e-05\n",
      "560/5000 done, time elapsed since training start: 4554s\n",
      "Dataset 561 created\n",
      "Epoch [1/1], Average Loss: 2.0925, Time Spent: 4563s\n",
      "5.7047799198337674e-05\n",
      "561/5000 done, time elapsed since training start: 4563s\n",
      "Dataset 562 created\n",
      "Epoch [1/1], Average Loss: 2.0838, Time Spent: 4572s\n",
      "5.6990751399139336e-05\n",
      "562/5000 done, time elapsed since training start: 4572s\n",
      "Dataset 563 created\n",
      "Epoch [1/1], Average Loss: 2.0936, Time Spent: 4582s\n",
      "5.6933760647740194e-05\n",
      "563/5000 done, time elapsed since training start: 4582s\n",
      "Dataset 564 created\n",
      "Epoch [1/1], Average Loss: 2.1001, Time Spent: 4590s\n",
      "5.687682688709245e-05\n",
      "564/5000 done, time elapsed since training start: 4590s\n",
      "Dataset 565 created\n",
      "Epoch [1/1], Average Loss: 2.0630, Time Spent: 4599s\n",
      "5.681995006020536e-05\n",
      "565/5000 done, time elapsed since training start: 4599s\n",
      "Dataset 566 created\n",
      "Epoch [1/1], Average Loss: 2.0929, Time Spent: 4607s\n",
      "5.676313011014515e-05\n",
      "566/5000 done, time elapsed since training start: 4607s\n",
      "Dataset 567 created\n",
      "Epoch [1/1], Average Loss: 2.0895, Time Spent: 4616s\n",
      "5.6706366980035006e-05\n",
      "567/5000 done, time elapsed since training start: 4616s\n",
      "Dataset 568 created\n",
      "Epoch [1/1], Average Loss: 2.0803, Time Spent: 4624s\n",
      "5.6649660613054974e-05\n",
      "568/5000 done, time elapsed since training start: 4624s\n",
      "Dataset 569 created\n",
      "Epoch [1/1], Average Loss: 2.0797, Time Spent: 4632s\n",
      "5.659301095244192e-05\n",
      "569/5000 done, time elapsed since training start: 4632s\n",
      "Dataset 570 created\n",
      "Epoch [1/1], Average Loss: 2.0617, Time Spent: 4640s\n",
      "5.653641794148948e-05\n",
      "570/5000 done, time elapsed since training start: 4640s\n",
      "Dataset 571 created\n",
      "Epoch [1/1], Average Loss: 2.0861, Time Spent: 4648s\n",
      "5.6479881523547984e-05\n",
      "571/5000 done, time elapsed since training start: 4648s\n",
      "Dataset 572 created\n",
      "Epoch [1/1], Average Loss: 2.0737, Time Spent: 4656s\n",
      "5.642340164202443e-05\n",
      "572/5000 done, time elapsed since training start: 4656s\n",
      "Dataset 573 created\n",
      "Epoch [1/1], Average Loss: 2.0990, Time Spent: 4664s\n",
      "5.636697824038241e-05\n",
      "573/5000 done, time elapsed since training start: 4664s\n",
      "Dataset 574 created\n",
      "Epoch [1/1], Average Loss: 2.1229, Time Spent: 4672s\n",
      "5.6310611262142026e-05\n",
      "574/5000 done, time elapsed since training start: 4672s\n",
      "Dataset 575 created\n",
      "Epoch [1/1], Average Loss: 2.0679, Time Spent: 4680s\n",
      "5.6254300650879884e-05\n",
      "575/5000 done, time elapsed since training start: 4680s\n",
      "Dataset 576 created\n",
      "Epoch [1/1], Average Loss: 2.0960, Time Spent: 4689s\n",
      "5.619804635022901e-05\n",
      "576/5000 done, time elapsed since training start: 4689s\n",
      "Dataset 577 created\n",
      "Epoch [1/1], Average Loss: 2.0983, Time Spent: 4697s\n",
      "5.614184830387878e-05\n",
      "577/5000 done, time elapsed since training start: 4697s\n",
      "Dataset 578 created\n",
      "Epoch [1/1], Average Loss: 2.1034, Time Spent: 4705s\n",
      "5.60857064555749e-05\n",
      "578/5000 done, time elapsed since training start: 4705s\n",
      "Dataset 579 created\n",
      "Epoch [1/1], Average Loss: 2.1053, Time Spent: 4713s\n",
      "5.6029620749119324e-05\n",
      "579/5000 done, time elapsed since training start: 4713s\n",
      "Dataset 580 created\n",
      "Epoch [1/1], Average Loss: 2.0929, Time Spent: 4722s\n",
      "5.5973591128370206e-05\n",
      "580/5000 done, time elapsed since training start: 4722s\n",
      "Dataset 581 created\n",
      "Epoch [1/1], Average Loss: 2.0784, Time Spent: 4730s\n",
      "5.5917617537241836e-05\n",
      "581/5000 done, time elapsed since training start: 4730s\n",
      "Dataset 582 created\n",
      "Epoch [1/1], Average Loss: 2.0948, Time Spent: 4738s\n",
      "5.586169991970459e-05\n",
      "582/5000 done, time elapsed since training start: 4738s\n",
      "Dataset 583 created\n",
      "Epoch [1/1], Average Loss: 2.1146, Time Spent: 4746s\n",
      "5.580583821978489e-05\n",
      "583/5000 done, time elapsed since training start: 4746s\n",
      "Dataset 584 created\n",
      "Epoch [1/1], Average Loss: 2.0796, Time Spent: 4754s\n",
      "5.57500323815651e-05\n",
      "584/5000 done, time elapsed since training start: 4754s\n",
      "Dataset 585 created\n",
      "Epoch [1/1], Average Loss: 2.0846, Time Spent: 4763s\n",
      "5.5694282349183536e-05\n",
      "585/5000 done, time elapsed since training start: 4763s\n",
      "Dataset 586 created\n",
      "Epoch [1/1], Average Loss: 2.0913, Time Spent: 4771s\n",
      "5.5638588066834355e-05\n",
      "586/5000 done, time elapsed since training start: 4771s\n",
      "Dataset 587 created\n",
      "Epoch [1/1], Average Loss: 2.0982, Time Spent: 4780s\n",
      "5.558294947876752e-05\n",
      "587/5000 done, time elapsed since training start: 4780s\n",
      "Dataset 588 created\n",
      "Epoch [1/1], Average Loss: 2.0975, Time Spent: 4788s\n",
      "5.552736652928875e-05\n",
      "588/5000 done, time elapsed since training start: 4788s\n",
      "Dataset 589 created\n",
      "Epoch [1/1], Average Loss: 2.0805, Time Spent: 4796s\n",
      "5.547183916275946e-05\n",
      "589/5000 done, time elapsed since training start: 4796s\n",
      "Dataset 590 created\n",
      "Epoch [1/1], Average Loss: 2.0936, Time Spent: 4804s\n",
      "5.54163673235967e-05\n",
      "590/5000 done, time elapsed since training start: 4804s\n",
      "Dataset 591 created\n",
      "Epoch [1/1], Average Loss: 2.0802, Time Spent: 4813s\n",
      "5.53609509562731e-05\n",
      "591/5000 done, time elapsed since training start: 4813s\n",
      "Dataset 592 created\n",
      "Epoch [1/1], Average Loss: 2.0828, Time Spent: 4821s\n",
      "5.530559000531683e-05\n",
      "592/5000 done, time elapsed since training start: 4821s\n",
      "Dataset 593 created\n",
      "Epoch [1/1], Average Loss: 2.1033, Time Spent: 4829s\n",
      "5.5250284415311514e-05\n",
      "593/5000 done, time elapsed since training start: 4829s\n",
      "Dataset 594 created\n",
      "Epoch [1/1], Average Loss: 2.0970, Time Spent: 4838s\n",
      "5.5195034130896205e-05\n",
      "594/5000 done, time elapsed since training start: 4838s\n",
      "Dataset 595 created\n",
      "Epoch [1/1], Average Loss: 2.0772, Time Spent: 4846s\n",
      "5.513983909676531e-05\n",
      "595/5000 done, time elapsed since training start: 4846s\n",
      "Dataset 596 created\n",
      "Epoch [1/1], Average Loss: 2.1137, Time Spent: 4854s\n",
      "5.508469925766855e-05\n",
      "596/5000 done, time elapsed since training start: 4854s\n",
      "Dataset 597 created\n",
      "Epoch [1/1], Average Loss: 2.0551, Time Spent: 4862s\n",
      "5.502961455841088e-05\n",
      "597/5000 done, time elapsed since training start: 4862s\n",
      "Dataset 598 created\n",
      "Epoch [1/1], Average Loss: 2.0874, Time Spent: 4871s\n",
      "5.497458494385247e-05\n",
      "598/5000 done, time elapsed since training start: 4871s\n",
      "Dataset 599 created\n",
      "Epoch [1/1], Average Loss: 2.1133, Time Spent: 4879s\n",
      "5.491961035890862e-05\n",
      "599/5000 done, time elapsed since training start: 4879s\n",
      "Dataset 600 created\n",
      "Epoch [1/1], Average Loss: 2.0889, Time Spent: 4888s\n",
      "5.486469074854971e-05\n",
      "600/5000 done, time elapsed since training start: 4888s\n",
      "Dataset 601 created\n",
      "Epoch [1/1], Average Loss: 2.0711, Time Spent: 4897s\n",
      "5.480982605780116e-05\n",
      "601/5000 done, time elapsed since training start: 4897s\n",
      "Dataset 602 created\n",
      "Epoch [1/1], Average Loss: 2.0975, Time Spent: 4905s\n",
      "5.475501623174336e-05\n",
      "602/5000 done, time elapsed since training start: 4905s\n",
      "Dataset 603 created\n",
      "Epoch [1/1], Average Loss: 2.0940, Time Spent: 4913s\n",
      "5.470026121551162e-05\n",
      "603/5000 done, time elapsed since training start: 4913s\n",
      "Dataset 604 created\n",
      "Epoch [1/1], Average Loss: 2.0856, Time Spent: 4921s\n",
      "5.4645560954296105e-05\n",
      "604/5000 done, time elapsed since training start: 4921s\n",
      "Dataset 605 created\n",
      "Epoch [1/1], Average Loss: 2.0739, Time Spent: 4929s\n",
      "5.4590915393341806e-05\n",
      "605/5000 done, time elapsed since training start: 4929s\n",
      "Dataset 606 created\n",
      "Epoch [1/1], Average Loss: 2.0769, Time Spent: 4937s\n",
      "5.4536324477948465e-05\n",
      "606/5000 done, time elapsed since training start: 4937s\n",
      "Dataset 607 created\n",
      "Epoch [1/1], Average Loss: 2.0935, Time Spent: 4946s\n",
      "5.4481788153470516e-05\n",
      "607/5000 done, time elapsed since training start: 4946s\n",
      "Dataset 608 created\n",
      "Epoch [1/1], Average Loss: 2.1151, Time Spent: 4954s\n",
      "5.4427306365317046e-05\n",
      "608/5000 done, time elapsed since training start: 4954s\n",
      "Dataset 609 created\n",
      "Epoch [1/1], Average Loss: 2.1134, Time Spent: 4962s\n",
      "5.437287905895173e-05\n",
      "609/5000 done, time elapsed since training start: 4962s\n",
      "Dataset 610 created\n",
      "Epoch [1/1], Average Loss: 2.0744, Time Spent: 4970s\n",
      "5.431850617989278e-05\n",
      "610/5000 done, time elapsed since training start: 4970s\n",
      "Dataset 611 created\n",
      "Epoch [1/1], Average Loss: 2.0897, Time Spent: 4978s\n",
      "5.426418767371288e-05\n",
      "611/5000 done, time elapsed since training start: 4978s\n",
      "Dataset 612 created\n",
      "Epoch [1/1], Average Loss: 2.0887, Time Spent: 4986s\n",
      "5.420992348603917e-05\n",
      "612/5000 done, time elapsed since training start: 4986s\n",
      "Dataset 613 created\n",
      "Epoch [1/1], Average Loss: 2.1055, Time Spent: 4995s\n",
      "5.4155713562553126e-05\n",
      "613/5000 done, time elapsed since training start: 4995s\n",
      "Dataset 614 created\n",
      "Epoch [1/1], Average Loss: 2.0851, Time Spent: 5003s\n",
      "5.4101557848990575e-05\n",
      "614/5000 done, time elapsed since training start: 5003s\n",
      "Dataset 615 created\n",
      "Epoch [1/1], Average Loss: 2.1088, Time Spent: 5011s\n",
      "5.404745629114158e-05\n",
      "615/5000 done, time elapsed since training start: 5011s\n",
      "Dataset 616 created\n",
      "Epoch [1/1], Average Loss: 2.0779, Time Spent: 5020s\n",
      "5.399340883485044e-05\n",
      "616/5000 done, time elapsed since training start: 5020s\n",
      "Dataset 617 created\n",
      "Epoch [1/1], Average Loss: 2.0953, Time Spent: 5029s\n",
      "5.393941542601559e-05\n",
      "617/5000 done, time elapsed since training start: 5029s\n",
      "Dataset 618 created\n",
      "Epoch [1/1], Average Loss: 2.1271, Time Spent: 5037s\n",
      "5.388547601058957e-05\n",
      "618/5000 done, time elapsed since training start: 5037s\n",
      "Dataset 619 created\n",
      "Epoch [1/1], Average Loss: 2.0637, Time Spent: 5045s\n",
      "5.383159053457898e-05\n",
      "619/5000 done, time elapsed since training start: 5046s\n",
      "Dataset 620 created\n",
      "Epoch [1/1], Average Loss: 2.0801, Time Spent: 5054s\n",
      "5.37777589440444e-05\n",
      "620/5000 done, time elapsed since training start: 5054s\n",
      "Dataset 621 created\n",
      "Epoch [1/1], Average Loss: 2.0765, Time Spent: 5063s\n",
      "5.372398118510036e-05\n",
      "621/5000 done, time elapsed since training start: 5063s\n",
      "Dataset 622 created\n",
      "Epoch [1/1], Average Loss: 2.1170, Time Spent: 5071s\n",
      "5.3670257203915256e-05\n",
      "622/5000 done, time elapsed since training start: 5071s\n",
      "Dataset 623 created\n",
      "Epoch [1/1], Average Loss: 2.1235, Time Spent: 5080s\n",
      "5.361658694671134e-05\n",
      "623/5000 done, time elapsed since training start: 5080s\n",
      "Dataset 624 created\n",
      "Epoch [1/1], Average Loss: 2.1154, Time Spent: 5089s\n",
      "5.3562970359764634e-05\n",
      "624/5000 done, time elapsed since training start: 5089s\n",
      "Dataset 625 created\n",
      "Epoch [1/1], Average Loss: 2.0790, Time Spent: 5097s\n",
      "5.350940738940487e-05\n",
      "625/5000 done, time elapsed since training start: 5097s\n",
      "Dataset 626 created\n",
      "Epoch [1/1], Average Loss: 2.0706, Time Spent: 5106s\n",
      "5.345589798201546e-05\n",
      "626/5000 done, time elapsed since training start: 5106s\n",
      "Dataset 627 created\n",
      "Epoch [1/1], Average Loss: 2.0823, Time Spent: 5115s\n",
      "5.3402442084033445e-05\n",
      "627/5000 done, time elapsed since training start: 5115s\n",
      "Dataset 628 created\n",
      "Epoch [1/1], Average Loss: 2.0649, Time Spent: 5124s\n",
      "5.334903964194941e-05\n",
      "628/5000 done, time elapsed since training start: 5124s\n",
      "Dataset 629 created\n",
      "Epoch [1/1], Average Loss: 2.0739, Time Spent: 5133s\n",
      "5.329569060230746e-05\n",
      "629/5000 done, time elapsed since training start: 5133s\n",
      "Dataset 630 created\n",
      "Epoch [1/1], Average Loss: 2.1072, Time Spent: 5142s\n",
      "5.3242394911705153e-05\n",
      "630/5000 done, time elapsed since training start: 5142s\n",
      "Dataset 631 created\n",
      "Epoch [1/1], Average Loss: 2.1007, Time Spent: 5151s\n",
      "5.318915251679345e-05\n",
      "631/5000 done, time elapsed since training start: 5151s\n",
      "Dataset 632 created\n",
      "Epoch [1/1], Average Loss: 2.0857, Time Spent: 5161s\n",
      "5.3135963364276656e-05\n",
      "632/5000 done, time elapsed since training start: 5161s\n",
      "Dataset 633 created\n",
      "Epoch [1/1], Average Loss: 2.0894, Time Spent: 5173s\n",
      "5.3082827400912376e-05\n",
      "633/5000 done, time elapsed since training start: 5173s\n",
      "Dataset 634 created\n",
      "Epoch [1/1], Average Loss: 2.0769, Time Spent: 5183s\n",
      "5.3029744573511465e-05\n",
      "634/5000 done, time elapsed since training start: 5183s\n",
      "Dataset 635 created\n",
      "Epoch [1/1], Average Loss: 2.0724, Time Spent: 5193s\n",
      "5.2976714828937954e-05\n",
      "635/5000 done, time elapsed since training start: 5193s\n",
      "Dataset 636 created\n",
      "Epoch [1/1], Average Loss: 2.1118, Time Spent: 5203s\n",
      "5.292373811410901e-05\n",
      "636/5000 done, time elapsed since training start: 5203s\n",
      "Dataset 637 created\n",
      "Epoch [1/1], Average Loss: 2.0888, Time Spent: 5211s\n",
      "5.2870814375994903e-05\n",
      "637/5000 done, time elapsed since training start: 5211s\n",
      "Dataset 638 created\n",
      "Epoch [1/1], Average Loss: 2.1058, Time Spent: 5220s\n",
      "5.2817943561618906e-05\n",
      "638/5000 done, time elapsed since training start: 5220s\n",
      "Dataset 639 created\n",
      "Epoch [1/1], Average Loss: 2.1089, Time Spent: 5229s\n",
      "5.2765125618057284e-05\n",
      "639/5000 done, time elapsed since training start: 5229s\n",
      "Dataset 640 created\n",
      "Epoch [1/1], Average Loss: 2.0964, Time Spent: 5238s\n",
      "5.2712360492439224e-05\n",
      "640/5000 done, time elapsed since training start: 5238s\n",
      "Dataset 641 created\n",
      "Epoch [1/1], Average Loss: 2.1051, Time Spent: 5246s\n",
      "5.2659648131946785e-05\n",
      "641/5000 done, time elapsed since training start: 5246s\n",
      "Dataset 642 created\n",
      "Epoch [1/1], Average Loss: 2.1053, Time Spent: 5254s\n",
      "5.260698848381484e-05\n",
      "642/5000 done, time elapsed since training start: 5254s\n",
      "Dataset 643 created\n",
      "Epoch [1/1], Average Loss: 2.0750, Time Spent: 5262s\n",
      "5.2554381495331025e-05\n",
      "643/5000 done, time elapsed since training start: 5262s\n",
      "Dataset 644 created\n",
      "Epoch [1/1], Average Loss: 2.0889, Time Spent: 5270s\n",
      "5.250182711383569e-05\n",
      "644/5000 done, time elapsed since training start: 5270s\n",
      "Dataset 645 created\n",
      "Epoch [1/1], Average Loss: 2.1122, Time Spent: 5278s\n",
      "5.244932528672185e-05\n",
      "645/5000 done, time elapsed since training start: 5278s\n",
      "Dataset 646 created\n",
      "Epoch [1/1], Average Loss: 2.0884, Time Spent: 5287s\n",
      "5.239687596143513e-05\n",
      "646/5000 done, time elapsed since training start: 5287s\n",
      "Dataset 647 created\n",
      "Epoch [1/1], Average Loss: 2.0910, Time Spent: 5295s\n",
      "5.23444790854737e-05\n",
      "647/5000 done, time elapsed since training start: 5295s\n",
      "Dataset 648 created\n",
      "Epoch [1/1], Average Loss: 2.0881, Time Spent: 5302s\n",
      "5.2292134606388224e-05\n",
      "648/5000 done, time elapsed since training start: 5302s\n",
      "Dataset 649 created\n",
      "Epoch [1/1], Average Loss: 2.0992, Time Spent: 5310s\n",
      "5.2239842471781836e-05\n",
      "649/5000 done, time elapsed since training start: 5310s\n",
      "Dataset 650 created\n",
      "Epoch [1/1], Average Loss: 2.0877, Time Spent: 5318s\n",
      "5.2187602629310054e-05\n",
      "650/5000 done, time elapsed since training start: 5318s\n",
      "Dataset 651 created\n",
      "Epoch [1/1], Average Loss: 2.0927, Time Spent: 5326s\n",
      "5.213541502668074e-05\n",
      "651/5000 done, time elapsed since training start: 5326s\n",
      "Dataset 652 created\n",
      "Epoch [1/1], Average Loss: 2.1011, Time Spent: 5334s\n",
      "5.2083279611654064e-05\n",
      "652/5000 done, time elapsed since training start: 5334s\n",
      "Dataset 653 created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Average Loss: 2.0907, Time Spent: 5343s\n",
      "5.203119633204241e-05\n",
      "653/5000 done, time elapsed since training start: 5343s\n",
      "Dataset 654 created\n",
      "Epoch [1/1], Average Loss: 2.1030, Time Spent: 5351s\n",
      "5.197916513571037e-05\n",
      "654/5000 done, time elapsed since training start: 5351s\n",
      "Dataset 655 created\n",
      "Epoch [1/1], Average Loss: 2.0970, Time Spent: 5359s\n",
      "5.192718597057466e-05\n",
      "655/5000 done, time elapsed since training start: 5359s\n",
      "Dataset 656 created\n",
      "Epoch [1/1], Average Loss: 2.0833, Time Spent: 5368s\n",
      "5.1875258784604084e-05\n",
      "656/5000 done, time elapsed since training start: 5368s\n",
      "Dataset 657 created\n",
      "Epoch [1/1], Average Loss: 2.0616, Time Spent: 5376s\n",
      "5.182338352581948e-05\n",
      "657/5000 done, time elapsed since training start: 5376s\n",
      "Dataset 658 created\n",
      "Epoch [1/1], Average Loss: 2.0812, Time Spent: 5384s\n",
      "5.177156014229366e-05\n",
      "658/5000 done, time elapsed since training start: 5384s\n",
      "Dataset 659 created\n",
      "Epoch [1/1], Average Loss: 2.0897, Time Spent: 5393s\n",
      "5.171978858215137e-05\n",
      "659/5000 done, time elapsed since training start: 5393s\n",
      "Dataset 660 created\n",
      "Epoch [1/1], Average Loss: 2.0883, Time Spent: 5402s\n",
      "5.166806879356922e-05\n",
      "660/5000 done, time elapsed since training start: 5402s\n",
      "Dataset 661 created\n",
      "Epoch [1/1], Average Loss: 2.0725, Time Spent: 5411s\n",
      "5.161640072477565e-05\n",
      "661/5000 done, time elapsed since training start: 5411s\n",
      "Dataset 662 created\n",
      "Epoch [1/1], Average Loss: 2.0964, Time Spent: 5419s\n",
      "5.156478432405087e-05\n",
      "662/5000 done, time elapsed since training start: 5419s\n",
      "Dataset 663 created\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-3847362c29ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m# Backpropagation and optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network.train()\n",
    "\n",
    "t = time.time()\n",
    "for rep in range(N):\n",
    "    dataset = smart_dataset(nr_of_algs, data_exp, min_alg_length, max_alg_length)\n",
    "    print(f\"Dataset {rep+1} created\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        i = 0\n",
    "        for input_data, target in dataset:\n",
    "            # Forward pass\n",
    "            output = network(input_data)\n",
    "\n",
    "            # Encode the target as class probabilities\n",
    "            target_onehot = F.one_hot(target, num_classes=18).float().squeeze()\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target_onehot)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            i+=1\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        average_loss = total_loss / len(dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}, Time Spent: {int(time.time()-t)}s\")\n",
    "    \n",
    "    optimizer.param_groups[0]['lr'] *= u\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    print(f\"{rep+1}/{N} done, time elapsed since training start: {int(time.time()-t)}s\")\n",
    "\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Specify a path\n",
    "PATH = \"nn_testt.pt\" #replace after use to not overwrite\n",
    "\n",
    "# # Save\n",
    "# torch.save(network.state_dict(), PATH)\n",
    "# network = ResNetModel()\n",
    "# network.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1216,  0.0193,  0.7977, -0.1878,  0.3387,  0.4995,  0.7464, -0.6312,\n",
       "          0.1056, -0.2269, -0.4448,  0.1897,  0.0219,  0.0268, -0.6919, -0.3570,\n",
       "         -0.1027,  0.0529]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if x.shape != out.shape:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 64, num_blocks=2)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=2)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=2)\n",
    "        self.layer4 = self._make_layer(256, 512, num_blocks=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the ResNet model\n",
    "input_size = 240\n",
    "num_classes = 18  # Including 0 to 17 (total 18 classes)\n",
    "model = ResNet(input_size, num_classes)\n",
    "\n",
    "# Generate random input tensor\n",
    "input_tensor = torch.randint(0, 2, (1, 1, input_size)).float()\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5]])\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "scalar_tensor = torch.tensor(5)  # Scalar tensor of dimension []\n",
    "list_tensor = scalar_tensor.unsqueeze(0)  # Transforming into [[]] by unsqueezing\n",
    "list_tensor = list_tensor.unsqueeze(0)  # Transforming into [[]] by unsqueezing\n",
    "\n",
    "print(list_tensor)\n",
    "print(scalar_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "Epoch: 0, Loss: 6.7970\n",
      "Epoch: 1, Loss: 6.9823\n",
      "Epoch: 2, Loss: 7.0937\n",
      "Epoch: 3, Loss: 7.4850\n",
      "Epoch: 4, Loss: 5.4554\n",
      "Epoch: 5, Loss: 7.2952\n",
      "Epoch: 6, Loss: 5.6416\n",
      "Epoch: 7, Loss: 4.5398\n",
      "Epoch: 8, Loss: 5.0171\n",
      "Epoch: 9, Loss: 2.7460\n",
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = ResNet18()\n",
    "input_tensor = torch.randn(1, 3, 224, 224)  # Example input tensor with shape (batch_size, channels, height, width)\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Output shape: (1, num_classes)\n",
    "\n",
    "# Initialize the optimizer and loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate random test data\n",
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(10):\n",
    "    # ...load and prepare your training data...\n",
    "    target_tensor = torch.tensor([random.randint(0,9)])\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "    loss = criterion(output, target_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss or other metrics during training\n",
    "    print('Epoch: {}, Loss: {:.4f}'.format(epoch, loss.item()))\n",
    "\n",
    "# After training, you can use the trained model for inference\n",
    "test_input_tensor = torch.randn(1, 3, 224, 224)  # Example input tensor for testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_input_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "\n",
    "print('Predicted class:', predicted_class.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6600,  0.7657,  0.3886, -0.4003,  0.6522, -0.2796,  0.3820, -0.0923,\n",
      "        -0.6773, -0.2975, -0.6874, -0.0036, -0.0055,  0.4915, -0.0099, -0.4279,\n",
      "        -0.4902, -0.2987], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.ReLU()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(BasicBlock, 512, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 64, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the input size and number of classes\n",
    "input_size = 240\n",
    "num_classes = 18  # 0 to 17 (18 classes in total)\n",
    "\n",
    "# Create an instance of the ResNet model\n",
    "network = ResNet(input_size, num_classes)\n",
    "\n",
    "# Create random input tensor for demonstration\n",
    "input_tensor = torch.randn(1,1,input_size)  # Example input tensor with shape (batch_size, 1, input_size)\n",
    "\n",
    "# Forward pass\n",
    "# print(input_tensor)\n",
    "output = network(input_tensor)[0]\n",
    "print(output)  # Output shape: (1, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algs_to_dataset(algs, min_alg_length):\n",
    "    # total number of data points: nr_of_algs*(max_alg_length - min_alg_length + 1)\n",
    "    dataset = []\n",
    "    for alg in algs:\n",
    "        cube = Cube()\n",
    "        '''\n",
    "        \n",
    "        M LEGGE INN ROTASJON HER?\n",
    "        \n",
    "        '''\n",
    "        cube.apply_moves(alg)\n",
    "        alg_moves = alg.split(\" \")\n",
    "        for j in range(len(alg_moves)-1,min_alg_length-2,-1):\n",
    "            inv = inverse_alg(alg_moves[j])\n",
    "            dataset.append((cube_to_tensor_2(cube).unsqueeze(0).unsqueeze(0),torch.tensor([moves.index(inv)])))\n",
    "            cube.apply_moves(inv)\n",
    "    return dataset\n",
    "\n",
    "def evalute_cube(cube):\n",
    "    state_tensor = cube_to_tensor_2(cube).unsqueeze(0).unsqueeze(0)\n",
    "#     print(state_tensor)\n",
    "    output = network(state_tensor)[0]\n",
    "#     print(output)\n",
    "    _, predicted_class = torch.max(output, dim=0)\n",
    "    return predicted_class.item()\n",
    "\n",
    "dataset = algs_to_dataset([\"R U R' F L\", \"F L F L D L F\"], 3+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 500/536 scrambles genned. Correct rate: 6.72%\n",
      "Average Loss: 3.2030, Time Spent: 31s\n",
      "Next Learning Rate: 0.029849999999999998\n",
      "2. 500/543 scrambles genned. Correct rate: 7.92%\n",
      "Average Loss: 2.9459, Time Spent: 62s\n",
      "Next Learning Rate: 0.029700749999999998\n",
      "3. 500/541 scrambles genned. Correct rate: 7.58%\n",
      "Average Loss: 2.9186, Time Spent: 94s\n",
      "Next Learning Rate: 0.029552246249999997\n",
      "4. 500/548 scrambles genned. Correct rate: 8.76%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-282e44a026df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Encode the target as class probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-0ffd82e601ed>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-0ffd82e601ed>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1458\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "n = 7 # nr of moves we want the training data on\n",
    "\n",
    "k = 6 # max length in table lookup\n",
    "treshold = 0.9 # acceptance rate for the network being correct\n",
    "nr_of_algs = 500 # number of algs we want to train on\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #CrossEntropyLoss MSELoss\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.03) #SGD\n",
    "u = 0.995\n",
    "\n",
    "correct_rate = 0\n",
    "t = time.time()\n",
    "its = 0\n",
    "while correct_rate < treshold:\n",
    "    network.eval()\n",
    "    algs = []\n",
    "    algs_genned = 0\n",
    "    while len(algs) < nr_of_algs:\n",
    "        # gen alg\n",
    "        alg = gen_random_alg(n)\n",
    "        algs_genned += 1\n",
    "        \n",
    "        # check if it can be solved using n-k AI iterations + lookup \n",
    "        cube = Cube()\n",
    "        cube.apply_moves(alg)\n",
    "        \n",
    "        sol = AI_linear_solver(cube, n-k)\n",
    "        \n",
    "        # if no solution, append to algs so we can train on it\n",
    "        if not sol:\n",
    "            algs.append(alg)\n",
    "    correct_rate = (algs_genned-nr_of_algs)/algs_genned\n",
    "    its += 1\n",
    "    print(f\"{its}. {nr_of_algs}/{algs_genned} scrambles genned. Correct rate: {100*correct_rate:.2f}%\")\n",
    "#     print(algs_genned, nr_of_algs)\n",
    "    \n",
    "#     algs = []\n",
    "#     for i in range(1000):\n",
    "#         algs.append(gen_random_alg(n))\n",
    "    \n",
    "    dataset = algs_to_dataset(algs, k+1)\n",
    "#     print(len(dataset))\n",
    "    \n",
    "    network.train()\n",
    "    # Training loop\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    for input_data, target in dataset:\n",
    "        # Forward pass\n",
    "        output = network(input_data)[0]\n",
    "\n",
    "        # Encode the target as class probabilities\n",
    "        target_onehot = F.one_hot(target, num_classes=18).float().squeeze()\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target_onehot)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        i+=1\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(dataset)\n",
    "    print(f\"Average Loss: {average_loss:.4f}, Time Spent: {int(time.time()-t)}s\")\n",
    "    optimizer.param_groups[0]['lr'] *= u\n",
    "    print(\"Next Learning Rate:\",optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "network.eval()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.7590e-01,  4.8106e-01, -6.7134e-01, -1.0616e+00, -8.4086e-01,\n",
      "         -1.1614e-02,  2.8793e-01,  2.9160e-01,  2.9286e-01, -2.3108e-02,\n",
      "         -1.3604e-01, -4.5624e-01, -1.1628e+00, -9.9339e-05, -3.6187e-01,\n",
      "          5.4141e-01,  1.4932e-01, -2.4157e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.ReLU()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(BasicBlock, 512, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128 * BasicBlock.expansion, num_classes)  # Adjusted for the final layer size\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the input size and number of classes\n",
    "input_size = 240\n",
    "num_classes = 18\n",
    "\n",
    "# Create an instance of the ResNet model\n",
    "network = ResNet(input_size, num_classes)\n",
    "\n",
    "# Create random input tensor for demonstration\n",
    "input_tensor = torch.randn(1, 1, 240)\n",
    "                           \n",
    "# Forward pass\n",
    "output = network(input_tensor)\n",
    "print(output)  # Output shape: (1, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x120 and 512x18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-3df5064b59d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Output shape: (1, num_classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-3df5064b59d5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x120 and 512x18)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, out_channels)\n",
    "        self.fc2 = nn.Linear(out_channels, out_channels)\n",
    "        self.shortcut = nn.Identity()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 240\n",
    "        self.fc1 = nn.Linear(input_size, 240)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.maxpool(out)\n",
    "        out = out.permute(0, 2, 1)  # Adjust the dimension ordering\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the input size and number of classes\n",
    "input_size = 240\n",
    "num_classes = 18\n",
    "\n",
    "# Create an instance of the ResNet model\n",
    "model = ResNet(input_size, num_classes)\n",
    "\n",
    "# Create random input tensor for demonstration\n",
    "batch_size = 1\n",
    "sequence_length = 240\n",
    "input_tensor = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Output shape: (1, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F2', 'L']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def branch(cube, k = 2, smart_branching = True):\n",
    "    '''\n",
    "    k: branching factor\n",
    "    smart_branching: if True, don't accept moves along the same layer. I.e. if R then R', replace R' with next\n",
    "    \n",
    "    returns the indexes of the moves in question\n",
    "    '''\n",
    "    state_tensor = cube_to_tensor(cube)\n",
    "    output = network(state_tensor)\n",
    "    if smart_branching:\n",
    "        sorted_move_indexes = output.sort()[1]\n",
    "        move_indexes = np.zeros(k, dtype = int)\n",
    "        move_indexes[0] = sorted_move_indexes[-1]\n",
    "        used_axes = [move_indexes[0]//3]\n",
    "        j = 1\n",
    "        for i in range(1,18): # 18 moves, first is already added\n",
    "            idx = sorted_move_indexes[-i]\n",
    "            if idx//3 not in used_axes:\n",
    "                used_axes.append(idx//3)\n",
    "                move_indexes[j] = idx\n",
    "                j+=1\n",
    "            if j==k:\n",
    "                break\n",
    "    else:\n",
    "        move_indexes = torch.flip(output.sort()[1][-k:],dims=[-1]).numpy() \n",
    "    return [moves[i] for i in move_indexes]\n",
    "\n",
    "\n",
    "scr = \"F' U2 R U2 B' L2 U2 L B D' L D L U' D2 L B2 R'\"\n",
    "cube = Cube()\n",
    "cube.apply_moves(scr)\n",
    "branch(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scramble: F' U' L' F2 B' U R2 F' U R2\n",
      "Solution: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14, 4]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST = [0,0]\n",
    "\n",
    "def AI_branch_solver(cube, it = 5, k = 2, smart_branching = True, plot = False, last_moves = [\"k\",\"l\"]):\n",
    "    '''\n",
    "    cube: scrambled cube\n",
    "    k: branching factor\n",
    "    smart_branching: if True, don't accept moves along the same layer. I.e. if R then R', replace R' with next\n",
    "    '''\n",
    "    alg = lookup_solver(cube)\n",
    "    if alg:\n",
    "        cube.apply_moves(alg)\n",
    "        return alg\n",
    "    if it == 0:\n",
    "#         print(\"test\",TEST[0])\n",
    "#         cube.plot()\n",
    "        return False\n",
    "    \n",
    "    branch_moves = branch(cube,k,smart_branching)\n",
    "    \n",
    "    for i in range(k):\n",
    "        new_move = branch_moves[i]\n",
    "        inv = inverse_alg(new_move)\n",
    "        # check that this move does not cancel the last one, and that it does not move along the same axis as the previous two moves\n",
    "        if (inv[0] != last_moves[0][0]):# and (new_move[0]!=last_moves[0][0] or new_move[0]!=last_moves[1][0]):\n",
    "            cube.apply_moves(new_move)\n",
    "            TEST[0]+=1\n",
    "            sol = AI_branch_solver(cube, it-1, k, smart_branching, last_moves = [new_move,last_moves[0]])\n",
    "            if cube.is_solved():\n",
    "                return new_move+\" \"+sol\n",
    "            cube.apply_moves(inv)\n",
    "        else:\n",
    "            TEST[1]+=1\n",
    "    return False\n",
    "\n",
    "it = 4\n",
    "k = 2\n",
    "scr = gen_random_alg(10)\n",
    "cube = Cube()\n",
    "print(\"Scramble:\",scr)\n",
    "cube.apply_moves(scr)\n",
    "# cube.plot()\n",
    "sol = AI_branch_solver(cube, it=it, k=k, smart_branching=True)\n",
    "print(\"Solution:\",sol)\n",
    "# cube.plot()\n",
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 genned in 0.0\n",
      "2 genned in 0.0009953975677490234\n",
      "3 genned in 0.013967037200927734\n"
     ]
    }
   ],
   "source": [
    "scr = gen_random_alg(10)\n",
    "algs = gen_all_algs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 B' L D' F D2 F D' R' F2 R' U R2 L' F' R' F B2 R B2 R' B R F D' R2 U F2 U' D2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evalute_cube(cube):\n",
    "    state_tensor = cube_to_tensor(cube)\n",
    "    output = network(state_tensor)\n",
    "    _, predicted_class = torch.max(output, dim=0)\n",
    "    return predicted_class.item()\n",
    "\n",
    "def AI_linear_solver(cube, iterations = 10):\n",
    "    solution = \"\"\n",
    "    last_move = \"NO\"\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        alg = lookup_solver(cube)\n",
    "        if alg:\n",
    "            cube.apply_moves(alg)\n",
    "            return solution + alg\n",
    "        ev = evalute_cube(cube)\n",
    "        move = moves[ev]\n",
    "        solution += move+\" \"\n",
    "        cube.apply_moves(move)\n",
    "    alg = lookup_solver(cube)\n",
    "    if alg:\n",
    "        cube.apply_moves(alg)\n",
    "        return solution + alg\n",
    "    else:\n",
    "#         cube.plot()\n",
    "        cube.apply_moves(inverse_alg(solution.strip()))\n",
    "    return False\n",
    "\n",
    "\n",
    "def epic_solver(cube, AI_iterations = 10, algs = [\"\"]):\n",
    "    '''\n",
    "    cube: scrambled cube that we want solved\n",
    "    AI_iterations: nr of iterations for the AI\n",
    "    cutoff: stop generating scrambles at this point\n",
    "    '''\n",
    "    \n",
    "    # check if AI + table lookup solves this case\n",
    "    sol0 = AI_linear_solver(cube,AI_iterations)\n",
    "    if sol0:\n",
    "        return sol0\n",
    "    TEST = 0\n",
    "    for alg in algs:\n",
    "        TEST+=1\n",
    "        inv = inverse_alg(alg)\n",
    "        cube.apply_moves(alg)\n",
    "        sol0 = AI_linear_solver(cube, AI_iterations)\n",
    "        if sol0:\n",
    "            return alg+\" \"+sol0\n",
    "        cube.apply_moves(inv)\n",
    "    return False\n",
    "     \n",
    "    \n",
    "scr = gen_random_alg(30)\n",
    "cube = Cube()\n",
    "cube.apply_moves(scr)\n",
    "print(scr)\n",
    "cube.plot()\n",
    "sol = epic_solver(cube, AI_iterations=12, algs = algs)\n",
    "print(sol)\n",
    "cube.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.518123865127563\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "for i in range(10000):\n",
    "    output = network(input_data)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
